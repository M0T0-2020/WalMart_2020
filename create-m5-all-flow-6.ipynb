{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import gc, pickle\n",
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold,TimeSeriesSplit, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, log_loss\n",
    "from sklearn.linear_model import Ridge,Lasso, BayesianRidge\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def create_is_sell_data(sell_prices_df, calendar_df, train_df):\n",
    "    sell_prices_df['id'] = sell_prices_df['item_id'].astype('str')+'_'+sell_prices_df['store_id']+'_validation'\n",
    "    sell_prices_data = sell_prices_df[sell_prices_df.wm_yr_wk.isin(calendar_df.wm_yr_wk.unique())]\n",
    "    sell_prices_data.reset_index(drop=True, inplace=True)\n",
    "    tmp = sell_prices_data.groupby(['id'])[['wm_yr_wk', 'sell_price']].apply(\n",
    "        lambda x: x.set_index('wm_yr_wk')['sell_price'].to_dict()\n",
    "    ).to_dict()\n",
    "    d = calendar_df.d\n",
    "    wm_yr_wk = calendar_df.wm_yr_wk\n",
    "    price_data = {}\n",
    "    for col in tqdm(train_df.id.unique()):\n",
    "        price_data[col] = wm_yr_wk.map(tmp[col])\n",
    "    price_data = pd.DataFrame(price_data)\n",
    "    price_data.index = d\n",
    "    is_sell = price_data.notnull().astype(float).T\n",
    "    price_data = price_data.fillna(0)\n",
    "    \n",
    "    is_sell.index=train_df.id\n",
    "    train_df.index=train_df.id\n",
    "    is_sell = pd.concat([\n",
    "        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']], is_sell\n",
    "    ], axis=1)\n",
    "    price_data = pd.concat([\n",
    "        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']], price_data.T  \n",
    "    ], axis=1)\n",
    "    \n",
    "    return price_data, is_sell\n",
    "\n",
    "def set_index(df, name):\n",
    "    d = {}\n",
    "    for col, value in df.iloc[0,:].items():\n",
    "        if type(col)==str:\n",
    "            if type(df[col].values[0])!=str:\n",
    "                v = 'd'\n",
    "            else:\n",
    "                v='id'\n",
    "        else:\n",
    "            v=name\n",
    "        d[col]=v\n",
    "    return d\n",
    "\n",
    "def dcol2int(col):\n",
    "    if col[:2]=='d_':\n",
    "        return int(col.replace('d_', ''))\n",
    "    else:\n",
    "        return col\n",
    "    \n",
    "def create_event_data(train_df, calendar_df):\n",
    "    new_df = pd.DataFrame()\n",
    "    D_COLS = [d for d in train_df.columns if type(d)!=str]\n",
    "    for event_name in ['event_name_1', 'event_name_2']:\n",
    "        tmp_df = pd.concat([\n",
    "            train_df.groupby(['dept_id'])[D_COLS].mean().T.astype(float),\n",
    "            train_df.groupby(['cat_id'])[D_COLS].mean().T.astype(float),\n",
    "            calendar_df.loc[D_COLS,event_name].replace(np.nan, 'NAN')\n",
    "        ],axis=1)\n",
    "\n",
    "        dept_id_cols = train_df.dept_id.unique().tolist()\n",
    "        cat_id_cols = train_df.cat_id.unique().tolist()\n",
    "\n",
    "        tmp_df = pd.concat([\n",
    "            tmp_df[[event_name]],\n",
    "            tmp_df.groupby([event_name])[dept_id_cols].transform(\n",
    "            lambda x: x.shift(1).rolling(len(x), min_periods=1).mean()\n",
    "            ),\n",
    "            tmp_df.groupby([event_name])[cat_id_cols].transform(\n",
    "            lambda x: x.shift(1).rolling(len(x), min_periods=1).mean()\n",
    "            )\n",
    "        ], axis=1)\n",
    "\n",
    "        tmp_df[dept_id_cols] = tmp_df[dept_id_cols]/tmp_df[dept_id_cols].rolling(56, min_periods=1).mean().shift(1)\n",
    "        tmp_df[cat_id_cols] = tmp_df[cat_id_cols]/tmp_df[cat_id_cols].rolling(56, min_periods=1).mean().shift(1)\n",
    "        tmp_df.loc[tmp_df[event_name]=='NAN', dept_id_cols+cat_id_cols]=1\n",
    "        \n",
    "        tmp_df.columns=[f'{event_name}_{col}' for col in tmp_df.columns]\n",
    "        \n",
    "        new_df = pd.concat([\n",
    "            new_df, tmp_df\n",
    "        ] ,axis=1)\n",
    "    new_df.index=D_COLS\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "def create_metadata(path, d_cols, submmit=True):\n",
    "    train_df = pd.read_csv(path+'sales_train_validation.csv')\n",
    "    calendar_df = pd.read_csv(path+'calendar.csv')\n",
    "    sell_prices_df = pd.read_csv(path+'sell_prices.csv')\n",
    "    sample_submission_df = pd.read_csv(path+'sample_submission.csv')\n",
    "\n",
    "    calendar_df['d'] = calendar_df.d.str.replace('d_', '').astype(int)\n",
    "    cols = train_df.columns\n",
    "    cols = [dcol2int(col) for col in cols]\n",
    "    train_df.columns=cols\n",
    "    calendar_df['date']=pd.to_datetime(calendar_df.date)\n",
    "    calendar_df.index = calendar_df.d\n",
    "    price_data, is_sell = create_is_sell_data(sell_prices_df, calendar_df, train_df)\n",
    "    \n",
    "    str_cols = [ col for col in train_df.columns if 'id' in str(col)]\n",
    "    new_columns = str_cols+d_cols\n",
    "    train_df = train_df.reindex(columns=new_columns)\n",
    "    \n",
    "    \n",
    "    train_df = pd.concat([\n",
    "        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']],\n",
    "        train_df.loc[:,d_cols]+is_sell[d_cols].replace(0, np.nan).replace(1, 0)\n",
    "    ], axis=1)\n",
    "    train_df.index = train_df.id\n",
    "    del is_sell;gc.collect()\n",
    "    \n",
    "    df = train_df.loc[:,d_cols].T.astype(float)\n",
    "    a = df.loc[d_cols[28:-56]].rolling(28, min_periods=1).sum().replace(0,np.nan)+df.loc[d_cols[28:-56]][::-1].rolling(28, min_periods=1).sum()[::-1].replace(0,np.nan)\n",
    "    a[a.notnull()]=0\n",
    "    df.loc[d_cols[28:-56]] += a\n",
    "    df = df.loc[d_cols,:].T.astype(float)\n",
    "    del a;gc.collect()\n",
    "    \n",
    "    #snap_data\n",
    "    snap_data = calendar_df[['snap_CA', 'snap_WI', 'snap_TX', 'd']]\n",
    "    snap_data.set_index('d', inplace=True)\n",
    "    \n",
    "    #dept_id_price\n",
    "    dept_id_price = price_data[d_cols]/price_data.groupby(['dept_id', 'store_id'])[d_cols].transform('mean')\n",
    "    dept_id_price = dept_id_price.T.astype(float)\n",
    "    #dept_id_price['d'] = dept_id_price.index\n",
    "    dept_id_price = dept_id_price.replace(0,np.nan)\n",
    "    \n",
    "    #cat_id_price\n",
    "    cat_id_price = price_data[d_cols]/price_data.groupby(['cat_id', 'store_id'])[d_cols].transform('mean')\n",
    "    cat_id_price = cat_id_price.T.astype(float)\n",
    "    #cat_id_price['d'] = cat_id_price.index\n",
    "    cat_id_price = cat_id_price.replace(0,np.nan)\n",
    "    \n",
    "    #price_data\n",
    "    price_data = price_data[d_cols].T\n",
    "    price_data.replace(0,np.nan, inplace=True)\n",
    "    #price_data['d']=price_data.index\n",
    "    \n",
    "    #event_df\n",
    "    event_df = create_event_data(train_df, calendar_df)\n",
    "    #event_df.reset_index(inplace=True)\n",
    "    \n",
    "    #calendar_dict\n",
    "    calendar_dict = calendar_df[['wday', 'month']].to_dict()\n",
    "    \n",
    "    return train_df, snap_data, dept_id_price, cat_id_price, price_data, event_df, calendar_dict, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def make_roll_data(data, win, agg={'mean', 'std'}):\n",
    "    data_2 = data.groupby(['id'])['TARGET'].apply(\n",
    "        lambda x:\n",
    "        x.shift(1).rolling(win, min_periods=1).agg(agg)\n",
    "    )\n",
    "    data_2.columns=[f'roll_{win}_{col}' for col in data_2.columns]\n",
    "    data = pd.concat([\n",
    "        data, data_2\n",
    "    ], axis=1)\n",
    "    return data\n",
    "\n",
    "def make_diff_data(data, win):\n",
    "    diff_data = data.groupby(['id'])['TARGET'].apply(\n",
    "        lambda x:\n",
    "        abs(x.shift(1).diff(1)).rolling(win, min_periods=1).agg({'mean', 'std'})\n",
    "    ) \n",
    "    diff_data.columns=[f'diff_{col}_{win}_1' for col in diff_data.columns]\n",
    "    data = pd.concat([\n",
    "        data, diff_data\n",
    "    ], axis=1)\n",
    "    \n",
    "    diff_data = data.groupby(['id'])['TARGET'].apply(\n",
    "        lambda x:\n",
    "        abs(x.shift(1).diff(7)).rolling(win, min_periods=1).agg({'mean', 'std'})\n",
    "    ) \n",
    "    diff_data.columns=[f'diff_{col}_{win}_7' for col in diff_data.columns]\n",
    "    data = pd.concat([\n",
    "        data, diff_data\n",
    "    ], axis=1)\n",
    "    return data\n",
    "\n",
    "def make_shift_data(data):\n",
    "    shift=7\n",
    "    for i, p in  enumerate([0,7]):\n",
    "        data[f'shift_{i+1}'] = data.groupby(['id'])['TARGET'].shift(shift+p)\n",
    "    data['shift_3'] = data[['shift_1', 'shift_2']].mean(1)\n",
    "    return data\n",
    "\n",
    "def preprocessing(path,d_cols,test):\n",
    "    train_df, snap_data, dept_id_price, cat_id_price, price_data, event_df, calendar_dict, df = create_metadata(path, d_cols)\n",
    "    if test:\n",
    "        train_df = train_df[train_df.id.isin(train_df.id.unique()[:2000])]\n",
    "    data = pd.concat([\n",
    "        train_df[['id', 'dept_id', 'store_id']],\n",
    "        df[d_cols[:-28]].isnull().sum(axis=1),\n",
    "        df[d_cols[:-28]].mean(1)\n",
    "    ],axis=1)\n",
    "    data.columns=['id', 'dept_id', 'store_id', 'null_num_600', 'sell_mean']\n",
    "    data['sell_mean_null_600'] = data['sell_mean']/data['null_num_600']\n",
    "    data = data.sort_values('sell_mean_null_600', ascending=False)#.index.tolist()\n",
    "    \n",
    "    ids = []\n",
    "    for dept in ['HOBBIES_1', 'HOBBIES_2', 'HOUSEHOLD_1', 'HOUSEHOLD_2', 'FOODS_1', 'FOODS_2', 'FOODS_3']:\n",
    "        ids += data[data.dept_id==dept][:500].index.tolist()\n",
    "    for store in ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']:\n",
    "        ids += data[data.store_id==store][:500].index.tolist()\n",
    "    ids += data.index.tolist()[:2000]\n",
    "    ids = np.unique(ids).tolist()\n",
    "    ids_2 = data[~data.id.isin(ids)].id.unique().tolist()\n",
    "    gc.collect()\n",
    "    \n",
    "    print('len  ids  ', len(ids))\n",
    "    print('len  ids_2  ', len(ids_2))\n",
    "    \n",
    "    data = train_df[train_df.id.isin(ids)][d_cols[-200:]].stack(dropna=False).reset_index()\n",
    "    data = data.rename(columns=set_index(data, 'TARGET'))\n",
    "    data.sort_values('d', inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data = reduce_mem_usage(data)\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    for key, value in train_df[['dept_id', 'cat_id', 'state_id', 'store_id']].to_dict().items():\n",
    "        data[key] = data.id.map(value)\n",
    "    \n",
    "    data[f'snap']=0\n",
    "    for key, value in snap_data.to_dict().items():\n",
    "        k = key.replace('snap_', '')\n",
    "        data.loc[data.state_id==k,'snap'] = data.loc[data.state_id==k, 'd'].map(value).fillna(0)\n",
    "    for shift in [-3,-2,-1,1,2,3]:\n",
    "        data[f'snap_{shift}'] = data.groupby(['id'])['snap'].shift(shift).fillna(0)\n",
    "\n",
    "\n",
    "    dept_id_price = dept_id_price.stack(dropna=False).reset_index()\n",
    "    cat_id_price = cat_id_price.stack(dropna=False).reset_index()\n",
    "\n",
    "    dept_id_price.rename(columns=set_index(dept_id_price, 'dept_id_price'), inplace=True)\n",
    "    cat_id_price.rename(columns=set_index(cat_id_price, 'cat_id_price'), inplace=True)\n",
    "\n",
    "    data = pd.merge(\n",
    "        data, dept_id_price, on=['d', 'id'], how='left'\n",
    "    )\n",
    "    data = pd.merge(\n",
    "        data, cat_id_price, on=['d', 'id'], how='left'\n",
    "    )\n",
    "\n",
    "\n",
    "    del dept_id_price,cat_id_price;gc.collect()\n",
    "\n",
    "    price_data = price_data.stack(dropna=False).reset_index()\n",
    "    price_data.rename(columns=set_index(price_data, 'price'), inplace=True)\n",
    "    data = pd.merge(\n",
    "        data, price_data, on=['d', 'id'], how='left'\n",
    "    )\n",
    "    del price_data;gc.collect()\n",
    "\n",
    "    data['wday'] = data.d.map(calendar_dict['wday'])\n",
    "    del calendar_dict;gc.collect()\n",
    "\n",
    "\n",
    "    tmp_dic = event_df.to_dict()\n",
    "    data[f'dept_id_event_name_1']=1\n",
    "    data[f'cat_id_event_name_1']=1\n",
    "    for key, value in tmp_dic.items():\n",
    "        if 'event_name_1' in key:\n",
    "            if key[13:] in train_df.dept_id.unique().tolist():\n",
    "                data.loc[data.dept_id==key[13:], f'dept_id_{key[:12]}']=data.loc[data.dept_id==key[13:], 'd'].map(value).fillna(1)\n",
    "            if key[13:] in train_df.cat_id.unique().tolist():\n",
    "                data.loc[data.cat_id==key[13:], f'cat_id_{key[:12]}']=data.loc[data.cat_id==key[13:], 'd'].map(value).fillna(1)\n",
    "    for shift in [-3,-2,-1,1,2,3]:\n",
    "        for event_name in ['dept_id_event_name_1', 'cat_id_event_name_1']:\n",
    "            data[f'{event_name}_shift{shift}'] = data.groupby(['id'])[event_name].shift(shift).fillna(1)\n",
    "\n",
    "    cols = data.columns.tolist()\n",
    "    print(cols)\n",
    "\n",
    "    data = make_roll_data(data=data,win=28,agg={'mean', 'std', 'skew'})\n",
    "    data = make_roll_data(data=data,win=7,agg={'mean', 'min', 'max'})\n",
    "    data = make_roll_data(data,win=56,agg={'std', 'skew'})\n",
    "    data = make_diff_data(data=data, win=28)\n",
    "    data = make_diff_data(data=data, win=7)\n",
    "    data = make_shift_data(data=data)\n",
    "    gc.collect()\n",
    "\n",
    "    print([col for col in data.columns if not col in cols])\n",
    "    \n",
    "    categories = [c for c in data.columns if data[c].dtype==object]\n",
    "    print(categories)\n",
    "    for c in categories:\n",
    "        if c=='id':\n",
    "            pass\n",
    "        else:\n",
    "            data[c] = pd.factorize(data[c])[0]\n",
    "    \n",
    "    return data, ids, ids_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce_mem_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF shift "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def shift_seven(data, cols):\n",
    "    #['shift_1', 'shift_2', 'shift_3', 'shift_4']\n",
    "    data[cols] = data.groupby(['id'])[cols].shift(7)\n",
    "    return data\n",
    "    \n",
    "def shift_one(data,cols):\n",
    "    #['roll_28_std', 'roll_28_mean', 'diff_std_1', 'diff_mean_1', 'diff_std_7', 'diff_mean_7']\n",
    "    data[cols]=data.groupby(['id'])[cols].shift(1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lgb model utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'tweedie',\n",
    "                    'tweedie_variance_power': 1.1,\n",
    "                    'metric': 'rmse',\n",
    "                    'subsample': 0.5,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.03,\n",
    "                    'num_leaves': 2**11-1,\n",
    "                    'min_data_in_leaf': 2**12-1,\n",
    "                    'feature_fraction': 0.5,\n",
    "                    'max_bin': 100,\n",
    "                    'n_estimators': 1400,\n",
    "                    'boost_from_average': False,\n",
    "                    'verbose': 1,\n",
    "                } \n",
    "\n",
    "def plot_importance(models, col, name):\n",
    "    importances = np.zeros(len(col))\n",
    "    for model in models:\n",
    "        importances+=model.feature_importance(importance_type='gain')\n",
    "    importance = pd.DataFrame()\n",
    "    importance['col'] = col\n",
    "    importance[f'importance_{name}'] = importances\n",
    "    importance[f'importance_{name}'] = minmax_scale(importance[f'importance_{name}'])\n",
    "    #importance.to_csv(f'importance_{name}.csv',index=False)\n",
    "    return importance\n",
    "    \n",
    "def run_nest_cv(x_train, y_train, trn_df, params=PARAMS):\n",
    "    models = []\n",
    "    #timesplit = TimeSeriesSplit(n_splits=3,max_train_size=30490*200)\n",
    "    k = GroupKFold(n_splits=5)\n",
    "    trn_df['y_pred'] = 0\n",
    "    \n",
    "    for trn_indx, val_indx in k.split(x_train, groups=x_train['dept_id']):\n",
    "        train_set = lgb.Dataset(x_train.loc[trn_indx,:], y_train.loc[trn_indx])\n",
    "        val_set = lgb.Dataset(x_train.loc[val_indx,:], y_train.loc[val_indx])\n",
    "        \n",
    "        categories = ['cat_id', 'dept_id', 'store_id']\n",
    "        \n",
    "        model = lgb.train(\n",
    "            train_set=train_set, \n",
    "            valid_sets=[train_set, val_set],\n",
    "            params=params, num_boost_round=3000, early_stopping_rounds=100, verbose_eval=500,\n",
    "            categorical_feature=categories+['wday']\n",
    "        )\n",
    "        \n",
    "        models.append(model)\n",
    "        trn_df.loc[val_indx, 'y_pred']=np.e**(model.predict(x_train.loc[val_indx,:]))-1\n",
    "        gc.collect()\n",
    "        \n",
    "    return models, trn_df\n",
    "\n",
    "def predict_cv(x_val, models):\n",
    "    preds = np.zeros(len(x_val))\n",
    "    for model in models:\n",
    "        pred = model.predict(x_val)\n",
    "        pred = np.e**pred - 1\n",
    "        preds+=pred/len(models)\n",
    "    return preds\n",
    "\n",
    "def show_eval_score(preds, val_df):\n",
    "    val_df['y_pred'] = preds\n",
    "    score= np.sqrt(mean_squared_error(val_df['TARGET'], preds))\n",
    "    print(\"EVALUATION SCORE : \", score)\n",
    "    return val_df\n",
    "\n",
    "def split_data(data, trn_day, val_day):\n",
    "    data = data[data.shift_2.notnull()]\n",
    "    \n",
    "    y = data[['d', 'id', 'TARGET']]\n",
    "    X = data.drop(columns=['id',  'TARGET','state_id']).astype(float)\n",
    "    \n",
    "    x_train, x_val = X[X.d.isin(trn_day)], X[X.d.isin(val_day)]\n",
    "    y_train, y_val = y[y.d.isin(trn_day)], y[y.d.isin(val_day)]\n",
    "    \n",
    "    x_train.reset_index(drop=True,inplace=True)\n",
    "    x_val.reset_index(drop=True,inplace=True)\n",
    "    y_train.reset_index(drop=True,inplace=True)\n",
    "    y_val.reset_index(drop=True,inplace=True)\n",
    "    trn_df = y_train[['id', 'd', 'TARGET']]\n",
    "    val_df = y_val[['id', 'd', 'TARGET']]\n",
    "    y_train['TARGET'] = np.log1p(y_train['TARGET'])\n",
    "    \n",
    "    x_train.drop('d', axis=1, inplace=True)\n",
    "    x_val.drop('d', axis=1, inplace=True)\n",
    "    y_train = y_train['TARGET'].astype(float)\n",
    "    return x_train, x_val, y_train, trn_df, val_df\n",
    "\n",
    "def train(data):\n",
    "    split=28\n",
    "    data = data[data.TARGET.notnull()]\n",
    "    d_cols = sorted(data.d.unique())\n",
    "    trn_day = d_cols[:-split]\n",
    "    val_day = d_cols[-split:]\n",
    "\n",
    "    x_train, x_val, y_train, trn_df, val_df = split_data(data, trn_day, val_day)\n",
    "    print(x_train.shape, x_val.shape)\n",
    "    models, trn_df = run_nest_cv(x_train, y_train, trn_df)\n",
    "    preds = predict_cv(x_val, models)\n",
    "    val_df = show_eval_score(preds, val_df)\n",
    "    plot_importance(models, x_train.columns)\n",
    "    return models, val_df, trn_df\n",
    "\n",
    "def split_data_for_sub(data):\n",
    "    data = data[data.TARGET.notnull()]\n",
    "    data = data[data.shift_2.notnull()]\n",
    "    data = data[data.diff_std_7_1.notnull()]\n",
    "    trn_df = data[['id', 'd', 'TARGET']]\n",
    "    y = np.log1p(data['TARGET']).astype(float)\n",
    "    X = data.drop(columns=['id','d', 'TARGET','state_id']).astype(float)\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "    trn_df.reset_index(drop=True, inplace=True)\n",
    "    return X, y, trn_df\n",
    "\n",
    "def train_sub_predict(data, for_predict):\n",
    "    train_d_cols = data.d.unique().tolist()\n",
    "    predict_day=train_d_cols[-28:][for_predict-1]\n",
    "    sub_predict_data = data[data.d==predict_day]\n",
    "    X, y, trn_df = split_data_for_sub(data)\n",
    "    print(X.shape)\n",
    "    models, trn_df = run_nest_cv(X, y, trn_df)\n",
    "    importance = plot_importance(models, X.columns, str(for_predict))\n",
    "    preds = predict_cv(sub_predict_data[X.columns], models)\n",
    "    \n",
    "    sub_df = sub_predict_data[['id', 'd', 'TARGET']]\n",
    "    sub_df[f'y_pred'] = preds\n",
    "    return trn_df, sub_df, importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_importance(data ,trn_df):\n",
    "    X = data[data.d.isin(trn_df.d.unique())]\n",
    "    X = X[X.TARGET.notnull()]\n",
    "    X = X[X.shift_2.notnull()]\n",
    "    X = X[X.diff_std_7_1.notnull()]\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y = np.log1p(X['TARGET'])\n",
    "    X = X.drop(columns=['id','d', 'TARGET','state_id']).astype(float)\n",
    "    \n",
    "    k = GroupKFold(n_splits=3,max_train_size=30490*200)\n",
    "    null_importance = pd.DataFrame()\n",
    "    null_importance['cols'] = X.columns\n",
    "    for n in range(num):\n",
    "        y = y.sample(frac=1)\n",
    "        y.reset_index(drop=True, inplace=True)\n",
    "        for trn_indx, val_indx in k.split(X, groups=X['dept_id']):\n",
    "            train_set = lgb.Dataset(X.loc[trn_indx,:], y.loc[trn_indx])\n",
    "            val_set = lgb.Dataset(X.loc[val_indx,:], y.loc[val_indx])\n",
    "\n",
    "            categories = ['cat_id', 'dept_id', 'store_id']\n",
    "\n",
    "            model = lgb.train(\n",
    "                train_set=train_set, \n",
    "                valid_sets=[train_set, val_set],\n",
    "                params=params, num_boost_round=3000, early_stopping_rounds=100, verbose_eval=500,\n",
    "                categorical_feature=categories+['wday']\n",
    "            )\n",
    "            \n",
    "            null_importance[f'null_importance_{n}'] = model.feature_importance(importance_type='gain')\n",
    "    return null_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linear model utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def to_onehot_data(data):\n",
    "    data = data.drop(columns=['state_id'])\n",
    "    category = ['cat_id', 'dept_id', 'store_id', 'wday']\n",
    "    one_hot_cols=[]\n",
    "    for cat in category:\n",
    "        one_hot_data = pd.get_dummies(data[cat]).rename(columns={i:f'{cat}_{int(i)}' for i in data[cat].unique()})\n",
    "        one_hot_cols+=one_hot_data.columns.tolist()\n",
    "        data = pd.concat([\n",
    "            data.drop(cat, axis=1),\n",
    "            one_hot_data\n",
    "        ], axis=1)\n",
    "    return data, one_hot_cols\n",
    "\n",
    "def data_split_lin(data, trn_days, val_days):\n",
    "    data.dropna(0, inplace=True)\n",
    "    train = data[data.d.isin(trn_days)]\n",
    "    val = data[data.d.isin(val_days)]\n",
    "    train.reset_index(drop=True, inplace=True)\n",
    "    val.reset_index(drop=True, inplace=True)\n",
    "    trn_df = train[['id', 'd', 'TARGET']]\n",
    "    val_df = val[['id', 'd', 'TARGET']]\n",
    "    return train, val, trn_df, val_df\n",
    "\n",
    "def linear_cv(data, trn_df):\n",
    "    k = StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "    models={}\n",
    "    models['ridge'] = []\n",
    "    models['lasso'] = []\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    X = data.drop(columns=['id', 'd', 'TARGET'])\n",
    "    y = data['TARGET']\n",
    "    data['ridge_preds'] = 0\n",
    "    data['lasso_preds'] = 0\n",
    "    for trn_indx, val_indx in k.split(data,y=y):\n",
    "        \n",
    "        ridge = Ridge()\n",
    "        lasso = Lasso()\n",
    "    \n",
    "        ridge.fit(X.loc[trn_indx,:],y.loc[trn_indx])\n",
    "        lasso.fit(X.loc[trn_indx,:],y.loc[trn_indx])\n",
    "        models['ridge'].append(ridge)\n",
    "        models['lasso'].append(lasso)\n",
    "        \n",
    "        trn_df.loc[val_indx, 'ridge_preds'] = ridge.predict(X.loc[val_indx,:])\n",
    "        trn_df.loc[val_indx, 'lasso_preds'] = lasso.predict(X.loc[val_indx,:])\n",
    "    \n",
    "    return models, trn_df\n",
    "\n",
    "def cv_predict_lin(data, models):\n",
    "    preds = np.zeros(len(data))\n",
    "    for model in models:\n",
    "        preds+=model.predict(data.drop(columns=['id', 'd', 'TARGET'])) /len(models)\n",
    "    return preds\n",
    "    \n",
    "def linear_predict(models, X, val_df):\n",
    "    for name, _models in models.items():\n",
    "        val_df[f'{name}_preds'] = cv_predict_lin(X, _models)\n",
    "    return val_df\n",
    "\n",
    "def train_lin(data, trn_days, val_days):\n",
    "    X = to_onehot_data(data)\n",
    "    train, val, trn_df, val_df = data_split_lin(X, trn_days, val_days)\n",
    "    print(train.shape, val.shape)\n",
    "    models, trn_df = linear_cv(train, trn_df)\n",
    "    val_df = linear_predict(models, val, val_df)\n",
    "    return val_df, trn_df\n",
    "\n",
    "def train_lin_sub(data, for_predict):\n",
    "    predict_day = data.d.unique()[-28:][for_predict-1]\n",
    "    X, one_hot_cols = to_onehot_data(data)\n",
    "    predict_sub_df = X[X.d==predict_day]\n",
    "    \n",
    "    X.dropna(0, inplace=True)\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    predict_sub_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    trn_df = X[['id', 'd', 'TARGET']+one_hot_cols]\n",
    "    val_df = predict_sub_df[['id', 'd', 'TARGET']+one_hot_cols]\n",
    "\n",
    "    models, trn_df = linear_cv(X, trn_df)\n",
    "    val_df = linear_predict(models, predict_sub_df, val_df)\n",
    "    return val_df, trn_df, one_hot_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM, BayesianRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def cv_bayesianRidge(trn_df ,val_df, one_hot_cols):\n",
    "    k = StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n",
    "    X = trn_df[trn_df.ridge_preds.notnull()][['id', 'd','y_pred','ridge_preds', 'lasso_preds']+one_hot_cols]\n",
    "    y = trn_df[trn_df.ridge_preds.notnull()]['TARGET']\n",
    "    X['br_pred'] = 0\n",
    "    val_df['br_pred'] = 0\n",
    "    X['br_std'] = 0\n",
    "    val_df['br_std'] = 0\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "    for trn_indx, val_indx in k.split(X,y=y):\n",
    "        \n",
    "        br = BayesianRidge()\n",
    "        br.fit(X.loc[trn_indx,['y_pred','ridge_preds', 'lasso_preds']+one_hot_cols], y.loc[trn_indx])\n",
    "        p, _std= br.predict(X.loc[val_indx,['y_pred','ridge_preds', 'lasso_preds']+one_hot_cols], return_std=True)\n",
    "        X.loc[val_indx, 'br_pred'] = p \n",
    "        X.loc[val_indx, 'br_std'] = _std\n",
    "        p, _std= br.predict(val_df[['y_pred','ridge_preds', 'lasso_preds']+one_hot_cols], return_std=True)\n",
    "        val_df['br_pred'] += p/5\n",
    "        val_df['br_std'] += _std/5\n",
    "    trn_df = pd.merge(trn_df, X[['id', 'd','br_pred', 'br_std']], how='outer', on=['id', 'd'])\n",
    "    return trn_df, val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PipeLine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def train_cv_pipeline(data):\n",
    "    models, val_df, trn_df = train(data)\n",
    "    \n",
    "    #lin_data = data.drop(columns=['roll_28_skew', 'roll_56_skew'])\n",
    "    #val_df_lin, trn_df_lin = train_lin(lin_data, trn_df.d.unique().tolist(), val_df.d.unique().tolist())\n",
    "    \n",
    "    #val_df = pd.merge(val_df, val_df_lin, how='outer', on=['id', 'TARGET', 'd'])\n",
    "    #trn_df = pd.merge(trn_df, trn_df_lin, how='outer', on=['id', 'TARGET', 'd'])\n",
    "\n",
    "    return val_df, trn_df\n",
    "\n",
    "def predict_sub_pipeline(data, for_predict):\n",
    "    trn_df, sub_df, importance = train_sub_predict(data, for_predict)\n",
    "    \n",
    "    \n",
    "    #lin_data = data.drop(columns=['roll_28_skew', 'roll_56_skew'])\n",
    "    #sub_df_lin, trn_df_lin, one_hot_cols = train_lin_sub(lin_data, for_predict)\n",
    "    \n",
    "    #sub_df = pd.merge(sub_df, sub_df_lin, how='outer', on=['id', 'TARGET', 'd'])\n",
    "    #trn_df = pd.merge(trn_df, trn_df_lin, how='outer', on=['id', 'TARGET', 'd'])\n",
    "\n",
    "    return sub_df,trn_df,one_hot_cols, importance\n",
    "\n",
    "\n",
    "def all_predict_run(data, for_predict):\n",
    "    print(F\"\"\"\n",
    "    ###################################\n",
    "               TRAIN {for_predict}\n",
    "    ###################################\n",
    "    \"\"\")\n",
    "    \n",
    "    s = time()\n",
    "    sub_df,trn_df,one_hot_cols,importance = predict_sub_pipeline(data, for_predict)\n",
    "    trn_df, sub_df = cv_bayesianRidge(trn_df ,sub_df, one_hot_cols)\n",
    "    #trn_df.to_csv(f'trn_{for_predict}.csv', index=False)\n",
    "    #,'ridge_preds', 'lasso_preds','br_pred', 'br_std'\n",
    "    sub_df = sub_df[['id', 'd', 'y_pred']]\n",
    "    s = (time()-s)/60\n",
    "    print(f'TIME :  {s :.4}  \\n')\n",
    "    return sub_df, importance\n",
    "\n",
    "    \n",
    "def sub_cycle(path, d_cols, stop=None, test=False):\n",
    "    t = time()\n",
    "    data, ids, ids_2 = preprocessing(path,d_cols,test=test)\n",
    "    t = (time()-t)/60\n",
    "    print(f'PREPROCESSING  TIME :  {t :.4}  \\n')\n",
    "    \n",
    "    shift_seven_cols = ['shift_1', 'shift_2', 'shift_3']\n",
    "    shift_one_cols = ['roll_28_skew', 'roll_28_mean', 'roll_28_std', 'roll_7_max',\n",
    "                      'roll_7_mean', 'roll_7_min', 'roll_56_skew', 'roll_56_std', \n",
    "                      'diff_mean_28_1', 'diff_std_28_1', 'diff_mean_28_7', 'diff_std_28_7',\n",
    "                      'diff_mean_7_1', 'diff_std_7_1', 'diff_mean_7_7', 'diff_std_7_7']\n",
    "    \n",
    "    for_predict=1\n",
    "    trn_df, sub_df, importance = train_sub_predict(data, for_predict)\n",
    "    #sub_df, importance = all_predict_run(data=data, for_predict=for_predict)\n",
    "    sub_df.drop('TARGET', axis=1, inplace=True)\n",
    "    sub_df.columns = ['id', 'd', f'y_pred_{for_predict}']\n",
    "    #['id', 'd', f'y_pred_{for_predict}',f'ridge_preds_{for_predict}',\n",
    "    #f'lasso_preds_{for_predict}',f'br_pred_{for_predict}', f'br_std_{for_predict}']\n",
    "    print('\\n')\n",
    "    \n",
    "    if stop  is  None:\n",
    "        stop = 29\n",
    "    \n",
    "    for for_predict in range(2,stop):\n",
    "        if (for_predict-1)%7==0:\n",
    "            data = shift_seven(data, shift_seven_cols)\n",
    "        data = shift_one(data, shift_one_cols)\n",
    "        tmp_trn_df, tmp_sub_df, importance = train_sub_predict(data, for_predict)\n",
    "        tmp_sub_df.drop('TARGET', axis=1, inplace=True)\n",
    "        #tmp_sub_df, tmp_importance = all_predict_run(data=data, for_predict=for_predict)\n",
    "        tmp_sub_df.columns =['id', 'd', f'y_pred_{for_predict}']\n",
    "        #['id', 'd', f'y_pred_{for_predict}',f'ridge_preds_{for_predict}',\n",
    "        #f'lasso_preds_{for_predict}',f'br_pred_{for_predict}', f'br_std_{for_predict}']\n",
    "        sub_df = pd.merge(sub_df,tmp_sub_df,on=['id', 'd'])\n",
    "        importance = pd.merge(importance,tmp_importance,on=['col'],how='outer')\n",
    "    return sub_df, importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30490/30490 [00:10<00:00, 2841.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len  ids   5940\n",
      "len  ids_2   24550\n",
      "Mem. usage decreased to 13.60 Mb (50.0% reduction)\n",
      "['id', 'd', 'TARGET', 'dept_id', 'cat_id', 'state_id', 'store_id', 'snap', 'snap_-3', 'snap_-2', 'snap_-1', 'snap_1', 'snap_2', 'snap_3', 'dept_id_price', 'cat_id_price', 'price', 'wday', 'dept_id_event_name_1', 'cat_id_event_name_1', 'dept_id_event_name_1_shift-3', 'cat_id_event_name_1_shift-3', 'dept_id_event_name_1_shift-2', 'cat_id_event_name_1_shift-2', 'dept_id_event_name_1_shift-1', 'cat_id_event_name_1_shift-1', 'dept_id_event_name_1_shift1', 'cat_id_event_name_1_shift1', 'dept_id_event_name_1_shift2', 'cat_id_event_name_1_shift2', 'dept_id_event_name_1_shift3', 'cat_id_event_name_1_shift3']\n",
      "['roll_28_std', 'roll_28_skew', 'roll_28_mean', 'roll_7_max', 'roll_7_min', 'roll_7_mean', 'roll_56_std', 'roll_56_skew', 'diff_std_28_1', 'diff_mean_28_1', 'diff_std_28_7', 'diff_mean_28_7', 'diff_std_7_1', 'diff_mean_7_1', 'diff_std_7_7', 'diff_mean_7_7', 'shift_1', 'shift_2', 'shift_3']\n",
      "['id', 'dept_id', 'cat_id', 'state_id', 'store_id']\n",
      "PREPROCESSING  TIME :  2.047  \n",
      "\n",
      "(1104840, 47)\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[500]\ttraining's rmse: 0.563778\tvalid_1's rmse: 0.618541\n",
      "Early stopping, best iteration is:\n",
      "[419]\ttraining's rmse: 0.564662\tvalid_1's rmse: 0.618097\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[500]\ttraining's rmse: 0.574403\tvalid_1's rmse: 0.567823\n",
      "Early stopping, best iteration is:\n",
      "[800]\ttraining's rmse: 0.571469\tvalid_1's rmse: 0.567263\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[97]\ttraining's rmse: 0.570763\tvalid_1's rmse: 0.658363\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[375]\ttraining's rmse: 0.581864\tvalid_1's rmse: 0.535093\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[113]\ttraining's rmse: 0.584049\tvalid_1's rmse: 0.558368\n",
      "\n",
      "\n",
      "Wall time: 3min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\n",
    "path = '/Users/abcdm/Downloads/m5-forecasting-accuracy/'\n",
    "#path = '../input/m5-forecasting-accuracy/'\n",
    "\n",
    "#d_cols=[i+1 for i in range(1941)]\n",
    "d_cols=[i+1 for i in range(1913)]\n",
    "sub_df, importance=sub_cycle(path=path, d_cols=d_cols[-1000:],stop=2, test=False)\n",
    "#sub_df, importance = sub_cycle(path=path, d_cols=d_cols[-1000:], test=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(path+'sales_train_validation.csv')\n",
    "sub_df = pd.merge(\n",
    "    train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd_1886']],\n",
    "    sub_df.drop('d', axis=1),on=['id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d_1886</th>\n",
       "      <th>y_pred_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>1.637003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>1</td>\n",
       "      <td>0.782966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_008</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>2.760855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_016_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_016</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>2</td>\n",
       "      <td>3.066590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_017_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_017</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>0.543811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5935</th>\n",
       "      <td>FOODS_3_816_WI_3_validation</td>\n",
       "      <td>FOODS_3_816</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>3</td>\n",
       "      <td>5.113096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5936</th>\n",
       "      <td>FOODS_3_818_WI_3_validation</td>\n",
       "      <td>FOODS_3_818</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>5</td>\n",
       "      <td>1.295695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5937</th>\n",
       "      <td>FOODS_3_819_WI_3_validation</td>\n",
       "      <td>FOODS_3_819</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>2</td>\n",
       "      <td>1.242808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5938</th>\n",
       "      <td>FOODS_3_820_WI_3_validation</td>\n",
       "      <td>FOODS_3_820</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>1</td>\n",
       "      <td>1.070883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5939</th>\n",
       "      <td>FOODS_3_821_WI_3_validation</td>\n",
       "      <td>FOODS_3_821</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>FOODS</td>\n",
       "      <td>WI_3</td>\n",
       "      <td>WI</td>\n",
       "      <td>0</td>\n",
       "      <td>0.572645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5940 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 id        item_id    dept_id   cat_id  \\\n",
       "0     HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES   \n",
       "1     HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES   \n",
       "2     HOBBIES_1_008_CA_1_validation  HOBBIES_1_008  HOBBIES_1  HOBBIES   \n",
       "3     HOBBIES_1_016_CA_1_validation  HOBBIES_1_016  HOBBIES_1  HOBBIES   \n",
       "4     HOBBIES_1_017_CA_1_validation  HOBBIES_1_017  HOBBIES_1  HOBBIES   \n",
       "...                             ...            ...        ...      ...   \n",
       "5935    FOODS_3_816_WI_3_validation    FOODS_3_816    FOODS_3    FOODS   \n",
       "5936    FOODS_3_818_WI_3_validation    FOODS_3_818    FOODS_3    FOODS   \n",
       "5937    FOODS_3_819_WI_3_validation    FOODS_3_819    FOODS_3    FOODS   \n",
       "5938    FOODS_3_820_WI_3_validation    FOODS_3_820    FOODS_3    FOODS   \n",
       "5939    FOODS_3_821_WI_3_validation    FOODS_3_821    FOODS_3    FOODS   \n",
       "\n",
       "     store_id state_id  d_1886  y_pred_1  \n",
       "0        CA_1       CA       0  1.637003  \n",
       "1        CA_1       CA       1  0.782966  \n",
       "2        CA_1       CA       0  2.760855  \n",
       "3        CA_1       CA       2  3.066590  \n",
       "4        CA_1       CA       0  0.543811  \n",
       "...       ...      ...     ...       ...  \n",
       "5935     WI_3       WI       3  5.113096  \n",
       "5936     WI_3       WI       5  1.295695  \n",
       "5937     WI_3       WI       2  1.242808  \n",
       "5938     WI_3       WI       1  1.070883  \n",
       "5939     WI_3       WI       0  0.572645  \n",
       "\n",
       "[5940 rows x 8 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
