{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Library","execution_count":null},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport numpy as np\nimport pandas as pd\nimport datetime\nfrom catboost import CatBoostClassifier\nimport lightgbm as lgb\nfrom time import time\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom scipy import stats\nimport gc, pickle\nimport ast\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, log_loss\nfrom sklearn.linear_model import Ridge,Lasso, BayesianRidge\nfrom sklearn.svm import LinearSVR\nfrom sklearn.preprocessing import minmax_scale","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def create_is_sell_data(sell_prices_df, calendar_df, train_df):\n    sell_prices_df['id'] = sell_prices_df['item_id'].astype('str')+'_'+sell_prices_df['store_id']+'_validation'\n    sell_prices_data = sell_prices_df[sell_prices_df.wm_yr_wk.isin(calendar_df.wm_yr_wk.unique())]\n    sell_prices_data.reset_index(drop=True, inplace=True)\n    tmp = sell_prices_data.groupby(['id'])[['wm_yr_wk', 'sell_price']].apply(\n        lambda x: x.set_index('wm_yr_wk')['sell_price'].to_dict()\n    ).to_dict()\n    d = calendar_df.d\n    wm_yr_wk = calendar_df.wm_yr_wk\n    price_data = {}\n    for col in tqdm(train_df.id.unique()):\n        price_data[col] = wm_yr_wk.map(tmp[col])\n    price_data = pd.DataFrame(price_data)\n    price_data.index = d\n    is_sell = price_data.notnull().astype(float).T\n    price_data = price_data.fillna(0)\n    \n    is_sell.index=train_df.id\n    train_df.index=train_df.id\n    is_sell = pd.concat([\n        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']], is_sell\n    ], axis=1)\n    price_data = pd.concat([\n        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']], price_data.T  \n    ], axis=1)\n    \n    return price_data, is_sell\n\ndef set_index(df, name):\n    d = {}\n    for col, value in df.iloc[0,:].items():\n        if type(col)==str:\n            if type(df[col].values[0])!=str:\n                v = 'd'\n            else:\n                v='id'\n        else:\n            v=name\n        d[col]=v\n    return d\n\ndef dcol2int(col):\n    if col[:2]=='d_':\n        return int(col.replace('d_', ''))\n    else:\n        return col\n    \ndef create_event_data(train_df, calendar_df):\n    new_df = pd.DataFrame()\n    D_COLS = [d for d in train_df.columns if type(d)!=str]\n    for event_name in ['event_name_1', 'event_name_2']:\n        tmp_df = pd.concat([\n            train_df.groupby(['dept_id'])[D_COLS].mean().T.astype(float),\n            train_df.groupby(['cat_id'])[D_COLS].mean().T.astype(float),\n            calendar_df.loc[D_COLS,event_name].replace(np.nan, 'NAN')\n        ],axis=1)\n\n        dept_id_cols = train_df.dept_id.unique().tolist()\n        cat_id_cols = train_df.cat_id.unique().tolist()\n\n        tmp_df = pd.concat([\n            tmp_df[[event_name]],\n            tmp_df.groupby([event_name])[dept_id_cols].transform(\n            lambda x: x.shift(1).rolling(len(x), min_periods=1).mean()\n            ),\n            tmp_df.groupby([event_name])[cat_id_cols].transform(\n            lambda x: x.shift(1).rolling(len(x), min_periods=1).mean()\n            )\n        ], axis=1)\n\n        tmp_df[dept_id_cols] = tmp_df[dept_id_cols]/tmp_df[dept_id_cols].rolling(56, min_periods=1).mean().shift(1)\n        tmp_df[cat_id_cols] = tmp_df[cat_id_cols]/tmp_df[cat_id_cols].rolling(56, min_periods=1).mean().shift(1)\n        tmp_df.loc[tmp_df[event_name]=='NAN', dept_id_cols+cat_id_cols]=1\n        \n        tmp_df.columns=[f'{event_name}_{col}' for col in tmp_df.columns]\n        \n        new_df = pd.concat([\n            new_df, tmp_df\n        ] ,axis=1)\n    new_df.index=D_COLS\n    return new_df","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def create_metadata(path, d_cols, submmit=True):\n    train_df = pd.read_csv(path+'sales_train_validation.csv')\n    calendar_df = pd.read_csv(path+'calendar.csv')\n    sell_prices_df = pd.read_csv(path+'sell_prices.csv')\n    sample_submission_df = pd.read_csv(path+'sample_submission.csv')\n\n    calendar_df['d'] = calendar_df.d.str.replace('d_', '').astype(int)\n    cols = train_df.columns\n    cols = [dcol2int(col) for col in cols]\n    train_df.columns=cols\n    calendar_df['date']=pd.to_datetime(calendar_df.date)\n    calendar_df.index = calendar_df.d\n    price_data, is_sell = create_is_sell_data(sell_prices_df, calendar_df, train_df)\n    \n    str_cols = [ col for col in train_df.columns if 'id' in str(col)]\n    new_columns = str_cols+d_cols\n    train_df = train_df.reindex(columns=new_columns)\n    \n    \n    train_df = pd.concat([\n        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']],\n        train_df.loc[:,d_cols]+is_sell[d_cols].replace(0, np.nan).replace(1, 0)\n    ], axis=1)\n    train_df.index = train_df.id\n    del is_sell;gc.collect()\n    \n    df = train_df.loc[:,d_cols].T.astype(float)\n    a = df.loc[d_cols[28:-56]].rolling(28, min_periods=1).sum().replace(0,np.nan)+df.loc[d_cols[28:-56]][::-1].rolling(28, min_periods=1).sum()[::-1].replace(0,np.nan)\n    a[a.notnull()]=0\n    df.loc[d_cols[28:-56]] += a\n    df = df.loc[d_cols,:].T.astype(float)\n    del a;gc.collect()\n    \n    #snap_data\n    snap_data = calendar_df[['snap_CA', 'snap_WI', 'snap_TX', 'd']]\n    snap_data.set_index('d', inplace=True)\n    \n    #dept_id_price\n    dept_id_price = price_data[d_cols]/price_data.groupby(['dept_id', 'store_id'])[d_cols].transform('mean')\n    dept_id_price = dept_id_price.T.astype(float)\n    #dept_id_price['d'] = dept_id_price.index\n    dept_id_price = dept_id_price.replace(0,np.nan)\n    \n    #cat_id_price\n    cat_id_price = price_data[d_cols]/price_data.groupby(['cat_id', 'store_id'])[d_cols].transform('mean')\n    cat_id_price = cat_id_price.T.astype(float)\n    #cat_id_price['d'] = cat_id_price.index\n    cat_id_price = cat_id_price.replace(0,np.nan)\n    \n    #price_data\n    price_data = price_data[d_cols].T\n    price_data.replace(0,np.nan, inplace=True)\n    #price_data['d']=price_data.index\n    \n    #event_df\n    event_df = create_event_data(train_df, calendar_df)\n    #event_df.reset_index(inplace=True)\n    \n    #calendar_dict\n    calendar_dict = calendar_df[['wday', 'month']].to_dict()\n    \n    return train_df, snap_data, dept_id_price, cat_id_price, price_data, event_df, calendar_dict, df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# feature engineering","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def make_roll_data(data, win, agg={'mean', 'std'}):\n    data_2 = data.groupby(['id'])['TARGET'].apply(\n        lambda x:\n        x.shift(1).rolling(win, min_periods=1).agg(agg)\n    )\n    data_2.columns=[f'roll_{win}_{col}' for col in data_2.columns]\n    data = pd.concat([\n        data, data_2\n    ], axis=1)\n    return data\n\ndef make_diff_data(data, win):\n    diff_data = data.groupby(['id'])['TARGET'].apply(\n        lambda x:\n        abs(x.shift(1).diff(1)).rolling(win, min_periods=1).agg({'mean', 'std'})\n    ) \n    diff_data.columns=[f'diff_{col}_{win}_1' for col in diff_data.columns]\n    data = pd.concat([\n        data, diff_data\n    ], axis=1)\n    \n    diff_data = data.groupby(['id'])['TARGET'].apply(\n        lambda x:\n        abs(x.shift(1).diff(7)).rolling(win, min_periods=1).agg({'mean', 'std'})\n    ) \n    diff_data.columns=[f'diff_{col}_{win}_7' for col in diff_data.columns]\n    data = pd.concat([\n        data, diff_data\n    ], axis=1)\n    return data\n\ndef make_shift_data(data):\n    shift=7\n    for i, p in  enumerate([0,7]):\n        data[f'shift_{i+1}'] = data.groupby(['id'])['TARGET'].shift(shift+p)\n    data['shift_3'] = data[['shift_1', 'shift_2']].mean(1)\n    return data\n\ndef preprocessing(path,d_cols,test):\n    \"\"\"\n    if test:\n        train_df = pd.read_csv(path+'train_df_short.csv')\n    else:\n        train_df = pd.read_csv(path+'train_df.csv')\n    train_df.columns= [int(col) if col.isnumeric() else str(col) for col in train_df.columns]\n    \"\"\"\n    train_df, snap_data, dept_id_price, cat_id_price, price_data, event_df, calendar_dict, df = create_metadata(path, d_cols)\n    if test:\n        train_df = train_df[train_df.id.isin(train_df.id.unique()[:2000])]\n    data = pd.concat([\n        train_df[['id', 'dept_id', 'store_id']],\n        df[d_cols[:-28]].isnull().sum(axis=1),\n        df[d_cols[:-28]].mean(1)\n    ],axis=1)\n    data.columns=['id', 'dept_id', 'store_id', 'null_num_600', 'sell_mean']\n    data['sell_mean_null_600'] = data['sell_mean']/data['null_num_600']\n    data = data.sort_values('sell_mean_null_600', ascending=False)#.index.tolist()\n    \n    ids = []\n    for dept in ['HOBBIES_1', 'HOBBIES_2', 'HOUSEHOLD_1', 'HOUSEHOLD_2', 'FOODS_1', 'FOODS_2', 'FOODS_3']:\n        ids += data[data.dept_id==dept][:500].index.tolist()\n    for store in ['CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']:\n        ids += data[data.store_id==store][:500].index.tolist()\n    ids += data.index.tolist()[:2000]\n    ids = np.unique(ids).tolist()\n    ids_2 = data[~data.id.isin(ids)].id.unique().tolist()\n    gc.collect()\n    \n    print('len  ids  ', len(ids))\n    print('len  ids_2  ', len(ids_2))\n    \n    data = pd.concat([\n        train_df[train_df.id.isin(ids)][d_cols[-200:]].stack(dropna=False).reset_index(),\n        train_df[train_df.id.isin(ids_2)][d_cols[-100:]].stack(dropna=False).reset_index()\n        ], axis=0)\n    data = data.rename(columns=set_index(data, 'TARGET'))\n    data.sort_values('d', inplace=True)\n    data.reset_index(drop=True, inplace=True)\n    data = reduce_mem_usage(data)\n    gc.collect()\n\n\n    for key, value in train_df[['dept_id', 'cat_id', 'state_id', 'store_id']].to_dict().items():\n        data[key] = data.id.map(value)\n    \n    \n    #snap_data = pd.read_csv(path+'snap_data.csv')\n    #snap_data.index=snap_data.d\n    #snap_data.drop('d',axis=1, inplace=True)\n    \n    data[f'snap']=0\n    for key, value in snap_data.to_dict().items():\n        k = key.replace('snap_', '')\n        data.loc[data.state_id==k,'snap'] = data.loc[data.state_id==k, 'd'].map(value).fillna(0)\n    for shift in [-3,-2,-1,1,2,3]:\n        data[f'snap_{shift}'] = data.groupby(['id'])['snap'].shift(shift).fillna(0)\n\n\n    #dept_id_price = pd.read_csv(path+'dept_id_price.csv')\n    #cat_id_price = pd.read_csv(path+'cat_id_price.csv')\n    #dept_id_price.index=dept_id_price.d\n    #cat_id_price.index=cat_id_price.d\n    #dept_id_price = dept_id_price[dept_id_price.d.isin(data.d.unique())].drop('d', axis=1)\n    #cat_id_price = cat_id_price[cat_id_price.d.isin(data.d.unique())].drop('d', axis=1)\n\n    dept_id_price = dept_id_price.stack(dropna=False).reset_index()\n    cat_id_price = cat_id_price.stack(dropna=False).reset_index()\n\n    dept_id_price.rename(columns=set_index(dept_id_price, 'dept_id_price'), inplace=True)\n    cat_id_price.rename(columns=set_index(cat_id_price, 'cat_id_price'), inplace=True)\n\n    data = pd.merge(\n        data, dept_id_price, on=['d', 'id'], how='left'\n    )\n    data = pd.merge(\n        data, cat_id_price, on=['d', 'id'], how='left'\n    )\n\n\n    del dept_id_price,cat_id_price;gc.collect()\n\n    #price_df = pd.read_csv(path+'price_data.csv')\n    #price_df.index=price_df.d\n    #price_df = price_df[price_df.d.isin(data.d.unique())].drop('d', axis=1)\n    price_data = price_data.stack(dropna=False).reset_index()\n    price_data.rename(columns=set_index(price_data, 'price'), inplace=True)\n    data = pd.merge(\n        data, price_data, on=['d', 'id'], how='left'\n    )\n    del price_data;gc.collect()\n\n    #with open(path+'calendar_dict.pkl', 'rb') as f:\n     #   calendar_dict = pickle.load(f)\n    for key, value in calendar_dict.items():\n        data[key] = data.d.map(value)\n    del calendar_dict;gc.collect()\n\n    #event_df = pd.read_csv(path+'event_df.csv')\n    #event_df.index=event_df['index']\n    #event_df.drop('index', axis=1, inplace=True)\n\n    tmp_dic = event_df.to_dict()\n    data[f'dept_id_event_name_1']=1\n    data[f'cat_id_event_name_1']=1\n    for key, value in tmp_dic.items():\n        if 'event_name_1' in key:\n            if key[13:] in train_df.dept_id.unique().tolist():\n                data.loc[data.dept_id==key[13:], f'dept_id_{key[:12]}']=data.loc[data.dept_id==key[13:], 'd'].map(value).fillna(1)\n            if key[13:] in train_df.cat_id.unique().tolist():\n                data.loc[data.cat_id==key[13:], f'cat_id_{key[:12]}']=data.loc[data.cat_id==key[13:], 'd'].map(value).fillna(1)\n    for shift in [-3,-2,-1,1,2,3]:\n        for event_name in ['dept_id_event_name_1', 'cat_id_event_name_1']:\n            data[f'{event_name}_shift{shift}'] = data.groupby(['id'])[event_name].shift(shift).fillna(1)\n\n    cols = data.columns.tolist()\n    print(cols)\n\n    data = make_roll_data(data=data,win=28,agg={'mean', 'std', 'skew'})\n    data = make_roll_data(data=data,win=7,agg={'mean', 'min', 'max'})\n    data = make_roll_data(data,win=56,agg={'std', 'skew'})\n    data = make_diff_data(data=data, win=28)\n    data = make_diff_data(data=data, win=7)\n    data = make_shift_data(data=data)\n    gc.collect()\n\n    print([col for col in data.columns if not col in cols])\n    \n    categories = [c for c in data.columns if data[c].dtype==object]\n    print(categories)\n    for c in categories:\n        if c=='id':\n            pass\n        else:\n            data[c] = pd.factorize(data[c])[0]\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### reduce_mem_usage","execution_count":null},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                       df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### DF shift ","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def shift_seven(data, cols):\n    #['shift_1', 'shift_2', 'shift_3', 'shift_4']\n    data[cols] = data.groupby(['id'])[cols].shift(7)\n    return data\n    \ndef shift_one(data,cols):\n    #['roll_28_std', 'roll_28_mean', 'diff_std_1', 'diff_mean_1', 'diff_std_7', 'diff_mean_7']\n    data[cols]=data.groupby(['id'])[cols].shift(1)\n    return data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# lgb model utils","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"PARAMS = {\n    'n_estimators':2000,\n    'boosting_type': 'gbdt',\n    \n    'objective': 'poisson',\n    'metric': 'rmse',\n    \n    'subsample': 0.75,\n    'subsample_freq': 1,\n    'learning_rate': 0.07,\n    'feature_fraction': 0.85,\n    'max_depth': 15,\n    'lambda_l1': 1,  \n    'lambda_l2': 1,\n    'verbose': 100,\n    'random_state':123\n}\n\n\ndef plot_importance(models, col, name):\n    importances = np.zeros(len(col))\n    for model in models:\n        importances+=model.feature_importance(importance_type='gain')\n    importance = pd.DataFrame()\n    importance['importance'] = importances\n    importance['importance'] = minmax_scale(importance.importance)\n    importance['col'] = col\n    importance.to_csv(f'importance_{name}.csv',index=False)\n    return importance\n    \ndef run_nest_cv(x_train, y_train, trn_df, params=PARAMS):\n    models = []\n    k = GroupKFold(n_splits=5)\n    trn_df['y_pred'] = 0\n    \n    for trn_indx, val_indx in k.split(x_train[['dept_id']],groups=x_train['dept_id']):\n        train_set = lgb.Dataset(x_train.loc[trn_indx,:], y_train.loc[trn_indx])\n        val_set = lgb.Dataset(x_train.loc[val_indx,:], y_train.loc[val_indx])\n        \n        categories = ['cat_id', 'dept_id', 'store_id']\n        \n        model = lgb.train(\n            train_set=train_set, \n            valid_sets=[train_set, val_set],\n            params=params, num_boost_round=3000, early_stopping_rounds=100, verbose_eval=500,\n            categorical_feature=categories+['wday', 'month']\n        )\n        \n        models.append(model)\n        trn_df.loc[val_indx, 'y_pred']=np.e**(model.predict(x_train.loc[val_indx,:]))-1\n        gc.collect()\n        \n    return models, trn_df\n\ndef predict_cv(x_val, models):\n    preds = np.zeros(len(x_val))\n    for model in models:\n        pred = model.predict(x_val)\n        pred = np.e**pred - 1\n        preds+=pred/len(models)\n    return preds\n\ndef show_eval_score(preds, val_df):\n    val_df['y_pred'] = preds\n    score= np.sqrt(mean_squared_error(val_df['TARGET'], preds))\n    print(\"EVALUATION SCORE : \", score)\n    return val_df\n\ndef split_data(data, trn_day, val_day):\n    data = data[data.shift_2.notnull()]\n    \n    y = data[['d', 'id', 'TARGET']]\n    X = data.drop(columns=['id',  'TARGET','state_id']).astype(float)\n    \n    x_train, x_val = X[X.d.isin(trn_day)], X[X.d.isin(val_day)]\n    y_train, y_val = y[y.d.isin(trn_day)], y[y.d.isin(val_day)]\n    \n    x_train.reset_index(drop=True,inplace=True)\n    x_val.reset_index(drop=True,inplace=True)\n    y_train.reset_index(drop=True,inplace=True)\n    y_val.reset_index(drop=True,inplace=True)\n    trn_df = y_train[['id', 'd', 'TARGET']]\n    val_df = y_val[['id', 'd', 'TARGET']]\n    y_train['TARGET'] = np.log1p(y_train['TARGET'])\n    \n    x_train.drop('d', axis=1, inplace=True)\n    x_val.drop('d', axis=1, inplace=True)\n    y_train = y_train['TARGET'].astype(float)\n    return x_train, x_val, y_train, trn_df, val_df\n\ndef train(data):\n    split=28\n    data = data[data.TARGET.notnull()]\n    d_cols = sorted(data.d.unique())\n    trn_day = d_cols[:-split]\n    val_day = d_cols[-split:]\n\n    x_train, x_val, y_train, trn_df, val_df = split_data(data, trn_day, val_day)\n    print(x_train.shape, x_val.shape)\n    models, trn_df = run_nest_cv(x_train, y_train, trn_df)\n    preds = predict_cv(x_val, models)\n    val_df = show_eval_score(preds, val_df)\n    plot_importance(models, x_train.columns)\n    return models, val_df, trn_df\n\ndef split_data_for_sub(data):\n    data = data[data.TARGET.notnull()]\n    data = data[data.shift_2.notnull()]\n    data = data[data.diff_std_7_1.notnull()]\n    trn_df = data[['id', 'd', 'TARGET']]\n    y = np.log1p(data['TARGET']).astype(float)\n    X = data.drop(columns=['id','d', 'TARGET','state_id']).astype(float)\n    X.reset_index(drop=True, inplace=True)\n    y.reset_index(drop=True, inplace=True)\n    trn_df.reset_index(drop=True, inplace=True)\n    return X, y, trn_df\n\ndef train_sub_predict(data, for_predict):\n    train_d_cols = data.d.unique().tolist()\n    predict_day=train_d_cols[-28:][for_predict-1]\n    sub_predict_data = data[data.d==predict_day]\n    X, y, trn_df = split_data_for_sub(data)\n    print(X.shape)\n    models, trn_df = run_nest_cv(X, y, trn_df)\n    plot_importance(models, X.columns, str(for_predict))\n    preds = predict_cv(sub_predict_data[X.columns], models)\n    \n    sub_df = sub_predict_data[['id', 'd', 'TARGET']]\n    sub_df[f'y_pred'] = preds\n    return trn_df, sub_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# linear model utils","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def to_onehot_data(data):\n    data = data.drop(columns=['state_id'])\n    category = ['cat_id', 'dept_id', 'store_id', 'month', 'wday']\n    one_hot_cols=[]\n    for cat in category:\n        one_hot_data = pd.get_dummies(data[cat]).rename(columns={i:f'{cat}_{int(i)}' for i in data[cat].unique()})\n        one_hot_cols+=one_hot_data.columns.tolist()\n        data = pd.concat([\n            data.drop(cat, axis=1),\n            one_hot_data\n        ], axis=1)\n    return data, one_hot_cols\n\ndef data_split_lin(data, trn_days, val_days):\n    data.dropna(0, inplace=True)\n    train = data[data.d.isin(trn_days)]\n    val = data[data.d.isin(val_days)]\n    train.reset_index(drop=True, inplace=True)\n    val.reset_index(drop=True, inplace=True)\n    trn_df = train[['id', 'd', 'TARGET']]\n    val_df = val[['id', 'd', 'TARGET']]\n    return train, val, trn_df, val_df\n\ndef linear_cv(data, trn_df):\n    k = StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n    models={}\n    models['ridge'] = []\n    models['lasso'] = []\n    data.reset_index(drop=True, inplace=True)\n    X = data.drop(columns=['id', 'd', 'TARGET'])\n    y = data['TARGET']\n    data['ridge_preds'] = 0\n    data['lasso_preds'] = 0\n    for trn_indx, val_indx in k.split(data,y=y):\n        \n        ridge = Ridge()\n        lasso = Lasso()\n    \n        ridge.fit(X.loc[trn_indx,:],y.loc[trn_indx])\n        lasso.fit(X.loc[trn_indx,:],y.loc[trn_indx])\n        models['ridge'].append(ridge)\n        models['lasso'].append(lasso)\n        \n        trn_df.loc[val_indx, 'ridge_preds'] = ridge.predict(X.loc[val_indx,:])\n        trn_df.loc[val_indx, 'lasso_preds'] = lasso.predict(X.loc[val_indx,:])\n    \n    return models, trn_df\n\ndef cv_predict_lin(data, models):\n    preds = np.zeros(len(data))\n    for model in models:\n        preds+=model.predict(data.drop(columns=['id', 'd', 'TARGET'])) /len(models)\n    return preds\n    \ndef linear_predict(models, X, val_df):\n    for name, _models in models.items():\n        val_df[f'{name}_preds'] = cv_predict_lin(X, _models)\n    return val_df\n\ndef train_lin(data, trn_days, val_days):\n    X = to_onehot_data(data)\n    train, val, trn_df, val_df = data_split_lin(X, trn_days, val_days)\n    print(train.shape, val.shape)\n    models, trn_df = linear_cv(train, trn_df)\n    val_df = linear_predict(models, val, val_df)\n    return val_df, trn_df\n\ndef train_lin_sub(data, for_predict):\n    predict_day = data.d.unique()[-28:][for_predict-1]\n    X, one_hot_cols = to_onehot_data(data)\n    predict_sub_df = X[X.d==predict_day]\n    \n    X.dropna(0, inplace=True)\n    X.reset_index(drop=True, inplace=True)\n    predict_sub_df.reset_index(drop=True, inplace=True)\n\n    trn_df = X[['id', 'd', 'TARGET']+one_hot_cols]\n    val_df = predict_sub_df[['id', 'd', 'TARGET']+one_hot_cols]\n\n    models, trn_df = linear_cv(X, trn_df)\n    val_df = linear_predict(models, predict_sub_df, val_df)\n    return val_df, trn_df, one_hot_cols","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVM, BayesianRidge","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def cv_bayesianRidge(trn_df ,val_df, one_hot_cols):\n    k = StratifiedKFold(n_splits=5, shuffle=True, random_state=2020)\n    X = trn_df[trn_df.ridge_preds.notnull()][['id', 'd','y_pred','ridge_preds', 'lasso_preds']+one_hot_cols]\n    y = trn_df[trn_df.ridge_preds.notnull()]['TARGET']\n    X['br_pred'] = 0\n    val_df['br_pred'] = 0\n    X['br_std'] = 0\n    val_df['br_std'] = 0\n    X.reset_index(drop=True, inplace=True)\n    y.reset_index(drop=True, inplace=True)\n    for trn_indx, val_indx in k.split(X,y=y):\n        \n        br = BayesianRidge()\n        br.fit(X.loc[trn_indx,['y_pred','ridge_preds', 'lasso_preds']+one_hot_cols], y.loc[trn_indx])\n        p, _std= br.predict(X.loc[val_indx,['y_pred','ridge_preds', 'lasso_preds']+one_hot_cols], return_std=True)\n        X.loc[val_indx, 'br_pred'] = p \n        X.loc[val_indx, 'br_std'] = _std\n        p, _std= br.predict(val_df[['y_pred','ridge_preds', 'lasso_preds']+one_hot_cols], return_std=True)\n        val_df['br_pred'] += p/5\n        val_df['br_std'] += _std/5\n    trn_df = pd.merge(trn_df, X[['id', 'd','br_pred', 'br_std']], how='outer', on=['id', 'd'])\n    return trn_df, val_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PipeLine","execution_count":null},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def train_cv_pipeline(data):\n    models, val_df, trn_df = train(data)\n    \n    lin_data = data.drop(columns=['roll_28_skew', 'roll_56_skew'])\n    val_df_lin, trn_df_lin = train_lin(lin_data, trn_df.d.unique().tolist(), val_df.d.unique().tolist())\n    \n    val_df = pd.merge(val_df, val_df_lin, how='outer', on=['id', 'TARGET', 'd'])\n    trn_df = pd.merge(trn_df, trn_df_lin, how='outer', on=['id', 'TARGET', 'd'])\n\n    return val_df, trn_df\n\ndef predict_sub_pipeline(data, for_predict):\n    trn_df, sub_df = train_sub_predict(data, for_predict)\n    \n    lin_data = data.drop(columns=['roll_28_skew', 'roll_56_skew'])\n    sub_df_lin, trn_df_lin, one_hot_cols = train_lin_sub(lin_data, for_predict)\n    \n    sub_df = pd.merge(sub_df, sub_df_lin, how='outer', on=['id', 'TARGET', 'd'])\n    trn_df = pd.merge(trn_df, trn_df_lin, how='outer', on=['id', 'TARGET', 'd'])\n\n    return sub_df,trn_df,one_hot_cols\n\n\ndef all_predict_run(data, for_predict):\n    print(F\"\"\"\n    ###################################\n               TRAIN {for_predict}\n    ###################################\n    \"\"\")\n    \n    s = time()\n    sub_df,trn_df,one_hot_cols = predict_sub_pipeline(data, for_predict)\n    trn_df, sub_df = cv_bayesianRidge(trn_df ,sub_df, one_hot_cols)\n    #trn_df.to_csv(f'trn_{for_predict}.csv', index=False)\n    sub_df[['id', 'd', 'TARGET', 'y_pred','br_pred', 'br_std']].to_csv(f'sub_{for_predict}.csv', index=False)\n    s = (time()-s)/60\n    print(f'TIME :  {s :.4}  \\n')\n    \ndef sub_cycle(path, d_cols, stop=None, test=False):\n    data = preprocessing(path,d_cols,test=test)\n    \n    shift_seven_cols = ['shift_1', 'shift_2', 'shift_3']\n    shift_one_cols = ['roll_28_skew', 'roll_28_mean', 'roll_28_std', 'roll_7_max',\n                      'roll_7_mean', 'roll_7_min', 'roll_56_skew', 'roll_56_std', \n                      'diff_mean_28_1', 'diff_std_28_1', 'diff_mean_28_7', 'diff_std_28_7',\n                      'diff_mean_7_1', 'diff_std_7_1', 'diff_mean_7_7', 'diff_std_7_7']\n    \n    all_predict_run(data=data, for_predict=1)\n    print('\\n')\n    \n    if stop is not None:\n        for for_predict in range(2,stop):\n            if (for_predict-1)%7==0:\n                data = shift_seven(data, shift_seven_cols)\n            data = shift_one(data, shift_one_cols)\n            all_predict_run(data=data, for_predict=for_predict)\n    else:\n        for for_predict in range(2,29):\n            if (for_predict-1)%7==0:\n                data = shift_seven(data, shift_seven_cols)\n            data = shift_one(data, shift_one_cols)\n            all_predict_run(data=data, for_predict=for_predict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"%%time\n#path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\n#path = '/Users/abcdm/Downloads/m5-forecasting-accuracy/'\npath = '../input/m5-forecasting-accuracy/'\n\nd_cols=[i+1 for i in range(1941)]\n#sub_cycle(path=path, d_cols=d_cols[-1000:],stop=2, test=False)\nsub_cycle(path=path, d_cols=d_cols[-1000:], test=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}