{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os, gc\n",
    "import termcolor\n",
    "\n",
    "import math, random\n",
    "import pickle\n",
    "import datetime, time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "RANDOM_SEED = 2020\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\"\"\"\n",
    "in_size=56\n",
    "model = Conv_1d_Net(in_size)\n",
    "\"\"\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_X_y(x_batch):\n",
    "    y_batch = x_batch[:,0,-28:]\n",
    "    x_batch = x_batch[:,:,:-28]\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_groupId(model, df, cols, index_index):\n",
    "    lr = 1e-4\n",
    "    eta_min = 1e-3\n",
    "    t_max = 10\n",
    "    model = model.to(DEVICE)\n",
    "    criterion = Loss_func_item_id_state_id_(cols=cols, df=df, index_index=index_index)\n",
    "    optimizer = RAdam(params=model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max, eta_min=eta_min)\n",
    "    return model, criterion, optimizer, scheduler\n",
    "\n",
    "def make_data_group_id(train_cols, state, train_df, calendar_data, price_data, is_sell_data, sample_submission_df, group_id):\n",
    "    \n",
    "    group_sum_df = train_df.groupby(group_id)[train_cols].transform('sum')\n",
    "    \n",
    "    data_train = train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']+train_cols]\n",
    "    train_product = sample_submission_df[(sample_submission_df.id.str.contains(state))&(sample_submission_df.id.str.contains('_validation'))].id.values\n",
    "    #train_product = data_train[data_train.state_id==state]['id'].unique()\n",
    "    \n",
    "    data = data_train.loc[train_product,train_cols]\n",
    "    group_sum_df = group_sum_df.loc[train_product, :]\n",
    "    \n",
    "    \n",
    "    calendar_index = [ f'snap_{state}']\n",
    "    event_index = [ f'snap_{state}']\n",
    "    calendar = calendar_data.loc[calendar_index,:]\n",
    "    for shift in [3, 7, 14, 28]:\n",
    "        tmp_calendar = calendar.loc[event_index, :]\n",
    "        tmp_calendar = tmp_calendar.T.shift(shift).T\n",
    "        tmp_calendar.index = [f'{col}_shift{shift}' for col in tmp_calendar.index]\n",
    "        calendar = pd.concat([\n",
    "            calendar,\n",
    "            tmp_calendar\n",
    "        ], axis=0)\n",
    "    calendar = calendar[train_cols]\n",
    "    \n",
    "    price = price_data.T[train_cols].loc[train_product,:]\n",
    "    past_price_1 = price_data.loc[:,train_product].shift(3).T[train_cols]\n",
    "    past_price_2 = price_data.loc[:,train_product].shift(7).T[train_cols]\n",
    "    past_price_3 = price_data.loc[:,train_product].shift(14).T[train_cols]\n",
    "    \n",
    "    \n",
    "    is_sell = is_sell_data[train_cols].loc[train_product,:]\n",
    "    past_is_sell_1 = is_sell_data.T.shift(3).T.loc[train_product, train_cols]\n",
    "    past_is_sell_2 = is_sell_data.T.shift(7).T.loc[train_product, train_cols]\n",
    "    past_is_sell_3 = is_sell_data.T.shift(14).T.loc[train_product, train_cols]\n",
    "\n",
    "    data = torch.FloatTensor(data.values.astype(float))\n",
    "    group_sum_df = torch.FloatTensor(group_sum_df.values.astype(float))\n",
    "    \n",
    "    calendar = torch.FloatTensor(calendar.values.astype(float))\n",
    "    \n",
    "    price = torch.FloatTensor(price.values.astype(float))\n",
    "    \n",
    "    past_price_1 = torch.FloatTensor(past_price_1.values.astype(float))\n",
    "    past_price_2 = torch.FloatTensor(past_price_2.values.astype(float))\n",
    "    past_price_3 = torch.FloatTensor(past_price_3.values.astype(float))\n",
    "    \n",
    "    is_sell = torch.FloatTensor(is_sell.values.astype(float))\n",
    "    past_is_sell_1 = torch.FloatTensor(past_is_sell_1.values.astype(float))\n",
    "    past_is_sell_2 = torch.FloatTensor(past_is_sell_2.values.astype(float))\n",
    "    past_is_sell_3 = torch.FloatTensor(past_is_sell_3.values.astype(float))\n",
    "    \n",
    "    data_list = []\n",
    "    for idx in range(len(data)):\n",
    "        _data = data[[idx],:]\n",
    "        _group_sum_data = group_sum_df[[idx],:]\n",
    "        _price = price[[idx],:]\n",
    "        \n",
    "        _past_price_1 = past_price_1[[idx],:]\n",
    "        _past_price_2 = past_price_2[[idx],:]\n",
    "        _past_price_3 = past_price_3[[idx],:]\n",
    "        \n",
    "        _is_sell = is_sell[[idx],:]\n",
    "        \n",
    "        _past_is_sell_1 = past_is_sell_1[[idx],:]\n",
    "        _past_is_sell_2 = past_is_sell_2[[idx],:]\n",
    "        _past_is_sell_3 = past_is_sell_3[[idx],:]\n",
    "        \n",
    "        x = torch.cat((\n",
    "            _data, _group_sum_data,\n",
    "            calendar,\n",
    "            _price,\n",
    "            _past_price_1, _past_price_2, _past_price_3,\n",
    "            _is_sell,\n",
    "            _past_is_sell_1, _past_is_sell_2, _past_is_sell_3\n",
    "        ), dim=0)\n",
    "        data_list.append(x.tolist())\n",
    "    data_list = torch.FloatTensor(data_list)\n",
    "    return data_list\n",
    "\n",
    "\n",
    "def train_model_gruopId(model, data, calendar):\n",
    "    df['index'] = df.index\n",
    "    index_df = pd.concat([\n",
    "        df.groupby(['item_id', 'state_id'])['index'].unique(),\n",
    "        df.groupby(['item_id', 'state_id'])['index'].nunique()\n",
    "    ], axis=1)\n",
    "    index_df.columns=['index', 'length']\n",
    "    index_df['index'] = index_df['index'].apply(lambda x: x.tolist())\n",
    "\n",
    "    index_index = index_df.index\n",
    "    index_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    data_set=indicate_index(index_df)\n",
    "    data_loader = torch.utils.data.DataLoader(data_set, batch_size = 33, shuffle = True)\n",
    "    \n",
    "    model, criterion, optimizer, scheduler = prepare_training_item_id_state_id_(model, df, cols, index_index)\n",
    "    \n",
    "    num_epochs = 40\n",
    "    best_epoch = -1\n",
    "    best_score = 10000\n",
    "    early_stoppping_cnt = 0\n",
    "    best_model = model\n",
    "    \n",
    "    \n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for idx in tqdm(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            index = sum(index_df.iloc[idx]['index'].values.tolist(),[])\n",
    "            length = index_df.iloc[idx]['length'].values.tolist()\n",
    "            x_batch = _create_batch_data(index, data, calendar)\n",
    "            x_batch = x_batch[:,:,:-28]\n",
    "            gc.collect()\n",
    "            \n",
    "            x_batch, y_batch = split_X_y(x_batch)\n",
    "            x_batch = x_batch.to(DEVICE)\n",
    "            y_batch = y_batch.to(DEVICE)\n",
    "            \n",
    "            preds = model(x_batch)\n",
    "            \n",
    "            loss = criterion(preds.cpu(), y_batch.cpu(), idx, length, train=True)\n",
    "            loss = loss.to(DEVICE)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            avg_loss += loss.item() / len(data_loader)\n",
    "            del loss; gc.collect()\n",
    "        \n",
    "        model.eval()\n",
    "        avg_val_loss = 0.\n",
    "        \n",
    "        for idx in data_loader:\n",
    "            x_batch = _create_batch_data(index, data, calendar)\n",
    "            \n",
    "            x_batch, y_batch = split_X_y(x_batch)\n",
    "            x_batch = x_batch.to(DEVICE)\n",
    "            y_batch = y_batch.to(DEVICE)\n",
    "            \n",
    "            preds = model(x_batch)\n",
    "            loss = criterion(preds.cpu(), y_batch.cpu(), idx, length, train=False)\n",
    "            \n",
    "            avg_val_loss += loss.item() / len(data_loader)\n",
    "            del loss; gc.collect()\n",
    "            \n",
    "            \n",
    "        if best_score>avg_val_loss:\n",
    "            best_score = avg_val_loss\n",
    "            early_stoppping_cnt=0\n",
    "            best_epoch=epoch\n",
    "            best_model = model\n",
    "            elapsed = time.time() - start_time\n",
    "            p_avg_val_loss = termcolor.colored(np.round(avg_val_loss, 4),\"red\")\n",
    "            \n",
    "            print(f'Epoch {epoch} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {p_avg_val_loss} time: {elapsed:.0f}s')\n",
    "        else:\n",
    "            early_stoppping_cnt+=1\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f'Epoch {epoch} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f} time: {elapsed:.0f}s')\n",
    "        \n",
    "        if (epoch>10) and (early_stoppping_cnt>7):\n",
    "                break\n",
    "    \n",
    "    print(f'best_score : {best_score}    best_epoch : {best_epoch}')\n",
    "    #torch.save(best_score.state_dict(), 'net.pt')\n",
    "    \n",
    "    return best_model, best_score\n",
    "\n",
    "\n",
    "class indicate_index(torch.utils.data.Dataset):\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return idx\n",
    "    \n",
    "def _create_batch_data(index, data, calendar):\n",
    "    _data = data[index, :, :]\n",
    "    x = torch.tensor([])\n",
    "    for tmp_x in _data:\n",
    "        tmp_x = torch.cat((tmp_x, calendar),dim=0)\n",
    "        x = torch.cat((x,tmp_x.unsqueeze(0)), dim=0)\n",
    "    return x\n",
    "\n",
    "class Loss_func_groupid(nn.Module):\n",
    "    def __init__(self, df, cols, index_index, group_id):\n",
    "        super(Loss_func_item_id_state_id_, self).__init__()\n",
    "        \n",
    "        self.index_index = index_index\n",
    "        last_d = int(cols[-1].replace('d_', ''))\n",
    "        d_cols = df.columns[df.columns.str.startswith('d_')]\n",
    "        train_d_cols = last_d-28*2\n",
    "        self.train_d_cols = d_cols[:train_d_cols]\n",
    "        test_d_cols = last_d-28\n",
    "        self.test_d_cols = d_cols[:test_d_cols]\n",
    "        self._create_denominator(df, group_id)\n",
    "        \n",
    "    def _create_denominator(self, df, group_id):\n",
    "        g_df = df.groupby(group_id)#[d_cols].sum()\n",
    "        \n",
    "        train_value = g_df[self.train_d_cols].sum()\n",
    "        train_value = train_value.loc[self.index_index,:]\n",
    "        train_value = train_value.values\n",
    "        train_value = train_value[:,1:]-train_value[:,:-1]\n",
    "        train_value = train_value**2\n",
    "        train_value = train_value.mean(1)\n",
    "        train_value[train_value==0]=1\n",
    "        self.train_value = torch.FloatTensor(train_value)\n",
    "        \n",
    "        test_value = g_df[self.test_d_cols].sum()\n",
    "        test_value = test_value.loc[self.index_index,:]\n",
    "        test_value = test_value.values\n",
    "        test_value = test_value[:,1:]-test_value[:,:-1]\n",
    "        test_value = test_value**2\n",
    "        test_value = test_value.mean(1)\n",
    "        test_value[test_value==0]=1\n",
    "        self.test_value = torch.FloatTensor(test_value)\n",
    "        \n",
    "    def forward(self, preds, true, idx, length, train):\n",
    "        a1=0\n",
    "        a2=0\n",
    "        Loss=0\n",
    "        for i, _len in enumerate(length):\n",
    "            _idx = idx[i]\n",
    "            a2=a1+_len\n",
    "            _preds = preds[a1:a2].sum(0)\n",
    "            _true = true[a1:a2].sum(0)\n",
    "            loss = (_preds -_true)**2\n",
    "            loss = loss.mean()\n",
    "            loss = loss.squeeze()\n",
    "            if train:\n",
    "                loss = loss/self.train_value[_idx]\n",
    "            else:\n",
    "                loss = loss/self.test_value[_idx]\n",
    "            loss = torch.sqrt(loss)\n",
    "            Loss+=loss/len(length)\n",
    "            a1=a2\n",
    "        return Loss\n",
    "\n",
    "class TrainModel_groupId():\n",
    "    def __init__(self, path, group_id):\n",
    "        self.group_id = group_id\n",
    "        self.path=path\n",
    "        self.df = pd.read_csv(self.path+'sales_train_validation.csv')\n",
    "        self.calendar_df = pd.read_csv(self.path+'calendar.csv')\n",
    "        self.sell_prices_df = pd.read_csv(self.path+'sell_prices.csv')\n",
    "        self.sample_submission_df = pd.read_csv(self.path+'sample_submission.csv')\n",
    "        \n",
    "        self.d_cols = self.df.columns[self.df.columns.str.startswith('d_')].values.tolist()\n",
    "        \n",
    "        self.train_df, self.calendar_df, self.calendar_data, self.price_data, self.is_sell = Preprocessing(df, calendar_df, sell_prices_df)\n",
    "        \n",
    "        \n",
    "    def make_data_loader(self, cols):\n",
    "        self.cols = cols\n",
    "        state='CA'\n",
    "        data_ca = make_data_group_id(\n",
    "            cols, state, self.train_df, self.calendar_data, self.price_data, \n",
    "            self.is_sell, self.sample_submission_df, self.group_id\n",
    "        )\n",
    "        state='TX'\n",
    "        data_tx = make_data_group_id(\n",
    "            cols, state, self.train_df, self.calendar_data, self.price_data,\n",
    "            self.is_sell, self.sample_submission_df, self.group_id\n",
    "        )\n",
    "        state='WI'\n",
    "        data_wi = make_data_group_id(\n",
    "            cols, state, self.train_df, self.calendar_data, self.price_data,\n",
    "            self.is_sell, self.sample_submission_df, self.group_id\n",
    "        )\n",
    "\n",
    "\n",
    "        data = torch.cat(\n",
    "            (data_ca, data_tx, data_wi),\n",
    "            dim=0\n",
    "        )\n",
    "        calendar = make_calendar_data(self.calendar_data, cols)\n",
    "        del data_ca, data_tx, data_wi; gc.collect()\n",
    "        \n",
    "        self.in_size=data.size()[1]+calendar.size()[0]\n",
    "        \n",
    "        data_set=item_id_store_id_Dataset(data, calendar)\n",
    "        data_loader = torch.utils.data.DataLoader(data_set, batch_size = 200, shuffle = True)\n",
    "        \n",
    "        \n",
    "        return data_loader\n",
    "    \n",
    "    def train_model_item_id_store_id_(self, model, data_loader):\n",
    "        model, criterion, optimizer, scheduler = prepare_training(model, self.df, self.cols)\n",
    "\n",
    "        num_epochs = 40\n",
    "        best_epoch = -1\n",
    "        best_score = 10000\n",
    "        early_stoppping_cnt = 0\n",
    "        best_model = model\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            model.train()\n",
    "            avg_loss = 0.\n",
    "            #data_loader = torch.utils.data.DataLoader(data_set, batch_size = 150, shuffle = True)\n",
    "            for x_batch, idx in tqdm(data_loader):\n",
    "                optimizer.zero_grad()\n",
    "                x_batch = x_batch[:,:,:-28]; gc.collect()\n",
    "\n",
    "                x_batch, y_batch = split_X_y(x_batch)\n",
    "                x_batch = x_batch.to(DEVICE)\n",
    "                y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "                preds = model(x_batch)\n",
    "\n",
    "                loss = criterion(preds.cpu(), y_batch.cpu(), idx, train=True)\n",
    "                loss = loss.to(DEVICE)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                #scheduler.step()\n",
    "\n",
    "                avg_loss += loss.item() / len(data_loader)\n",
    "                del loss; gc.collect()\n",
    "\n",
    "            model.eval()\n",
    "            avg_val_loss = 0.\n",
    "\n",
    "            for x_batch, idx in data_loader:\n",
    "                x_batch, y_batch = split_X_y(x_batch)\n",
    "                x_batch = x_batch.to(DEVICE)\n",
    "                y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "                preds = model(x_batch)\n",
    "                loss = criterion(preds.cpu(), y_batch.cpu(), idx, train=False)\n",
    "\n",
    "                avg_val_loss += loss.item() / len(data_loader)\n",
    "                del loss; gc.collect()\n",
    "\n",
    "\n",
    "            if best_score>avg_val_loss:\n",
    "                best_score = avg_val_loss\n",
    "                early_stoppping_cnt=0\n",
    "                best_epoch=epoch\n",
    "                best_model = model\n",
    "                elapsed = time.time() - start_time\n",
    "                p_avg_val_loss = termcolor.colored(np.round(avg_val_loss, 4),\"red\")\n",
    "\n",
    "                print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {p_avg_val_loss} time: {elapsed:.0f}s')\n",
    "            else:\n",
    "                early_stoppping_cnt+=1\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f} time: {elapsed:.0f}s')\n",
    "\n",
    "            if (epoch>10) and (early_stoppping_cnt>7):\n",
    "                    break\n",
    "\n",
    "        print(f'best_score : {best_score}    best_epoch : {best_epoch}')\n",
    "        #torch.save(best_score.state_dict(), 'net.pt')\n",
    "\n",
    "        return best_model, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\n",
    "#path = '/Users/abcdm/Downloads/m5-forecasting-accuracy/'\n",
    "#path = '../input/m5-forecasting-accuracy/'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
