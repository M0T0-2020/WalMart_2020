{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Library"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport numpy as np\nimport pandas as pd\nimport datetime, random, math\nfrom catboost import CatBoostClassifier\nimport lightgbm as lgb\nfrom time import time\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom scipy import stats\nimport gc, pickle\nimport ast\nfrom typing import Union\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, log_loss\nfrom sklearn.linear_model import Ridge,Lasso, BayesianRidge\nfrom sklearn.svm import LinearSVR\nfrom sklearn.preprocessing import minmax_scale\nfrom sklearn.cluster import KMeans\n%matplotlib inline","execution_count":86,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def create_is_sell_data(sell_prices_df, calendar_df, train_df):\n    sell_prices_df['id'] = sell_prices_df['item_id'].astype('str')+'_'+sell_prices_df['store_id']+'_evaluation'\n    sell_prices_data = sell_prices_df[sell_prices_df.wm_yr_wk.isin(calendar_df.wm_yr_wk.unique())]\n    sell_prices_data.reset_index(drop=True, inplace=True)\n    tmp = sell_prices_data.groupby(['id'])[['wm_yr_wk', 'sell_price']].apply(\n        lambda x: x.set_index('wm_yr_wk')['sell_price'].to_dict()\n    ).to_dict()\n    d = calendar_df.d\n    wm_yr_wk = calendar_df.wm_yr_wk\n    price_data = {}\n    for col in tqdm(train_df.id.unique()):\n        price_data[col] = wm_yr_wk.map(tmp[col])\n    price_data = pd.DataFrame(price_data)\n    price_data.index = d\n    is_sell = price_data.notnull().astype(float).T\n    price_data = price_data.fillna(0).T\n    \n    is_sell = pd.concat([\n        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']], is_sell\n    ], axis=1)\n    price_data = pd.concat([\n        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']], price_data\n    ], axis=1)\n    \n    return price_data, is_sell\n\ndef set_index(df, name):\n    d = {}\n    for col, value in df.iloc[0,:].items():\n        try:\n            if '_evaluation' in value:\n                d[col] = 'id'\n            if 'd_' in value:\n                d[col] = 'd'\n        except:\n            if type(value)!=str:\n                d[col]=name\n    return d\n\ndef dcol2int(col):\n    if col[:2]=='d_':\n        return int(col.replace('d_', ''))\n    else:\n        return col\n    \ndef str_category_2_int(data):\n    categories = [c for c in data.columns if data[c].dtype==object]\n    for c in categories:\n        if c=='id' or c=='d':\n            pass\n        else:\n            data[c] = pd.factorize(data[c])[0]\n            data[c] = data[c].replace(-1, np.nan)\n    return data\n\ndef select_near_event(x, event_name):\n    z = ''\n    for y in x:\n        if y in event_name:\n            z+=y+'_'\n    if len(z)==0:\n        return np.nan\n    else:\n        return z\n    \ndef sort_d_cols(d_cols):\n    d_cols = [int(d.replace('d_','')) for d in d_cols]\n    d_cols = sorted(d_cols)\n    d_cols = [f'd_{d}' for d in d_cols]\n    return d_cols","execution_count":87,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def preprocessing(path, d_cols, train_d_cols):\n    train_df = pd.read_csv(path+'sales_train_evaluation.csv')\n    calendar_df = pd.read_csv(path+'calendar.csv')\n    sell_prices_df = pd.read_csv(path+'sell_prices.csv')\n    sample_submission_df = pd.read_csv(path+'sample_submission.csv')\n    \n    train_df.index = train_df.id\n    calendar_df['date']=pd.to_datetime(calendar_df.date)\n    calendar_df.index = calendar_df.d\n    price_data, is_sell = create_is_sell_data(sell_prices_df, calendar_df, train_df)\n    \n    str_cols = [ col for col in train_df.columns if 'id' in str(col)]\n    new_columns = str_cols+d_cols\n    train_df = train_df.reindex(columns=new_columns)\n    \n    train_df = pd.concat([\n        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']],\n        train_df.loc[train_df.index,d_cols]*price_data.loc[train_df.index,d_cols]\n    ], axis=1)\n    \n    data = train_df[train_d_cols].stack(dropna=False).reset_index()\n    data = data.rename(columns=set_index(data, 'TARGET'))\n    data.reset_index(drop=True, inplace=True)\n    \n    data = reduce_mem_usage(data)\n    \n    for key, value in train_df[['dept_id', 'cat_id', 'state_id', 'store_id', 'item_id']].to_dict().items():\n        data[key] = data.id.map(value)\n    \n    #snap_data\n    snap_data = calendar_df[['snap_CA', 'snap_WI', 'snap_TX', 'd']]\n    snap_data.set_index('d', inplace=True)\n    data[f'snap']=0\n    for key, value in snap_data.to_dict().items():\n        k = key.replace('snap_', '')\n        data.loc[data.state_id==k,'snap'] = data.loc[data.state_id==k, 'd'].map(value).fillna(0)\n        \n    #price_data\n    price_data = price_data[train_d_cols].T.astype(float)\n    price_data = price_data.stack(dropna=False).reset_index()\n    price_data.rename(columns=set_index(price_data, 'price'), inplace=True)\n    data = pd.merge(data, price_data, on=['d', 'id'], how='left')\n    \n    event_name = ['SuperBowl', 'ValentinesDay', 'PresidentsDay', 'LentStart', 'LentWeek2', 'StPatricksDay', 'Purim End', \n              'OrthodoxEaster', 'Pesach End', 'Cinco De Mayo', \"Mother's day\", 'MemorialDay', 'NBAFinalsStart', 'NBAFinalsEnd',\n              \"Father's day\", 'IndependenceDay', 'Ramadan starts', 'Eid al-Fitr', 'LaborDay', 'ColumbusDay', 'Halloween', \n              'EidAlAdha', 'VeteransDay', 'Thanksgiving', 'Christmas', 'Chanukah End', 'NewYear', 'OrthodoxChristmas', \n              'MartinLutherKingDay', 'Easter']\n    event_type = ['Sporting', 'Cultural', 'National', 'Religious']\n    event_names = {'event_name_1':event_name, 'event_type_1':event_type}\n    for event, event_name in event_names.items():\n        for w in [4]:\n            calendar_df[f'new_{event}_{w}']=''\n            for i in range(-1,-(w+1),-1):\n                calendar_df[f'new_{event}_{w}'] += calendar_df[event].shift(i).astype(str)+'|'\n            calendar_df[f'new_{event}_{w}'] = calendar_df[f'new_{event}_{w}'].apply(lambda x: x.split('|'))\n            calendar_df[f'new_{event}_{w}'] = calendar_df[f'new_{event}_{w}'].apply(lambda x: select_near_event(x, event_name))\n\n    #calendar_dict\n    cols = ['new_event_name_1_4', 'new_event_type_1_4', 'wday', 'month', 'year', 'event_name_1','event_type_1']\n    for key, value in calendar_df[cols].to_dict().items():\n        data[key] = data.d.map(value)\n    for shift in [-1,1]:\n        data[f'snap_{shift}'] = data.groupby(['id'])['snap'].shift(shift)\n    \n    return data","execution_count":88,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# feature engineering"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def make_roll_data(data, win):\n    data_2 = data.groupby(['id'])['TARGET'].apply(\n            lambda x:\n            x.shift(1).rolling(win, min_periods=1).agg({'mean'})\n        )\n    for col in data_2.columns:\n        data[f'roll_{win}_{col}'] = data_2[col]\n        \n    return data\n\ndef shift_diff_data(data):\n    data[f'shift_diff']=0\n    for i in range(4):\n        data[f'shift_diff'] += data.groupby(['id'])['TARGET'].apply(lambda x: x.diff(7).shift(7))/4\n    \n    return data\n    \n\ndef make_lag_roll_data(data, lag):\n    data[f'lag{lag}_roll_14_mean'] = data.groupby(['id'])['TARGET'].apply(\n        lambda x:\n        x.shift(lag).rolling(28, min_periods=1).mean()\n    )\n    \n    return data\n\ndef make_shift_data(data):\n    for i in [0,3]:\n        data[f'shift_{7*(i+1)}'] = data.groupby(['id'])['TARGET'].shift(7*(i+1))\n        \n    return data\n\n\ndef fe(data):\n    data = make_roll_data(data, 7)\n    data = make_roll_data(data, 28)\n    \n    #data = shift_diff_data(data) \n    data = make_lag_roll_data(data, 28)\n    data = make_lag_roll_data(data, 7)\n    #data = make_lag_roll_data(data, 84)\n    #data = roll_diff_data(data)\n    \n    data = make_shift_data(data)\n    \n    return data","execution_count":89,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### reduce_mem_usage"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                       df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":90,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### WRMSSEEvaluator"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"## evaluation metric\n## from https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/133834 and edited to get scores at all levels\nclass WRMSSEEvaluator(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 0  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n        return weight_df\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = getattr(self, f'lv{lv}_scale')\n        return (score / scale).map(np.sqrt)\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n\n        group_ids = []\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n            group_ids.append(group_id)\n            all_scores.append(lv_scores.sum())\n\n        return group_ids, all_scores","execution_count":91,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# lgb model utils"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"\"\"\"PARAMS = {'boosting_type': 'gbdt', 'objective': 'tweedie', 'metric': 'rmse', 'max_bin': 100, \n          'n_estimators': 2000, 'boost_from_average': False, 'verbose': -1, 'random_state': 2020,\n          'tweedie_variance_power': 1.141893486974509, 'subsample': 0.8710431222390667, \n          'subsample_freq': 0.5692738176797527, 'learning_rate': 0.10957379305366494, 'num_leaves': 8,  \n          'feature_fraction': 0.45380044045308154, 'bagging_freq': 4, 'min_child_samples': 5, \n          'lambda_l1': 7.510525772813387e-06, 'lambda_l2': 4.1004528526443944e-07,\n          'device_type':'cpu'}\"\"\"\n\nPARAMS = {'boosting_type': 'gbdt', \n          'objective' : 'tweedie', 'tweedie_variance_power': 1.141893486974509,\n          \"metric\" :\"rmse\", \"force_row_wise\" : True, \"learning_rate\" : 0.075, \"sub_row\" : 0.75, \"bagging_freq\" : 1,\n          \"lambda_l2\" : 0.1, 'verbosity': 1, 'num_iterations' : 2500}","execution_count":92,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def plot_importance(models, col):\n    importances = np.zeros(len(col))\n    for model in models:\n        importances+=model.feature_importance(importance_type='gain')\n    importance = pd.DataFrame()\n    importance['col'] = col\n    importance['importance'] = minmax_scale(importances)\n    #importance.to_csv(f'importance_{name}.csv',index=False)\n    return importance\n\ndef predict_cv(x_val, models):\n    preds = np.zeros(len(x_val))\n    for model in models:\n        pred = model.predict(x_val)\n        preds+=pred/len(models)\n    return preds\n\n\ndef kmean_cluster(X):\n    gk = GroupKFold(n_splits=10)\n    group = X['wday'].astype(str)+'_'+X['month'].astype(str)\n    for trn, val in gk.split(X,groups=group):\n        tmp_X = X.loc[val, ['TARGET', 'wday', 'month']]\n        km = KMeans(n_clusters=60, random_state=2020)\n        k = StratifiedKFold(n_splits=10, random_state=2020, shuffle=True)\n        tmp_X.reset_index(drop=True, inplace=True)\n        for _trn, _val in k.split(tmp_X,y=tmp_X['TARGET'].astype(int)):\n            sns.distplot(tmp_X.loc[_val,'TARGET'].values)\n            plt.show()\n            km.fit(tmp_X.loc[_val,['TARGET', 'wday', 'month']])\n            print(len(val))\n            break\n        break\n    \n    X['group'] = km.predict(X[['TARGET', 'wday', 'month']])\n    \n    return X\n\ndef train_predict_RE(data,path,params=PARAMS):\n    days = sorted(data.d.unique())\n    days = sort_d_cols(days)\n    tmp_days = days[-63:]\n    trn_days = days[:-28]\n    val_days = days[-28:]\n    \n    for i in range(28):\n        data = fe(data)\n        if i==0:\n            shift_cols = [col for col in data.columns if 'shift' in col]\n            roll_cols = [col for col in data.columns if 'roll' in col]\n            cat_cols = ['item_id', 'store_id', 'snap', 'snap_-1', 'snap_1',\n                      # 'dept_id_price', 'cat_id_price',\n                      'price',  'new_event_type_1_4','event_name_1','event_type_1', 'wday', 'month', 'year'\n                      #'is_sell_cnt_dept_id_store_id', 'is_sell_cnt_cat_id_store_id'\n                     ]\n            features=cat_cols+shift_cols+roll_cols\n\n            print(f' FEATIRE LEN {len(features)}')\n            models=[]\n            X = data[data.d.isin(trn_days)][data.TARGET.notnull()]\n            X = X.dropna(0)\n            X.reset_index(drop=True, inplace=True)\n            k = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)\n            y = (500*minmax_scale(X['TARGET'])).astype(int)\n            for trn, val in k.split(X,y=y):\n                train_set = lgb.Dataset(X.loc[trn,features], X.loc[trn,'TARGET'])\n                val_set = lgb.Dataset(X.loc[val,features], X.loc[val,'TARGET'])\n                model = lgb.train(train_set=train_set, valid_sets=[train_set, val_set], params=params,\n                                  early_stopping_rounds=100, verbose_eval=500, categorical_feature=cat_cols)\n                models.append(model)\n            data = data[data.d.isin(tmp_days)]\n\n            importance = plot_importance(models, features)\n            importance.to_csv(f'train_{i}_importance.csv', index=False)\n        \n        val_day = val_days[i]\n        predict_data = data[data.d==val_day]\n        preds = predict_cv(predict_data[features], models)\n        \n        data.loc[data.d==val_day, 'TARGET'] = preds\n        \n    sub = data[data.d.isin(val_days)][['id', 'd', 'TARGET', 'price']]\n    sub.to_csv('all_result.csv', index=False)\n    sub['TARGET'] = sub['TARGET']/sub['price']\n    sub = sub.groupby(['id'])[['d', 'TARGET']].apply(lambda x: x.set_index('d')['TARGET'].T)[val_days]\n    sub.columns=[f'F{i}' for i in range(1,29)]\n    \n    return sub","execution_count":93,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"def all_flow(path, d_cols, train_d_cols):\n    \n    data = preprocessing(path, d_cols, train_d_cols)\n    \"\"\"\n    data = fe(data)\n    \n    \n    f = ['id', 'd', 'TARGET','cat_id', 'state_id', 'month']\n    shift_cols = [col for col in data.columns if 'shift' in col]\n    roll_cols = [col for col in data.columns if 'roll' in col]\n    features=['dept_id', 'store_id', 'snap', 'snap_-1', 'snap_1', 'dept_id_price', 'cat_id_price', 'price',\n              'new_event_name_1_4', 'new_event_type_1_4', 'wday', 'event_name_1','event_type_1',\n              'is_sell_cnt_dept_id_store_id', 'is_sell_cnt_cat_id_store_id'\n             ]+shift_cols+roll_cols\n    f = f+features\n    data = data[f]\n    \"\"\"\n    data = str_category_2_int(data)\n    \n    use_days=data.d.unique().tolist()\n    use_days=sort_d_cols(use_days)[63:]\n    \n    data = data[data.d.isin(use_days)]\n    gc.collect()\n    mem = data.memory_usage().sum()/1024**2\n    \n    print(f\"\"\"\n    DATA SHAPE   {data.shape}\n    MEMORY USAGE   {mem:.2f}MB\n    DATA COLUMNS  {data.columns.tolist()}\n    \"\"\")\n    \n    gc.collect()\n    sub = train_predict_RE(data=data, path=path)\n    \n    sample_sub = pd.read_csv(path+'sample_submission.csv')\n    sample_sub = sample_sub.set_index('id')\n    sample_sub.loc[sub.index, sub.columns]= sub.values\n    sample_sub=sample_sub.reset_index()\n    \n    return sample_sub","execution_count":94,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\n#path = '/Users/abcdm/Downloads/m5-forecasting-accuracy/'\npath = '../input/m5-forecasting-accuracy/'\n\nd_cols=[f'd_{i+1}' for i in range(1969)]\ntrain_d_cols = d_cols[:-56]\ntrain_d_cols = train_d_cols[-530:]\n\nsub = all_flow(path, d_cols, train_d_cols)\nsub.to_csv('submission.csv', index=False)\n\n\ndf_train_full = pd.read_csv(path+\"sales_train_evaluation.csv\")\ndf_calendar = pd.read_csv(path+\"calendar.csv\")\ndf_prices = pd.read_csv(path+\"sell_prices.csv\")\ndf_sample_submission = pd.read_csv(path+\"sample_submission.csv\")\ndf_sample_submission[\"order\"] = range(df_sample_submission.shape[0])\ndf_train_full = pd.read_csv(path+\"sales_train_evaluation.csv\")\n\ndf_train = df_train_full.iloc[:, :-56]\ndf_valid = df_train_full.iloc[:, -56:-56+28]\n\nevaluator = WRMSSEEvaluator(df_train, df_valid, df_calendar, df_prices)","execution_count":null,"outputs":[{"output_type":"stream","text":"100%|██████████| 30490/30490 [00:28<00:00, 1079.57it/s]\n","name":"stderr"},{"output_type":"stream","text":"Mem. usage decreased to 277.40 Mb (25.0% reduction)\n\n    DATA SHAPE   (14238830, 19)\n    MEMORY USAGE   2091.20MB\n    DATA COLUMNS  ['id', 'd', 'TARGET', 'dept_id', 'cat_id', 'state_id', 'store_id', 'item_id', 'snap', 'price', 'new_event_name_1_4', 'new_event_type_1_4', 'wday', 'month', 'year', 'event_name_1', 'event_type_1', 'snap_-1', 'snap_1']\n    \n","name":"stdout"}]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#sub = pd.read_csv('../input/create-m5-all-flow-11/submission.csv')\nvalidation = sub[sub.id.str.contains('_validation')]\nevaluation = sub[sub.id.str.contains('_evaluation')]\nevaluation.id = evaluation.id.str.replace('_evaluation', '_validation')\nevaluation = evaluation.set_index('id').loc[validation.id]\nsub[sub.id.str.contains('_validation')] = evaluation.reset_index()\nsub.loc[sub.id.str.contains('_evaluation'),[f'F{i}' for i in range(1,29)]]=0\n\npreds_valid = sub#pd.read_csv(\"../input/submission-m5-ver3/submission.csv\")\npreds_valid = preds_valid[preds_valid.id.str.contains(\"validation\")]\npreds_valid = preds_valid.merge(df_sample_submission[[\"id\", \"order\"]], on = \"id\").sort_values(\"order\").drop([\"id\", \"order\"], axis = 1)\npreds_valid.rename(columns = {f'F{i+1}':d for i, d in enumerate(train_d_cols[-28:])}, inplace = True)\n\ngroups, scores = evaluator.score(preds_valid)\n\nscore_public_lb = np.mean(scores)\n\nfor i in range(len(groups)):\n    print(f\"Score for group {groups[i]}: {round(scores[i], 5)}\")\n\nprint(f\"\\nPublic LB Score: {round(score_public_lb, 5)}\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}