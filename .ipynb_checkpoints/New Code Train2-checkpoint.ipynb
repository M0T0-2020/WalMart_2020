{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os, gc\n",
    "import termcolor\n",
    "\n",
    "import math, random\n",
    "import pickle\n",
    "import datetime, time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "RANDOM_SEED = 2020\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(DEVICE)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish(input):\n",
    "    return input * torch.tanh(nn.functional.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return mish(input)\n",
    "    \n",
    "class residual_conv1d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel):\n",
    "        super(residual_conv1d, self).__init__()\n",
    "        \n",
    "        self.mish = Mish()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv1d(in_channel, in_channel, 1),\n",
    "            Mish(),\n",
    "            nn.Conv1d(in_channel, in_channel, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x+self.layer(x)\n",
    "        x = self.mish(x)\n",
    "        return x\n",
    "\n",
    "class Conv_1d_Net(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel):\n",
    "        super(Conv_1d_Net, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channel, 2*in_channel, 1),\n",
    "            nn.Dropout(0.2),\n",
    "            Mish(),\n",
    "            residual_conv1d(2*in_channel)\n",
    "        )\n",
    "        \n",
    "        self.layer_2 = nn.Sequential(\n",
    "            nn.Conv1d(2*in_channel, 4*in_channel, 1),\n",
    "            nn.Dropout(0.2),\n",
    "            Mish(),\n",
    "            residual_conv1d(4*in_channel)\n",
    "        )\n",
    "        \n",
    "        self.layer_3 = nn.Sequential(\n",
    "            nn.Conv1d(4*in_channel, 8*in_channel, 1),\n",
    "            nn.Dropout(0.2),\n",
    "            Mish(),\n",
    "            residual_conv1d(8*in_channel)\n",
    "        )\n",
    "       \n",
    "         \n",
    "        self.avgpool1d = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(8*in_channel, 8*in_channel),\n",
    "            nn.Dropout(0.1),\n",
    "            Mish(),\n",
    "            nn.Linear(8*in_channel, 16*in_channel),\n",
    "            nn.Dropout(0.1),\n",
    "            Mish(),\n",
    "            nn.Linear(16*in_channel, 28)\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        #_in = x.size()[1]\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        #x = self.layer_4(x)\n",
    "        x = self.avgpool1d(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:            \n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_preprocessing(d_cols, train_df, calendar_df, sell_prices_df):\n",
    "    sell_prices_df['id'] = sell_prices_df['item_id'].astype('str')+'_'+sell_prices_df['store_id']+'_validation'\n",
    "    \n",
    "    event_type_1 = pd.get_dummies(calendar_df.event_type_1)\n",
    "    event_type_1.columns = [f'{col}_event_type_1' for col in event_type_1.columns]\n",
    "    event_type_2 = pd.get_dummies(calendar_df.event_type_1)\n",
    "    event_type_2.columns = [f'{col}_event_type_2' for col in event_type_2.columns]\n",
    "    calendar_data = pd.concat([\n",
    "        calendar_df.drop(columns=['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2'])[['wday', 'd','month','snap_CA', 'snap_TX', 'snap_WI']],\n",
    "        event_type_1,\n",
    "        event_type_2\n",
    "    ], axis=1)\n",
    "    calendar_data = calendar_data.set_index('d').T\n",
    "    \n",
    "    \n",
    "    \n",
    "    sell_prices_data = sell_prices_df[sell_prices_df.wm_yr_wk.isin(calendar_df.wm_yr_wk.unique())]\n",
    "    sell_prices_data.reset_index(drop=True, inplace=True)\n",
    "    tmp = sell_prices_data.groupby(['id'])[['wm_yr_wk', 'sell_price']].apply(lambda x: x.set_index('wm_yr_wk')['sell_price'].to_dict()).to_dict()\n",
    "    d = calendar_df.d\n",
    "    wm_yr_wk = calendar_df.wm_yr_wk\n",
    "    price_data = {}\n",
    "    for col in tqdm(train_df.id.unique()):\n",
    "        price_data[col] = wm_yr_wk.map(tmp[col])\n",
    "    price_data = pd.DataFrame(price_data)\n",
    "    price_data.index = d\n",
    "    \n",
    "    \n",
    "    is_sell = price_data.notnull().astype(float).T\n",
    "    price_data = price_data.fillna(0)\n",
    "    \n",
    "    train_df = train_df.T\n",
    "    train_df.columns = train_df.loc['id', :].values\n",
    "    train_df = train_df.T\n",
    "    id_cols = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "    \n",
    "    train_df[d_cols] = train_df[d_cols].astype(float)\n",
    "    item_id = train_df.groupby(['item_id'])[d_cols].transform('mean') \n",
    "    print('complete  item_id')\n",
    "    item_id_state_id = train_df.groupby(['item_id', 'state_id'])[d_cols].transform('mean')\n",
    "    print('complete  item_id  state_id')\n",
    "    item_id_store_id  = train_df.groupby(['item_id', 'store_id'])[d_cols].transform('mean')\n",
    "    print('complete  item_id  store_id')\n",
    "    dept_id_store_id = train_df.groupby(['dept_id', 'store_id'])[d_cols].transform('mean')\n",
    "    print('complete  dept_id  store_id')\n",
    "\n",
    "    return train_df[id_cols+d_cols], calendar_df, calendar_data[d_cols], price_data.T[d_cols], is_sell[d_cols],\\\n",
    "                item_id, item_id_state_id, item_id_store_id, dept_id_store_id\n",
    "                \n",
    "\n",
    "\n",
    "class preprocessing():\n",
    "    def __init__(self, path):\n",
    "        self.path=path\n",
    "        self.df = pd.read_csv(self.path+'sales_train_validation.csv')\n",
    "        self.df['all_id'] = 1\n",
    "        self.calendar_df = pd.read_csv(self.path+'calendar.csv')\n",
    "        self.sell_prices_df = pd.read_csv(self.path+'sell_prices.csv')\n",
    "        self.sample_submission_df = pd.read_csv(self.path+'sample_submission.csv')\n",
    "        \n",
    "        self.d_cols = self.df.columns[self.df.columns.str.startswith('d_')].values.tolist()\n",
    "        self.d_cols = self.d_cols[-1200:]\n",
    "        \n",
    "        self.train_df, self.calendar_df, self.calendar_data, self.price_data, self.is_sell,\\\n",
    "        self.item_id, self.item_id_state_id, self.item_id_store_id, self.dept_id_store_id = do_preprocessing(\n",
    "            self.d_cols, self.df, self.calendar_df, self.sell_prices_df\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6e5712360d646f8915aea871560caeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30490), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f7cb1d112e3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mPreProcessing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-db1f09d1bf36>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalendar_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalendar_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprice_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         self.item_id, self.item_id_state_id, self.item_id_store_id, self.dept_id_store_id = do_preprocessing(\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalendar_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msell_prices_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         )\n",
      "\u001b[0;32m<ipython-input-3-db1f09d1bf36>\u001b[0m in \u001b[0;36mdo_preprocessing\u001b[0;34m(d_cols, train_df, calendar_df, sell_prices_df)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mid_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dept_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cat_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'store_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'state_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_cols\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mitem_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'item_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md_cols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'complete  item_id'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3114\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3115\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3116\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_setitem_array\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3138\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Columns must be same length as key'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3139\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3140\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3141\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3117\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3118\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3119\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_valid_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3194\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3195\u001b[0;31m         \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3197\u001b[0m         \u001b[0;31m# check if we are modifying a copy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   2598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2600\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2601\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mset\u001b[0;34m(self, item, value, check)\u001b[0m\n\u001b[1;32m   4271\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4272\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blklocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4273\u001b[0;31m                     \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4274\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blklocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(self, loc)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mDelete\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mplace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \"\"\"\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36mdelete\u001b[0;34m(arr, obj, axis)\u001b[0m\n\u001b[1;32m   4416\u001b[0m         \u001b[0mkeep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4417\u001b[0m         \u001b[0mslobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4418\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\n",
    "PreProcessing = preprocessing(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normal_Dataset_Train(torch.utils.data.Dataset):\n",
    "    def __init__(self, PreProcessing, d_cols, data_size=100):\n",
    "        self.PreProcessing = PreProcessing\n",
    "        self.datanum = len(PreProcessing.train_df)\n",
    "        self.d_cols = d_cols\n",
    "        self.data_size=data_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        a = random.randint(0,len(self.d_cols)-self.data_size-28)\n",
    "        \n",
    "        #cols = self.d_cols[a:a+self.data_size]\n",
    "        #next_cols = self.d_cols[a+28:a+self.data_size+28]\n",
    "        \n",
    "        #True_cols = self.d_cols[a+self.data_size:a+self.data_size+28]\n",
    "        \n",
    "        state = self.PreProcessing.train_df.iloc[idx,:]['state_id']\n",
    "        calendar_cols = [\n",
    "            'wday', 'month', f'snap_{state}', 'Cultural_event_type_1', 'National_event_type_1', 'Religious_event_type_1',\n",
    "            'Sporting_event_type_1', 'Cultural_event_type_2', 'National_event_type_2', 'Religious_event_type_2', 'Sporting_event_type_2'\n",
    "        ]\n",
    "        \n",
    "        x = np.vstack((\n",
    "            self.PreProcessing.train_df.iloc[[idx],6+a:6+a+self.data_size].values.astype(float),\n",
    "            self.PreProcessing.item_id.iloc[[idx],a:a+self.data_size].values.astype(float),\n",
    "            self.PreProcessing.item_id_state_id.iloc[[idx],a:a+self.data_size].values.astype(float),\n",
    "            self.PreProcessing.item_id_store_id.iloc[[idx],a:a+self.data_size].values.astype(float),\n",
    "            self.PreProcessing.dept_id_store_id.iloc[[idx],a:a+self.data_size].values.astype(float),\n",
    "            \n",
    "            self.PreProcessing.price_data.iloc[[idx],a:a+self.data_size].values.astype(float),\n",
    "            self.PreProcessing.price_data.iloc[[idx],a+28:a+self.data_size+28].values.astype(float),\n",
    "            \n",
    "            self.PreProcessing.is_sell.iloc[[idx],a:a+self.data_size].values.astype(float),\n",
    "            self.PreProcessing.is_sell.iloc[[idx],a+28:a+self.data_size+28].values.astype(float),\n",
    "            \n",
    "            self.PreProcessing.calendar_data.loc[calendar_cols, :].values.astype(float)[:, a:a+self.data_size],\n",
    "            self.PreProcessing.calendar_data.loc[calendar_cols, :].values.astype(float)[:, a+28:a+self.data_size+28]\n",
    "        ))\n",
    "        y = self.PreProcessing.train_df.iloc[[idx],6+a+self.data_size:6+a+self.data_size+28].values.astype(float)\n",
    "        \n",
    "        x = torch.FloatTensor(x)\n",
    "        y = torch.FloatTensor(y)\n",
    "        return x, y\n",
    "    \n",
    "    \n",
    "    \n",
    "class Normal_Dataset_Test(torch.utils.data.Dataset):\n",
    "    def __init__(self, PreProcessing, d_cols, data_size=100):\n",
    "        self.PreProcessing = PreProcessing\n",
    "        self.datanum = len(PreProcessing.train_df)\n",
    "        self.d_cols_len = len(d_cols)\n",
    "        #_int = int(d_cols[-1].replace('d_', ''))\n",
    "        #next_dcols = [f'd_{i}' for i in range(_int+1, _int+1+28)]\n",
    "        #self.d_cols = d_cols[-100:]+next_dcols\n",
    "                   \n",
    "        \n",
    "        self.data_size=data_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        #cols = self.d_cols[:100]\n",
    "        #next_cols = self.d_cols[28:100+28]\n",
    "        \n",
    "        #True_cols = self.d_cols[-28:]\n",
    "        \n",
    "        state = self.PreProcessing.train_df.iloc[idx,:]['state_id']\n",
    "        calendar_cols = [\n",
    "            'wday', 'month', f'snap_{state}', 'Cultural_event_type_1', 'National_event_type_1', 'Religious_event_type_1',\n",
    "            'Sporting_event_type_1', 'Cultural_event_type_2', 'National_event_type_2', 'Religious_event_type_2', 'Sporting_event_type_2'\n",
    "        ]\n",
    "        \n",
    "        x = np.vstack((\n",
    "            self.PreProcessing.train_df.iloc[[idx],6+self.d_cols_len-self.data_size:6+self.d_cols_len].values.astype(float),\n",
    "            self.PreProcessing.item_id.iloc[[idx],self.d_cols_len-self.data_size:self.d_cols_len].values.astype(float),\n",
    "            self.PreProcessing.item_id_state_id.iloc[[idx],self.d_cols_len-self.data_size:self.d_cols_len].values.astype(float),\n",
    "            self.PreProcessing.item_id_store_id.iloc[[idx],self.d_cols_len-self.data_size:self.d_cols_len].values.astype(float),\n",
    "            self.PreProcessing.dept_id_store_id.iloc[[idx],self.d_cols_len-self.data_size:self.d_cols_len].values.astype(float),\n",
    "            \n",
    "            self.PreProcessing.price_data.iloc[[idx],self.d_cols_len-self.data_size:self.d_cols_len].values.astype(float),\n",
    "            self.PreProcessing.price_data.iloc[[idx],self.d_cols_len+28-self.data_size:self.d_cols_len+28].values.astype(float),\n",
    "            \n",
    "            self.PreProcessing.is_sell.iloc[[idx],self.d_cols_len-self.data_size:self.d_cols_len].values.astype(float),\n",
    "            self.PreProcessing.is_sell.iloc[[idx],self.d_cols_len+28-self.data_size:self.d_cols_len+28].values.astype(float),\n",
    "            \n",
    "            self.PreProcessing.calendar_data.loc[calendar_cols, :].values.astype(float)[:, self.d_cols_len-self.data_size:self.d_cols_len],\n",
    "            self.PreProcessing.calendar_data.loc[calendar_cols, :].values.astype(float)[:, self.d_cols_len+28-self.data_size:self.d_cols_len+28]\n",
    "        ))\n",
    "        y = self.PreProcessing.train_df.iloc[[idx],6+self.d_cols_len:6+self.d_cols_len+28].values.astype(float)\n",
    "        \n",
    "        x = torch.FloatTensor(x)\n",
    "        y = torch.FloatTensor(y)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, PreProcesing, trn_cols):\n",
    "    model = model.to(DEVICE)\n",
    "    dataset_train = Normal_Dataset_Train(PreProcessing, trn_cols)\n",
    "    dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size = 100, shuffle = True)\n",
    "\n",
    "    dataset_val = Normal_Dataset_Test(PreProcessing, trn_cols)\n",
    "    dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size = 100, shuffle = False)\n",
    "    \n",
    "    num_epochs = 40\n",
    "    best_epoch = -1\n",
    "    best_score = 10000\n",
    "    early_stoppping_cnt = 0\n",
    "    best_model = model\n",
    "    \n",
    "    optimizer = RAdam(model.parameters(), lr=4e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        \n",
    "        for x_batch, y_batch in dataloader_train:\n",
    "            optimizer.zero_grad()\n",
    "            x_batch = x_batch.to(DEVICE)\n",
    "            y_batch = y_batch.to(DEVICE)\n",
    "            \n",
    "            preds = model(x_batch)\n",
    "            \n",
    "            loss = criterion(preds.cpu(), y_batch.cpu())\n",
    "            loss = torch.sqrt(loss)\n",
    "            loss = loss.to(DEVICE)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(dataloader_train)\n",
    "            del loss; gc.collect()\n",
    "\n",
    "        model.eval()\n",
    "        avg_val_loss = 0.\n",
    "        \n",
    "        for x_batch, y_batch in dataloader_val:\n",
    "            x_batch = x_batch.to(DEVICE)\n",
    "            y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "            preds = model(x_batch)\n",
    "            loss = self.criterion(preds.cpu(), y_batch.cpu())\n",
    "            loss = torch.sqrt(loss)\n",
    "            \n",
    "            avg_val_loss += loss.item() / len(dataloader_val)\n",
    "            del loss; gc.collect()\n",
    "\n",
    "\n",
    "        if best_score>avg_val_loss:\n",
    "            best_score = avg_val_loss\n",
    "            early_stoppping_cnt=0\n",
    "            best_epoch=epoch+1\n",
    "            best_model = model\n",
    "            elapsed = time.time() - start_time\n",
    "            p_avg_val_loss = termcolor.colored(np.round(avg_val_loss, 4),\"red\")\n",
    "\n",
    "            print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {p_avg_val_loss} time: {elapsed:.0f}s')\n",
    "        else:\n",
    "            early_stoppping_cnt+=1\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f} time: {elapsed:.0f}s')\n",
    "\n",
    "        if (epoch>8) and (early_stoppping_cnt>5):\n",
    "            break\n",
    "\n",
    "    print(f'best_score : {best_score}    best_epoch : {best_epoch}')\n",
    "    \n",
    "    preds = []\n",
    "    for x_batch, y_batch in dataloader_val:\n",
    "        x_batch = x_batch.to(DEVICE)\n",
    "        y_batch = y_batch.to(DEVICE)\n",
    "        preds += model(x_batch).detch().cpu().tolist()\n",
    "    \n",
    "    return best_model, best_score, np.array(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_cols = PreProcessing.d_cols[:900]\n",
    "\n",
    "dataset_train = Normal_Dataset_Train(PreProcessing, trn_cols)\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size = 100, shuffle = True)\n",
    "\n",
    "dataset_val = Normal_Dataset_Test(PreProcessing, trn_cols)\n",
    "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size = 100, shuffle = True)\n",
    "\n",
    "c=0\n",
    "for x,y in dataloader_train:\n",
    "    if c<1:\n",
    "        print(x.size(), y.size())\n",
    "        c+=1\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "c=0\n",
    "for x,y in dataloader_val:\n",
    "    if c<1:\n",
    "        print(x.size(), y.size())\n",
    "        c+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset_train, dataloader_train, dataset_val, dataloader_val, x,y;gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Conv_1d_Net(31)\n",
    "trn_cols = PreProcessing.d_cols[:900]\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, PreProcessing, trn_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
