{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import gc, pickle\n",
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold,TimeSeriesSplit, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, log_loss\n",
    "from sklearn.linear_model import Ridge,Lasso, BayesianRidge\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'tweedie',\n",
    "    'tweedie_variance_power': 1.1,\n",
    "    'metric': 'rmse',\n",
    "    'subsample': 0.5,\n",
    "    'subsample_freq': 1,\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 2**11-1,\n",
    "    'min_data_in_leaf': 2**12-1,\n",
    "    'feature_fraction': 0.5,\n",
    "    'max_bin': 100,\n",
    "    'n_estimators': 1400,\n",
    "    'boost_from_average': False,\n",
    "    'verbose': 1,\n",
    "    \n",
    "    'random_state':2020\n",
    "    } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def create_is_sell_data(sell_prices_df, calendar_df, train_df):\n",
    "    sell_prices_df['id'] = sell_prices_df['item_id'].astype('str')+'_'+sell_prices_df['store_id']+'_evaluation'\n",
    "    sell_prices_data = sell_prices_df[sell_prices_df.wm_yr_wk.isin(calendar_df.wm_yr_wk.unique())]\n",
    "    sell_prices_data.reset_index(drop=True, inplace=True)\n",
    "    tmp = sell_prices_data.groupby(['id'])[['wm_yr_wk', 'sell_price']].apply(\n",
    "        lambda x: x.set_index('wm_yr_wk')['sell_price'].to_dict()\n",
    "    ).to_dict()\n",
    "    d = calendar_df.d\n",
    "    wm_yr_wk = calendar_df.wm_yr_wk\n",
    "    price_data = {}\n",
    "    for col in tqdm(train_df.id.unique()):\n",
    "        price_data[col] = wm_yr_wk.map(tmp[col])\n",
    "    price_data = pd.DataFrame(price_data)\n",
    "    price_data.index = d\n",
    "    is_sell = price_data.notnull().astype(float).T\n",
    "    price_data = price_data.fillna(0)\n",
    "    \n",
    "    is_sell.index=train_df.id\n",
    "    train_df.index=train_df.id\n",
    "    is_sell = pd.concat([\n",
    "        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']], is_sell\n",
    "    ], axis=1)\n",
    "    price_data = pd.concat([\n",
    "        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']], price_data.T  \n",
    "    ], axis=1)\n",
    "    \n",
    "    return price_data, is_sell\n",
    "\n",
    "def set_index(df, name):\n",
    "    d = {}\n",
    "    for col, value in df.iloc[0,:].items():\n",
    "        if type(col)==str:\n",
    "            if type(df[col].values[0])!=str:\n",
    "                v = 'd'\n",
    "            else:\n",
    "                v='id'\n",
    "        else:\n",
    "            v=name\n",
    "        d[col]=v\n",
    "    return d\n",
    "\n",
    "def dcol2int(col):\n",
    "    if col[:2]=='d_':\n",
    "        return int(col.replace('d_', ''))\n",
    "    else:\n",
    "        return col\n",
    "    \n",
    "def create_event_data(train_df, calendar_df):\n",
    "    new_df = pd.DataFrame()\n",
    "    D_COLS = [d for d in train_df.columns if type(d)!=str]\n",
    "    for event_name in ['event_name_1', 'event_name_2']:\n",
    "        tmp_df = pd.concat([\n",
    "            train_df.groupby(['dept_id'])[D_COLS].mean().T.astype(float),\n",
    "            train_df.groupby(['cat_id'])[D_COLS].mean().T.astype(float),\n",
    "            calendar_df.loc[D_COLS,event_name].replace(np.nan, 'NAN')\n",
    "        ],axis=1)\n",
    "\n",
    "        dept_id_cols = train_df.dept_id.unique().tolist()\n",
    "        cat_id_cols = train_df.cat_id.unique().tolist()\n",
    "\n",
    "        tmp_df = pd.concat([\n",
    "            tmp_df[[event_name]],\n",
    "            tmp_df.groupby([event_name])[dept_id_cols].transform(\n",
    "            lambda x: x.shift(1).rolling(len(x), min_periods=1).mean()\n",
    "            ),\n",
    "            tmp_df.groupby([event_name])[cat_id_cols].transform(\n",
    "            lambda x: x.shift(1).rolling(len(x), min_periods=1).mean()\n",
    "            )\n",
    "        ], axis=1)\n",
    "\n",
    "        tmp_df[dept_id_cols] = tmp_df[dept_id_cols]/tmp_df[dept_id_cols].rolling(56, min_periods=1).mean().shift(1)\n",
    "        tmp_df[cat_id_cols] = tmp_df[cat_id_cols]/tmp_df[cat_id_cols].rolling(56, min_periods=1).mean().shift(1)\n",
    "        tmp_df.loc[tmp_df[event_name]=='NAN', dept_id_cols+cat_id_cols]=1\n",
    "        \n",
    "        tmp_df.columns=[f'{event_name}_{col}' for col in tmp_df.columns]\n",
    "        \n",
    "        new_df = pd.concat([\n",
    "            new_df, tmp_df\n",
    "        ] ,axis=1)\n",
    "    new_df.index=D_COLS\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def create_metadata(path, d_cols, submmit=True):\n",
    "    train_df = pd.read_csv(path+'sales_train_evaluation.csv')\n",
    "    calendar_df = pd.read_csv(path+'calendar.csv')\n",
    "    sell_prices_df = pd.read_csv(path+'sell_prices.csv')\n",
    "    #sample_submission_df = pd.read_csv(path+'sample_submission.csv')\n",
    "\n",
    "    calendar_df['d'] = calendar_df.d.str.replace('d_', '').astype(int)\n",
    "    cols = train_df.columns\n",
    "    cols = [dcol2int(col) for col in cols]\n",
    "    train_df.columns=cols\n",
    "    calendar_df['date']=pd.to_datetime(calendar_df.date)\n",
    "    calendar_df.index = calendar_df.d\n",
    "    price_data, is_sell = create_is_sell_data(sell_prices_df, calendar_df, train_df)\n",
    "    \n",
    "    str_cols = [ col for col in train_df.columns if 'id' in str(col)]\n",
    "    new_columns = str_cols+d_cols\n",
    "    train_df = train_df.reindex(columns=new_columns)\n",
    "    \n",
    "    \n",
    "    train_df = pd.concat([\n",
    "        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']],\n",
    "        train_df.loc[:,d_cols]+is_sell[d_cols].replace(0, np.nan).replace(1, 0)\n",
    "    ], axis=1)\n",
    "    train_df.index = train_df.id\n",
    "    del is_sell;gc.collect()\n",
    "    \n",
    "    df = train_df.loc[:,d_cols].T.astype(float)\n",
    "    a = df.loc[d_cols[28:-56]].rolling(28, min_periods=1).sum().replace(0,np.nan)+df.loc[d_cols[28:-56]][::-1].rolling(28, min_periods=1).sum()[::-1].replace(0,np.nan)\n",
    "    a[a.notnull()]=0\n",
    "    df.loc[d_cols[28:-56]] += a\n",
    "    df = df.loc[d_cols,:].T.astype(float)\n",
    "    del a;gc.collect()\n",
    "    \n",
    "    #snap_data\n",
    "    snap_data = calendar_df[['snap_CA', 'snap_WI', 'snap_TX', 'd']]\n",
    "    snap_data.set_index('d', inplace=True)\n",
    "    \n",
    "    #dept_id_price\n",
    "    dept_id_price = price_data[d_cols]/price_data.groupby(['dept_id', 'store_id'])[d_cols].transform('mean')\n",
    "    dept_id_price = dept_id_price.T.astype(float)\n",
    "    #dept_id_price['d'] = dept_id_price.index\n",
    "    dept_id_price = dept_id_price.replace(0,np.nan)\n",
    "    \n",
    "    #cat_id_price\n",
    "    cat_id_price = price_data[d_cols]/price_data.groupby(['cat_id', 'store_id'])[d_cols].transform('mean')\n",
    "    cat_id_price = cat_id_price.T.astype(float)\n",
    "    #cat_id_price['d'] = cat_id_price.index\n",
    "    cat_id_price = cat_id_price.replace(0,np.nan)\n",
    "    \n",
    "    #price_data\n",
    "    price_data = price_data[d_cols].T\n",
    "    price_data.replace(0,np.nan, inplace=True)\n",
    "    #price_data['d']=price_data.index\n",
    "    \n",
    "    #event_df\n",
    "    event_df = create_event_data(train_df, calendar_df)\n",
    "    #event_df.reset_index(inplace=True)\n",
    "    \n",
    "    #calendar_dict\n",
    "    calendar_dict = calendar_df[['wday', 'month']].to_dict()\n",
    "    \n",
    "    return train_df, snap_data, dept_id_price, cat_id_price, price_data, event_df, calendar_dict, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_shift_roll_data(data, shift, agg={'mean', 'std'}):\n",
    "    data_2 = data.groupby(['id'])['TARGET'].apply(\n",
    "        lambda x:\n",
    "        x.shift(shift).rolling(14, min_periods=1).agg(agg)\n",
    "    )\n",
    "    for col in data_2.columns:\n",
    "        data[f'shift_roll_14_{col}'] = data_2[col]\n",
    "        \n",
    "    data_2 = data.groupby(['id'])['TARGET'].apply(\n",
    "        lambda x:\n",
    "        x.shift(shift).rolling(28, min_periods=1).agg(agg)\n",
    "    )\n",
    "    for col in data_2.columns:\n",
    "        data[f'shift_roll_28_{col}'] = data_2[col]\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def make_roll_data(data, win, agg={'mean', 'std'}):\n",
    "    data_2 = data.groupby(['id'])['TARGET'].apply(\n",
    "        lambda x:\n",
    "        x.shift(1).rolling(win, min_periods=1).agg(agg)\n",
    "    )\n",
    "    for col in data_2.columns:\n",
    "        data[f'roll_{win}_{col}'] = data_2[col]\n",
    "        \n",
    "    return data\n",
    "\n",
    "def make_diff_data(data, win):\n",
    "    data[f'diff_mean_{win}_1'] = data.groupby(['id'])['TARGET'].apply(\n",
    "        lambda x:\n",
    "        abs(x.shift(1).diff(1)).rolling(win, min_periods=1).agg('mean')\n",
    "    ) \n",
    "    \n",
    "    data[f'diff_mean_{win}_7'] = data.groupby(['id'])['TARGET'].apply(\n",
    "        lambda x:\n",
    "        abs(x.shift(1).diff(7)).rolling(win, min_periods=1).agg('mean')\n",
    "    ) \n",
    "    \n",
    "    diff_data = data.groupby(['id'])['TARGET'].apply(\n",
    "        lambda x:\n",
    "        x.shift(1).diff(7).rolling(win, min_periods=1).agg({'min', 'max'})\n",
    "    ) \n",
    "    for col in diff_data.columns:\n",
    "        data[f'diff_{col}_{win}_7'] = diff_data[col]\n",
    "        \n",
    "    diff_data = data.groupby(['id'])['TARGET'].apply(\n",
    "        lambda x:\n",
    "        x.shift(1).diff(1).rolling(win, min_periods=1).agg({'min', 'max'})\n",
    "    ) \n",
    "    for col in diff_data.columns:\n",
    "        data[f'diff_{col}_{win}_1'] = diff_data[col]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def make_shift_data(data):\n",
    "    shift=7\n",
    "    for i, p in  enumerate(range(4)):\n",
    "        data[f'shift_{i+1}'] = data.groupby(['id'])['TARGET'].shift(shift+7*p)\n",
    "    return data\n",
    "\n",
    "def create_sale_feature(data):\n",
    "    cols = data.columns.tolist()\n",
    "    \n",
    "    data = make_shift_roll_data(data=data, shift=14, agg={'mean'})\n",
    "    data = make_roll_data(data=data,win=28,agg={'mean', 'min', 'max'})\n",
    "    data = make_roll_data(data=data,win=7,agg={'mean', 'min', 'max'})\n",
    "    data = make_diff_data(data=data, win=28)\n",
    "    one_cols = [col for col in data.columns if not col in cols]\n",
    "    cols = data.columns.tolist()\n",
    "    \n",
    "    data = make_shift_data(data=data)\n",
    "    seven_cols = [col for col in data.columns if not col in cols]\n",
    "    \n",
    "    return data, one_cols, seven_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_3(path,state,d_cols,test):\n",
    "    train_df, snap_data, dept_id_price, cat_id_price, price_data, event_df, calendar_dict, df = create_metadata(path, d_cols)\n",
    "    \n",
    "    train_df[train_df.state_id==state]\n",
    "    if test:\n",
    "        train_df = train_df[train_df.id.isin(train_df.id.unique()[:2000])]\n",
    "    \n",
    "    data = train_df[d_cols].stack(dropna=False).reset_index()\n",
    "    data = data.rename(columns=set_index(data, 'TARGET'))\n",
    "    data.sort_values('d', inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data = reduce_mem_usage(data)\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    for key, value in train_df[['dept_id', 'cat_id', 'state_id', 'store_id']].to_dict().items():\n",
    "        data[key] = data.id.map(value)\n",
    "    \n",
    "    data[f'snap']=0\n",
    "    for key, value in snap_data.to_dict().items():\n",
    "        k = key.replace('snap_', '')\n",
    "        data.loc[data.state_id==k,'snap'] = data.loc[data.state_id==k, 'd'].map(value).fillna(0)\n",
    "    for shift in [-3,-2,-1,1,2,3]:\n",
    "        data[f'snap_{shift}'] = data.groupby(['id'])['snap'].shift(shift).fillna(0)\n",
    "        \n",
    "    data.drop('state_id', axis=1, inplace=True)\n",
    "\n",
    "    dept_id_price = dept_id_price.stack(dropna=False).reset_index()\n",
    "    cat_id_price = cat_id_price.stack(dropna=False).reset_index()\n",
    "\n",
    "    dept_id_price.rename(columns=set_index(dept_id_price, 'dept_id_price'), inplace=True)\n",
    "    cat_id_price.rename(columns=set_index(cat_id_price, 'cat_id_price'), inplace=True)\n",
    "\n",
    "    data = pd.merge(\n",
    "        data, dept_id_price, on=['d', 'id'], how='left'\n",
    "    )\n",
    "    data = pd.merge(\n",
    "        data, cat_id_price, on=['d', 'id'], how='left'\n",
    "    )\n",
    "\n",
    "\n",
    "    del dept_id_price,cat_id_price;gc.collect()\n",
    "\n",
    "    price_data = price_data.stack(dropna=False).reset_index()\n",
    "    price_data.rename(columns=set_index(price_data, 'price'), inplace=True)\n",
    "    data = pd.merge(\n",
    "        data, price_data, on=['d', 'id'], how='left'\n",
    "    )\n",
    "    del price_data;gc.collect()\n",
    "\n",
    "    data['wday'] = data.d.map(calendar_dict['wday'])\n",
    "    data['month'] = data.d.map(calendar_dict['month'])\n",
    "    del calendar_dict;gc.collect()\n",
    "\n",
    "\n",
    "    tmp_dic = event_df.to_dict()\n",
    "    data[f'dept_id_event_name_1']=1\n",
    "    data[f'cat_id_event_name_1']=1\n",
    "    for key, value in tmp_dic.items():\n",
    "        if 'event_name_1' in key:\n",
    "            if key[13:] in train_df.dept_id.unique().tolist():\n",
    "                data.loc[data.dept_id==key[13:], f'dept_id_{key[:12]}']=data.loc[data.dept_id==key[13:], 'd'].map(value).fillna(1)\n",
    "            if key[13:] in train_df.cat_id.unique().tolist():\n",
    "                data.loc[data.cat_id==key[13:], f'cat_id_{key[:12]}']=data.loc[data.cat_id==key[13:], 'd'].map(value).fillna(1)\n",
    "    for shift in [-7,-1,1]:\n",
    "        for event_name in ['dept_id_event_name_1', 'cat_id_event_name_1']:\n",
    "            data[f'{event_name}_shift{shift}'] = data.groupby(['id'])[event_name].shift(shift).fillna(1)\n",
    "\n",
    "    categories = [c for c in data.columns if data[c].dtype==object]\n",
    "    print(categories)\n",
    "    for c in categories:\n",
    "        if c=='id':\n",
    "            pass\n",
    "        else:\n",
    "            data[c] = pd.factorize(data[c])[0]\n",
    "    \n",
    "    data['TARGET'] = data['TARGET']*data['price']\n",
    "    data, one_cols, seven_cols = create_sale_feature(data)\n",
    "    \n",
    "    #data = data[data.d.isin(d_cols[-150:])]\n",
    "    \n",
    "    return data, one_cols, seven_cols\n",
    "\n",
    "\n",
    "def predict_cv(x_val, models):\n",
    "    preds = np.zeros(len(x_val))\n",
    "    for model in models:\n",
    "        pred = model.predict(x_val)\n",
    "        #pred = np.e**(pred)-1\n",
    "        preds+=pred/len(models)\n",
    "    return preds\n",
    "\n",
    "def train(data, params=PARAMS):\n",
    "    data = data[data.TARGET.notnull()]\n",
    "    data = data.reset_index(drop=True)\n",
    "    models = []\n",
    "    k = GroupKFold(n_splits=6)\n",
    "    categories = ['cat_id', 'dept_id', 'store_id']\n",
    "\n",
    "    y = data['TARGET']\n",
    "    group = data['group']\n",
    "    data = data.drop(columns=['id', 'd', 'TARGET', 'month', 'group'], axis=1)\n",
    "    cols = data.columns.tolist()\n",
    "\n",
    "    stop_cnt = 0\n",
    "    for trn_indx, val_indx in k.split(data, groups=group):\n",
    "        train_set = lgb.Dataset(data.loc[trn_indx,:], y.loc[trn_indx])\n",
    "        val_set = lgb.Dataset(data.loc[val_indx,:], y.loc[val_indx])\n",
    "\n",
    "        \n",
    "        model = lgb.train(\n",
    "            train_set=train_set, \n",
    "            valid_sets=[train_set, val_set],\n",
    "            params=params, num_boost_round=3000, early_stopping_rounds=100, verbose_eval=500,\n",
    "            categorical_feature=categories+['wday']\n",
    "        )\n",
    "        \n",
    "        models.append(model)\n",
    "        gc.collect()\n",
    "        stop_cnt+=1\n",
    "        if stop_cnt==3:\n",
    "            break\n",
    "    return models, cols\n",
    "\n",
    "\n",
    "def main():\n",
    "    #path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\n",
    "    path = '/Users/abcdm/Downloads/m5-forecasting-accuracy/'\n",
    "    #path = '../input/m5-forecasting-accuracy/'\n",
    "    \n",
    "    d_cols=[i+1 for i in range(1969)]\n",
    "    test = False\n",
    "    state='CA'\n",
    "    data, one_cols, seven_cols = preprocessing_3(path, state, d_cols[-200:], test=test)\n",
    "    \n",
    "    \n",
    "    data['group'] = data['wday'].astype(str)+'_'+data['dept_id'].astype(str)\n",
    "    print(data.shape)\n",
    "    day_one = d_cols[-28]\n",
    "    sub_df = pd.DataFrame()\n",
    "    for i in range(28):\n",
    "        models, cols = train(data[data.d.isin(d_cols[-150:])], params=PARAMS)\n",
    "        pred_X=data[data.d==day_one+i]\n",
    "        pred = predict_cv(pred_X[cols], models)\n",
    "        pred_X['pred'] = pred/pred_X['price']\n",
    "        sub_df = pd.concat([\n",
    "            sub_df, pred_X[['id', 'd', 'pred']]\n",
    "        ], axis=0)\n",
    "        if i%7==0 and i>0:\n",
    "            data[seven_cols] = data.groupby(['id'])[seven_cols].shift(7)\n",
    "        data[one_cols] = data.groupby(['id'])[one_cols].shift(1)\n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30490/30490 [00:10<00:00, 2818.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 69.79 Mb (50.0% reduction)\n",
      "['id', 'dept_id', 'cat_id', 'store_id']\n",
      "(6098000, 45)\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[500]\ttraining's rmse: 6.891\tvalid_1's rmse: 6.43902\n",
      "[1000]\ttraining's rmse: 6.72468\tvalid_1's rmse: 6.33564\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1400]\ttraining's rmse: 6.65464\tvalid_1's rmse: 6.30155\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[500]\ttraining's rmse: 6.67164\tvalid_1's rmse: 7.76577\n",
      "[1000]\ttraining's rmse: 6.50353\tvalid_1's rmse: 7.60402\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1400]\ttraining's rmse: 6.43002\tvalid_1's rmse: 7.55199\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[500]\ttraining's rmse: 6.83794\tvalid_1's rmse: 6.91931\n",
      "[1000]\ttraining's rmse: 6.65473\tvalid_1's rmse: 6.86148\n",
      "Early stopping, best iteration is:\n",
      "[1187]\ttraining's rmse: 6.61743\tvalid_1's rmse: 6.85208\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"int\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-97ddf45ae734>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m28\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_cols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mPARAMS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0mpred_X\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mday_one\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mpred_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pred'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mpred_X\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'price'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate list (not \"int\") to list"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
