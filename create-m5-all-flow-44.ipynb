{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "from time import time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import gc, pickle\n",
    "import ast\n",
    "import math, random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold,TimeSeriesSplit, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, log_loss\n",
    "from sklearn.linear_model import Ridge,Lasso, BayesianRidge\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "RANDOM_SEED = 2020\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1593256436.351873"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_is_sell_data(sell_prices_df, calendar_df, train_df):\n",
    "    sell_prices_df['id'] = sell_prices_df['item_id'].astype('str')+'_'+sell_prices_df['store_id']+'_evaluation'\n",
    "    sell_prices_data = sell_prices_df[sell_prices_df.wm_yr_wk.isin(calendar_df.wm_yr_wk.unique())]\n",
    "    sell_prices_data.reset_index(drop=True, inplace=True)\n",
    "    tmp = sell_prices_data.groupby(['id'])[['wm_yr_wk', 'sell_price']].apply(\n",
    "        lambda x: x.set_index('wm_yr_wk')['sell_price'].to_dict()\n",
    "    ).to_dict()\n",
    "    d = calendar_df.d\n",
    "    wm_yr_wk = calendar_df.wm_yr_wk\n",
    "    price_data = {}\n",
    "    for col in tqdm(train_df.id.unique()):\n",
    "        price_data[col] = wm_yr_wk.map(tmp[col])\n",
    "    price_data = pd.DataFrame(price_data)\n",
    "    price_data.index = d\n",
    "    is_sell = price_data.notnull().astype(float).T\n",
    "    price_data = price_data.fillna(0).T\n",
    "    \n",
    "    is_sell = pd.concat([\n",
    "        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']], is_sell\n",
    "    ], axis=1)\n",
    "    price_data = pd.concat([\n",
    "        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']], price_data\n",
    "    ], axis=1)\n",
    "    \n",
    "    return price_data, is_sell\n",
    "\n",
    "def set_index(df, name):\n",
    "    d = {}\n",
    "    for col, value in df.iloc[0,:].items():\n",
    "        try:\n",
    "            if '_evaluation' in value:\n",
    "                d[col] = 'id'\n",
    "            if 'd_' in value:\n",
    "                d[col] = 'd'\n",
    "        except:\n",
    "            if type(value)!=str:\n",
    "                d[col]=name\n",
    "    return d\n",
    "\n",
    "def dcol2int(col):\n",
    "    if col[:2]=='d_':\n",
    "        return int(col.replace('d_', ''))\n",
    "    else:\n",
    "        return col\n",
    "    \n",
    "def str_category_2_int(data):\n",
    "    categories = [c for c in data.columns if data[c].dtype==object]\n",
    "    for c in categories:\n",
    "        if c=='id' or c=='d':\n",
    "            pass\n",
    "        else:\n",
    "            data[c] = pd.factorize(data[c])[0]\n",
    "            data[c] = data[c].replace(-1, np.nan)\n",
    "    return data\n",
    "\n",
    "def select_near_event(x, event_name):\n",
    "    z = ''\n",
    "    for y in x:\n",
    "        if y in event_name:\n",
    "            z+=y+'_'\n",
    "    if len(z)==0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return z\n",
    "    \n",
    "def sort_d_cols(d_cols):\n",
    "    d_cols = [int(d.replace('d_','')) for d in d_cols]\n",
    "    d_cols = sorted(d_cols)\n",
    "    d_cols = [f'd_{d}' for d in d_cols]\n",
    "    return d_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(path, d_cols, train_d_cols):\n",
    "    train_df = pd.read_csv(path+'sales_train_evaluation.csv')\n",
    "    calendar_df = pd.read_csv(path+'calendar.csv')\n",
    "    sell_prices_df = pd.read_csv(path+'sell_prices.csv')\n",
    "    sample_submission_df = pd.read_csv(path+'sample_submission.csv')\n",
    "    \n",
    "    train_df.index = train_df.id\n",
    "    calendar_df['date']=pd.to_datetime(calendar_df.date)\n",
    "    calendar_df.index = calendar_df.d\n",
    "    price_data, is_sell = create_is_sell_data(sell_prices_df, calendar_df, train_df)\n",
    "    \n",
    "    str_cols = [ col for col in train_df.columns if 'id' in str(col)]\n",
    "    new_columns = str_cols+d_cols\n",
    "    train_df = train_df.reindex(columns=new_columns)\n",
    "    \n",
    "    train_df = pd.concat([\n",
    "        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']],\n",
    "        train_df.loc[train_df.index,d_cols]*price_data.loc[train_df.index,d_cols]\n",
    "    ], axis=1)\n",
    "    train_df = train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']+train_d_cols]\n",
    "    \n",
    "    is_sell_data = is_sell[train_d_cols]\n",
    "    groups = ['dept_id', 'store_id']\n",
    "    _id = '_'.join(groups)\n",
    "    is_sell_dept_store = is_sell.groupby(groups)[train_d_cols].transform('sum')\n",
    "    groups = ['cat_id', 'store_id']\n",
    "    _id = '_'.join(groups)\n",
    "    is_sell_cat_store = is_sell.groupby(groups)[train_d_cols].transform('sum')\n",
    "    \n",
    "    #price_data\n",
    "    price_data = price_data[train_d_cols]\n",
    "    \n",
    "\n",
    "    event_type = ['Sporting', 'Cultural', 'National', 'Religious']\n",
    "    calendar_df['quarter'] = pd.to_datetime(calendar_df['date']).dt.day.apply(lambda x: x//7)\n",
    "    cols = ['quarter', 'wday', 'event_type_1', 'snap_CA', 'snap_TX', 'snap_WI']\n",
    "    calendar_df[cols] = calendar_df[cols].shift(-28)\n",
    "    \n",
    "    calendar_df = pd.concat([\n",
    "        calendar_df.drop( columns=['wday', 'event_type_1']),\n",
    "        pd.get_dummies(calendar_df[ ['wday']].replace(np.nan, 0).astype(int).astype(str)),\n",
    "        pd.get_dummies( calendar_df[['event_type_1']])\n",
    "    ], axis=1)\n",
    "   \n",
    "    calendar_df=calendar_df[calendar_df.d.isin(train_d_cols)]\n",
    "    return train_df, price_data, is_sell_data, is_sell_dept_store, is_sell_cat_store, calendar_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduce_mem_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.get_dummies(train_df[[ 'dept_id', 'store_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, train_df, price_data, is_sell_data, is_sell_dept_store, is_sell_cat_store, calendar_df):\n",
    "        self.d_cols = sort_d_cols([d for d in train_df.columns if  'd_' in d])\n",
    "        self.data = train_df[self.d_cols].astype(float)\n",
    "        self.index = train_df.index\n",
    "        self.cat_data = pd.get_dummies(train_df[[ 'dept_id', 'store_id']]).astype(float)\n",
    "        self.price_data = price_data.loc[train_df.index, self.d_cols].astype(float)\n",
    "        self.is_sell_data = is_sell_data.loc[train_df.index, self.d_cols].astype(float)\n",
    "        self.is_sell_dept_store = is_sell_dept_store.loc[train_df.index, self.d_cols].astype(float)\n",
    "        self.is_sell_cat_store = is_sell_cat_store.loc[train_df.index, self.d_cols].astype(float)\n",
    "        use_cols=[f'snap_CA',f'snap_WI',f'snap_TX', 'quarter' , 'event_type_1_Cultural', 'event_type_1_National', 'event_type_1_Religious',\n",
    "         'event_type_1_Sporting', 'month', 'year','wday']\n",
    "        self.calendar_df = calendar_df.loc[use_cols,d_cols].astype(float)\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        d = int(random.choice(self.d_cols[:-112]).replace('d_', ''))\n",
    "        train_d_cols = [f'd_{d+i}' for i in range(112)]\n",
    "        trn_d_cols = train_d_cols[:-28]\n",
    "        val_d_cols = train_d_cols[-28:]\n",
    "        \n",
    "        _id = self.index[idx]\n",
    "        for s in ['CA', 'TX', 'WI']:\n",
    "            if s in _id:\n",
    "                state=s\n",
    "                break\n",
    "        \n",
    "        calendar_cols = [f'snap_{state}', 'quarter' , 'event_type_1_Cultural', 'event_type_1_National', 'event_type_1_Religious',\n",
    "                         'event_type_1_Sporting']\n",
    "        x_1 = pd.concat([\n",
    "            self.data.iloc[[idx],:][trn_d_cols],\n",
    "            self.price_data.iloc[[idx],:][trn_d_cols],\n",
    "            self.is_sell_data.iloc[[idx],:][trn_d_cols],\n",
    "            self.is_sell_dept_store.iloc[[idx],:][trn_d_cols],\n",
    "            self.is_sell_cat_store.iloc[[idx],:][trn_d_cols],\n",
    "            self.calendar_df.loc[calendar_cols,trn_d_cols]\n",
    "        ],axis=0)\n",
    "        \n",
    "        \n",
    "        calendar_cols= ['month', 'year','wday_1', 'wday_2', 'wday_3', 'wday_4', 'wday_5', 'wday_6', 'wday_7']\n",
    "        x_2 = self.cat_data.iloc[idx,:].values.tolist()+calendar_df.loc[calendar_cols, trn_d_cols[-1]].values.tolist()\n",
    "        \n",
    "        x_1 = torch.FloatTensor(x_1.values)\n",
    "        x_2 = torch.FloatTensor(x_2)\n",
    "        \n",
    "        y = torch.FloatTensor(self.data.loc[_id,:][val_d_cols].values.astype(float))\n",
    "        \n",
    "        return x_1, x_2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish(input):\n",
    "    return input * torch.tanh(nn.functional.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return mish(input)\n",
    "    \n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "class RAdam(Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:            \n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_rgrssor(nn.Module):\n",
    "    def __init__(self, in_size):\n",
    "        super(My_rgrssor, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_size, in_size*2, kernel_size=2),\n",
    "            Mish(),\n",
    "            nn.Conv1d(in_size*2, in_size*2, kernel_size=4),\n",
    "            Mish(),\n",
    "            nn.Conv1d(in_size*2, in_size*4, kernel_size=8),\n",
    "            nn.AdaptiveAvgPool1d(5)\n",
    "        )\n",
    "        in_2=220+26\n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(in_2, in_2*2),\n",
    "            Mish(),\n",
    "            nn.Linear(in_2*2, 28)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x_1, x_2):\n",
    "        x_1 = self.conv(x_1)\n",
    "        x_1 = x_1.flatten(1)\n",
    "        x_1 = torch.cat((x_1, x_2), dim=1)\n",
    "        x_1 = self.lin(x_1)\n",
    "        return nn.ReLU()(x_1)+1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweedieLoss(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(TweedieLoss, self).__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, preds, true):\n",
    "        loss = - true * ((preds**(1-self.p))/ (1-self.p))+((preds**(2- self.p))/ (2-self.p))\n",
    "        loss = loss.mean(1)\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6387636b2340bfa49ff0ca0b152a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30490), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 9s, sys: 10.9 s, total: 1min 20s\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\n",
    "#path = '/Users/abcdm/Downloads/m5-forecasting-accuracy/'\n",
    "#path = '../input/m5-forecasting-accuracy/'\n",
    "\n",
    "d_cols=[f'd_{i+1}' for i in range(1969)]\n",
    "trn_d_cols = d_cols[:-28]\n",
    "trn_d_cols = trn_d_cols[-730:]\n",
    "\n",
    "train_df, price_data, is_sell_data, is_sell_dept_store, is_sell_cat_store, calendar_df = preprocessing(path, d_cols, trn_d_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader):\n",
    "    \n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    num_epochs = 40\n",
    "    best_epoch = -1\n",
    "    best_score = 10000\n",
    "    early_stoppping_cnt = 0\n",
    "    best_model = model\n",
    "    \n",
    "    optimizer = RAdam(model.parameters(), lr=4e-4)\n",
    "    criterion = TweedieLoss(1.15)\n",
    "    eval_loss = nn.MSELoss()\n",
    "    \n",
    "    try:\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time()\n",
    "            model.train()\n",
    "            avg_loss = 0.\n",
    "\n",
    "            for x_1, x_2, y in tqdm(data_loader):\n",
    "                optimizer.zero_grad()\n",
    "                x_1 = x_1.to(DEVICE)\n",
    "                x_2 = x_2.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "\n",
    "                preds = model(x_1, x_2)\n",
    "\n",
    "                loss = criterion(preds, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                tmp_eval_loss = torch.sqrt(eval_loss(preds, y))\n",
    "                avg_loss += tmp_eval_loss.item() / len(data_loader)\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  time: {elapsed:.0f}s')\n",
    "            \n",
    "    except:\n",
    "        return model\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4041825a0a7043b49b76a249f9fa6950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=61), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_data_set= My_Dataset(train_df, price_data, is_sell_data, is_sell_dept_store, is_sell_cat_store, calendar_df)\n",
    "data_loader = torch.utils.data.DataLoader(my_data_set, batch_size=500, shuffle=True)\n",
    "for x_1,x_2 ,y in tqdm(data_loader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 28])"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = My_rgrssor(11)\n",
    "out = model(x_1,x_2)\n",
    "model(x_1,x_2).size()\n",
    "loss = TweedieLoss(1.1)\n",
    "loss(out, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1a1a3fd67a432585bc47064752d1d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=61), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_model(model, data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(73.4361, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
