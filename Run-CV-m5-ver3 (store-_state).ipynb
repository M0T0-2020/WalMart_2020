{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport numpy as np\nimport pandas as pd\nimport datetime\nfrom catboost import CatBoostClassifier\nimport lightgbm as lgb\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport ast\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, mean_squared_error\nfrom sklearn import linear_model\n\nfrom tqdm import tqdm_notebook as tqdm\nimport gc, pickle\n\nimport datetime\nfrom catboost import CatBoostClassifier\nfrom time import time\nfrom collections import Counter\nfrom scipy import stats\n\nfrom sklearn import preprocessing","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_is_sell_data(sell_prices_df, calendar_df, train_df):\n    sell_prices_df['id'] = sell_prices_df['item_id'].astype('str')+'_'+sell_prices_df['store_id']+'_validation'\n    sell_prices_data = sell_prices_df[sell_prices_df.wm_yr_wk.isin(calendar_df.wm_yr_wk.unique())]\n    sell_prices_data.reset_index(drop=True, inplace=True)\n    tmp = sell_prices_data.groupby(['id'])[['wm_yr_wk', 'sell_price']].apply(\n        lambda x: x.set_index('wm_yr_wk')['sell_price'].to_dict()\n    ).to_dict()\n    d = calendar_df.d\n    wm_yr_wk = calendar_df.wm_yr_wk\n    price_data = {}\n    for col in tqdm(train_df.id.unique()):\n        price_data[col] = wm_yr_wk.map(tmp[col])\n    price_data = pd.DataFrame(price_data)\n    price_data.index = d\n    is_sell = price_data.notnull().astype(float).T\n    price_data = price_data.fillna(0)\n    \n    is_sell.index=train_df.id\n    train_df.index=train_df.id\n    is_sell = pd.concat([\n        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']], is_sell\n    ], axis=1)\n    price_data = pd.concat([\n        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']], price_data.T  \n    ], axis=1)\n    \n    return price_data, is_sell\n\ndef set_index(df, name):\n    d = {}\n    for col, value in df.iloc[0,:].items():\n        if type(col)==str:\n            if type(df[col].values[0])!=str:\n                v = 'd'\n            else:\n                v='id'\n        else:\n            v=name\n        d[col]=v\n    return d\n\ndef dcol2int(col):\n    if col[:2]=='d_':\n        return int(col.replace('d_', ''))\n    else:\n        return col","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_category_data(data, dfT):\n    C = ['item_id', 'cat_id', 'dept_id', 'store_id', 'state_id']\n    tmp = dfT[C]\n    tmp = tmp.to_dict()\n    for key, value in tmp.items():\n        data[key] = data.id.map(value)\n    return data\ndef create_calendar_data_dict(calendar_df, use_d_cols, _state):\n    calendar_data_dict = {}\n    calendar_data_dict.update(\n        calendar_df[calendar_df.d.isin(use_d_cols)][['wday', 'month', f'snap_{_state}']].to_dict()\n    )\n    \n    for shift in [0,1]:\n        tmp = calendar_df[calendar_df.d.isin(use_d_cols)][[f'snap_{_state}']].shift(shift)\n        tmp=tmp.rename(columns={f'snap_{_state}':f'snap_{_state}_shift{shift}'})\n        calendar_data_dict.update(tmp.to_dict())\n        \n    return calendar_data_dict\ndef create_groupby_price_data(data, store_price_data, use_d_cols, groups=['dept_id', 'cat_id']):\n    for group in groups:\n        tmp = store_price_data[use_d_cols]/store_price_data.groupby([group])[use_d_cols].transform('mean')\n        tmp = tmp.T.astype(float)\n        tmp = tmp.rolling(7, min_periods=1).mean().stack(dropna=False).reset_index()\n        tmp = tmp.rename(columns=set_index(tmp, f'price_mean_{group}'))\n        data = pd.merge(data, tmp, on=['d', 'id'], how='left')\n    \n    return data","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_event_data(data, name):\n    a = pd.concat([\n        train_df.groupby(['item_id'])[d_cols].agg('mean').T.astype(float),\n        calendar_df[name]\n    ],axis=1)\n    \n    cols = a.columns[:-1]\n    a[name]=a[name].fillna('NAN')\n    a[cols] = a.groupby([name])[cols].transform(\n        lambda x:x.sort_index().rolling(len(x), min_periods=1).mean().shift(1)\n    )/train_df.groupby(['item_id'])[d_cols].agg('mean').T.astype(float).shift(1).rolling(100, min_periods=1).mean()[cols]\n    \n    a.loc[a[name]=='NAN', cols]=0\n    a.loc[a[name]=='NAN', name]=np.nan\n    a.drop(name, axis=1, inplace=True)\n    \n    for shift in [-3,-2,-1,0,1,2,3]:\n        tmp_a = a.shift(shift).stack().reset_index()\n        tmp_a = tmp_a.rename(columns=set_index(tmp_a, 'value'))\n        tmp_a.index = tmp_a.id.astype(str)+'_'+tmp_a.d.astype(str)\n        tmp_a = tmp_a[tmp_a.index.isin(data.d.unique().tolist())]\n        tmp_a.drop(columns=['d', 'id'], inplace=True)\n        data[f'{name}_shift{shift}'] = (data.item_id.astype(str)+'_'+data.d.astype(str)).map(tmp_a['value']).apply(\n            lambda x: float(x) if float(x)>0 else 0\n        )\n    del a, tmp_a; gc.collect()\n    return data","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_roll_data(data, df, wins, use_d_cols):\n    for win in wins:\n        for agg in ['mean', 'std', 'skew']:\n            tmp = df.rolling(window=win, min_periods=1).agg(agg).loc[use_d_cols].stack(dropna=False).reset_index()\n            tmp = tmp.rename(columns=set_index(tmp, f'_diff_roll{win}_{agg}'))\n            data = pd.merge(data, tmp, on=['d', 'id'], how='left')\n    return data\n\ndef create_diff_data(data, df, i, wins, use_d_cols, name=None):\n    a = df.diff(i)\n    for win in wins:\n        #for agg in ['mean', 'std', 'skew']:\n        for agg in ['mean']:\n            tmp = a.rolling(window=win, min_periods=1).agg(agg).loc[use_d_cols].stack(dropna=False).reset_index()\n            if name is None:\n                tmp = tmp.rename(columns=set_index(tmp, f'diff_{i}_roll{win}_{agg}'))\n            else:\n                tmp = tmp.rename(columns=set_index(tmp, f'{name}_diff_{i}_roll{win}_{agg}'))\n            data = pd.merge(data, tmp, on=['d', 'id'], how='left')\n    a = abs(a)\n    for win in wins:\n        #for agg in ['mean', 'std', 'skew']:\n        for agg in ['mean']:\n            tmp = a.rolling(window=win, min_periods=1).agg(agg).loc[use_d_cols].stack(dropna=False).reset_index()\n            if name is None:\n                tmp = tmp.rename(columns=set_index(tmp, f'abs_diff_{i}_roll{win}_{agg}'))\n            else:\n                tmp = tmp.rename(columns=set_index(tmp, f'{name}_abs_diff_{i}_roll{win}_{agg}'))\n            data = pd.merge(data, tmp, on=['d', 'id'], how='left')\n    return data\n\ndef create_group_sell_data(data, dfT, use_d_cols, groups=['dept_id', 'cat_id']):\n    for group in groups:\n        tmp = dfT[use_d_cols]/ dfT.groupby([group])[use_d_cols].transform('mean')\n        tmp = tmp.T.astype(float)\n        data = create_diff_data(data, tmp, 7, wins=[360, 30, 28, 7], use_d_cols=use_d_cols, name=f'By_{group}')\n        data = create_diff_data(data, tmp, 1, wins=[360, 7], use_d_cols=use_d_cols, name=f'By_{group}')\n        data = create_diff_data(data, tmp, 28, wins=[360, 7], use_d_cols=use_d_cols, name=f'By_{group}')\n    return data\n\n\ndef create_shift_data(data, df, shifts, use_d_cols, name=None):\n    tmp_df = df[data.id.unique()]+df[data.id.unique()].shift(7)+df[data.id.unique()].shift(14)+df[data.id.unique()].shift(21)\n    \n    for i, shift in enumerate(shifts):\n        tmp = tmp_df.shift(shift).loc[use_d_cols].stack(dropna=False).reset_index()\n        if name is None:\n            tmp = tmp.rename(columns=set_index(tmp, f'shift_no{i+1}'))\n        else:\n            tmp = tmp.rename(columns=set_index(tmp, f'{name}_shift_no{i+1}'))\n        data = pd.merge(data, tmp, on=['d', 'id'], how='left')\n    return data\n\ndef create_group_shift_data(data, df, dfT, shifts, use_d_cols):\n    for group in ['dept_id', 'cat_id']:\n        _tmp = dfT.groupby([group])[use_d_cols].transform('mean').T.astype(float)\n        data = create_shift_data(data, _tmp, shifts, use_d_cols, name=f'By_{group}')\n    return data","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_input(train_d_cols, _state):\n    use_d_cols = [i for i in range(train_d_cols[0]-380,train_d_cols[0])]+train_d_cols\n    ids = train_df[train_df.state_id==_state].id.unique().tolist()\n    \n    df = train_df.loc[ids,:]\n    df = df.T.loc[d_cols]\n    df = pd.DataFrame(df.values.astype(float), index=d_cols, columns=df.columns)\n\n    df['d']=df.index\n    calendar_dict = calendar_df[[\n        'date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'd', 'event_name_1',\n        'event_type_1', 'event_name_2', 'event_type_2',f'snap_{_state}'\n    ]].set_index('d').to_dict()\n\n    for key, value in calendar_dict.items():\n        df[key] = df['d'].map(value)\n    df.drop('d', axis=1, inplace=True)\n    \n    store_price_data = price_data.loc[ids,:]\n    store_is_sell = is_sell.loc[ids,:]\n    store_price_data[[col for col in store_price_data.columns if type(col)!=str]] =\\\n            store_price_data[[col for col in store_price_data.columns if type(col)!=str]].replace(0, np.nan)\n    \n    \n    df = df.loc[use_d_cols]\n    dfT = train_df.loc[ids,['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']+use_d_cols]\n    store_is_sell = store_is_sell[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']+use_d_cols]\n    store_price_data = store_price_data[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']+use_d_cols]\n    \n    data = df.loc[train_d_cols, ids].stack(dropna=False).reset_index().rename(columns={'level_0':'d', 'id':'id', 0:'TARGET'})\n    \n    \n    data = fe(data, ids, df, dfT, store_price_data, calendar_df, use_d_cols, train_d_cols, _state)\n    \n    return data\n\ndef fe(data, ids, df, dfT, store_price_data, calendar_df, use_d_cols, train_d_cols, _state):\n    data = create_category_data(data, dfT)\n    \n    calendar_data_dict = create_calendar_data_dict(calendar_df, use_d_cols, _state)\n    for key, values in calendar_data_dict.items():\n        data[key] = data.d.map(values)\n        \n        \n    for name in ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']:\n        data = create_event_data(data, name)\n    \n    data = create_groupby_price_data(data, store_price_data, use_d_cols)\n    \n    data = create_diff_data(data, df[ids], 7, wins=[30], use_d_cols=train_d_cols)\n    data = create_diff_data(data, df[ids], 1, wins=[360, 30, 7], use_d_cols=train_d_cols)\n    data = create_diff_data(data, df[ids], 28, wins=[30], use_d_cols=train_d_cols)\n    #data = create_roll_data(data, df[ids], [7,28,56], use_d_cols)\n    \n    categories = [c for c in data.columns if data[c].dtype==object]\n    print(categories)\n    for c in categories:\n        if c=='id':\n            pass\n        else:\n            data[c] = pd.factorize(data[c])[0]\n    \n    return data, df, dfT","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n            'n_estimators':2000,\n            'boosting_type': 'gbdt',\n            'objective': 'poisson',\n            'metric': 'rmse',\n            'subsample': 0.75,\n            'subsample_freq': 1,\n            'learning_rate': 0.07,\n            'feature_fraction': 0.85,\n            'max_depth': 15,\n            'lambda_l1': 1,  \n            'lambda_l2': 1,\n            'verbose': 100,\n            'random_state':123,\n            }\n\ndef train(data, df, dfT):\n    train_d_cols = data.d.unique().tolist()\n    use_d_cols = [i for i in range(train_d_cols[0]-380,train_d_cols[0])]+train_d_cols\n    log = []\n    for for_predict in range(28):\n        print(\"\"\"\n        ###########################################\n         ############## {} - {}  ##############\n        ###########################################\"\"\".format(_state, for_predict+1))\n        \n        g_for_predict = for_predict//7 + 1\n        shifts = [7*i for i in [g_for_predict, g_for_predict+2, g_for_predict+3, g_for_predict+4]]\n        if for_predict==0:\n                  split=int(0.3*len(train_d_cols[shifts[0]:]))\n        \n        if for_predict%7==0:\n            if for_predict>0:\n                data.drop(columns=shift_cols, inplace=True)\n            data = create_shift_data(data, df, shifts, use_d_cols)\n            data = create_group_shift_data(data, df, dfT, shifts, use_d_cols)\n                \n        \n        diff_cols = [col for col in data.columns if ('diff' in col)]\n        shift_cols = [col for col in data.columns if ('shift' in col) and (not 'diff' in col)and (not 'snap' in col)]\n        data = pd.concat([\n            data.drop(columns=diff_cols),\n            data[diff_cols+['id']].groupby(['id']).transform(\n                lambda x: x.sort_index().shift(1)\n            )\n        ], axis=1)\n        \n        data =pd.concat([\n            data['id'],\n            data[[c for c in data.columns if c!='id']].astype(float)\n        ], axis=1)\n        \n        y = data[data.TARGET.notnull()][['TARGET']+['d', 'id']]\n        X = data[data.TARGET.notnull()].drop(columns=['id',  'TARGET', 'item_id','state_id']).astype(float)\n        \n        \n        split=int(0.3*len(train_d_cols[shifts[0]:]))\n        x_train, x_val = X[X.d.isin(train_d_cols[shifts[0]:-split])], X[X.d.isin(train_d_cols[-split:])]\n        y_train, y_val = y[y.d.isin(train_d_cols[shifts[0]:-split])], y[y.d.isin(train_d_cols[-split:])]\n        \n        #x_train, x_val = X[X.d.isin(train_d_cols[:-3])], X[X.d.isin(train_d_cols[-3:])]\n        #y_train, y_val = y[y.d.isin(train_d_cols[:-3])], y[y.d.isin(train_d_cols[-3:])]\n        \n        x_train.drop('d', axis=1, inplace=True)\n        x_val.drop('d', axis=1, inplace=True)\n        y_train = y_train['TARGET'].astype(float)\n        y_val = y_val['TARGET'].astype(float)\n        if for_predict==0:\n            print(x_train.shape)\n            print(x_val.shape)\n            \n        train_set = lgb.Dataset(x_train, np.log1p(y_train))\n        val_set = lgb.Dataset(x_val, np.log1p(y_val))\n        \n        categories = ['cat_id', 'dept_id', 'store_id']\n        \n        model = lgb.train(\n                    train_set=train_set, \n                    valid_sets=[train_set, val_set],\n                    params=params, num_boost_round=3000, early_stopping_rounds=100, verbose_eval=500,\n            categorical_feature=categories+['wday', 'month']\n        )\n        \n        \n        pred = model.predict(x_train)\n        pred = np.e**(pred)-1\n        print(f'TRAIN    {np.sqrt(mean_squared_error(pred, y_train))}')\n        \n        pred = model.predict(x_val)\n        pred = np.e**(pred)-1\n        print(f'VAL    {np.sqrt(mean_squared_error(pred, y_val))}')\n        \n        importance = pd.DataFrame()\n        importance['importance'] = model.feature_importance(importance_type='gain')\n        importance['importance'] = preprocessing.minmax_scale(importance.importance)\n        importance['columns'] = x_val.columns\n        log.append([importance])\n        gc.collect()\n        del train_set, val_set, X, x_train, x_val, y_train, y_val;gc.collect()\n    gc.collect()\n    \n    with open(f'train_log_{_state}.pickle', 'wb') as f:\n        pickle.dump(log, f)\n        \ndef plot_importance(model, col, num):\n    importance = pd.DataFrame()\n    importance['importance'] = model.feature_importance(importance_type='gain')\n    importance['importance'] = preprocessing.minmax_scale(importance.importance)\n    importance.index = col\n    importance = importance['importance']\n\n    plt.figure(figsize=(10,30))\n    importance.sort_values( ascending=True).plot('barh')\n    plt.title(f'for {num}')\n    \ndef train_predict(data, df, dfT, _store):\n    train_d_cols = data[data.TARGET.notnull()].d.unique().tolist()\n    use_d_cols = [i for i in range(train_d_cols[0]-380,train_d_cols[0])]+data.d.unique().tolist()\n    ids = data.id.unique().tolist()\n    \n    sub_df = pd.DataFrame()\n    #sub_df.index = ids\n    models = {}\n    for for_predict in range(28):\n        print(\"\"\"\n        ############################################\n         ############## {} - {}  ##############\n        ############################################\"\"\".format(_state,for_predict+1))\n        \n        g_for_predict = for_predict//7 + 1\n        shifts = [7*i for i in [g_for_predict, g_for_predict+3]]\n        \n        if for_predict%7==0:\n            if for_predict>0:\n                data.drop(columns=shift_cols, inplace=True)\n                \n            data = create_shift_data(data, df, shifts, use_d_cols)\n            data = create_group_shift_data(data, df, dfT, shifts, use_d_cols)\n                \n        \n        diff_cols = [col for col in data.columns if ('diff' in col)]\n        shift_cols = [col for col in data.columns if ('shift' in col) and (not 'diff' in col)and (not 'snap' in col)]\n        data = pd.concat([\n            data.drop(columns=diff_cols),\n            data[diff_cols+['id']].groupby(['id']).transform(\n                lambda x: x.sort_index().shift(1)\n            )\n        ], axis=1)\n        \n        data =pd.concat([\n            data['id'],\n            data[[c for c in data.columns if c!='id']].astype(float)\n        ], axis=1)\n        \n        sub_df = pd.concat([\n            sub_df,\n            data[data.d==TARGET_D_COLS[for_predict]]\n        ],axis=0)\n        \n        y = data[data.TARGET.notnull()][['TARGET']+['d']].astype(float)\n        X = data[data.TARGET.notnull()].drop(columns=['id', 'TARGET', 'item_id','state_id']).astype(float)\n        \n        split=int(0.3*len(train_d_cols[shifts[0]:]))\n        x_train, x_val = X[X.d.isin(train_d_cols[shifts[0]:-split])], X[X.d.isin(train_d_cols[-split:])]\n        y_train, y_val = y[y.d.isin(train_d_cols[shifts[0]:-split])], y[y.d.isin(train_d_cols[-split:])]\n        \n        #x_train, x_val = X[X.d.isin(train_d_cols[:-3])], X[X.d.isin(train_d_cols[-3:])]\n        #y_train, y_val = y[y.d.isin(train_d_cols[:-3])], y[y.d.isin(train_d_cols[-3:])]\n        \n        x_train.drop('d', axis=1, inplace=True)\n        x_val.drop('d', axis=1, inplace=True)\n        y_train = y_train['TARGET']\n        y_val = y_val['TARGET']\n        if for_predict==0:\n            print(x_train.shape)\n            print(x_val.shape)\n        train_set = lgb.Dataset(x_train, y_train)\n        val_set = lgb.Dataset(x_val, y_val)\n        \n        categories = ['cat_id', 'dept_id', 'store_id']\n        \n        model = lgb.train(\n                    train_set=train_set, \n                    valid_sets=[train_set, val_set],\n                    params=params, num_boost_round=3000, early_stopping_rounds=100, verbose_eval=500,\n            categorical_feature=categories+['wday', 'month']\n        )\n        \n        models[f'{_state}_model_{for_predict+1}']=model\n        \n        plot_importance(model, x_train.columns, for_predict+1)\n        gc.collect()\n        \n        \n        del train_set, val_set, X, y, x_train, x_val, y_train, y_val;gc.collect()\n    \n    gc.collect()\n    \n    sub_df.reset_index(drop=True, inplace=True)\n    sub_df.to_pickle(f'{_state}_csv.pickle')\n    with open(f'models_{_state}.pickle', 'wb') as f:\n        pickle.dump(models, f)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\n#path = '/Users/abcdm/Downloads/m5-forecasting-accuracy/'\npath = '../input/m5-forecasting-accuracy/'\n\ntrain_df = pd.read_csv(path+'sales_train_validation.csv')\ncalendar_df = pd.read_csv(path+'calendar.csv')\nsell_prices_df = pd.read_csv(path+'sell_prices.csv')\nsample_submission_df = pd.read_csv(path+'sample_submission.csv')\n\n\ncalendar_df['d'] = calendar_df.d.str.replace('d_', '').astype(int)\ncols = train_df.columns\ncols = [dcol2int(col) for col in cols]\ntrain_df.columns=cols\ncalendar_df['date']=pd.to_datetime(calendar_df.date)\ncalendar_df.index = calendar_df.d\nprice_data, is_sell = create_is_sell_data(sell_prices_df, calendar_df, train_df)\n\nd_cols = [ col for col in train_df.columns if type(col)!=str ]\nfor i in range(1,29):\n    train_df[d_cols[-1]+i]=np.nan\nd_cols = [ col for col in train_df.columns if type(col)!=str ]\n\ntrain_df = pd.concat([\n    train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']],\n    train_df[d_cols]+is_sell[d_cols].replace(0, np.nan).replace(1, 0)\n], axis=1)","execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=30490.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e64d690d86e341c8b0d79e065f95eaaf"}},"metadata":{}},{"output_type":"stream","text":"\nCPU times: user 1min, sys: 6.11 s, total: 1min 6s\nWall time: 1min 6s\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#_state = 'CA'\n#lag=28\n#train_d_cols = d_cols[-(lag+60+56):-lag]\n#data, df, dfT = make_input(train_d_cols, _state)\n#train(data, df, dfT)","execution_count":10,"outputs":[{"output_type":"stream","text":"['id', 'item_id', 'cat_id', 'dept_id', 'store_id', 'state_id']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train(data, df, dfT)","execution_count":null,"outputs":[{"output_type":"stream","text":"\n        ###########################################\n         ############## CA - 1  ##############\n        ###########################################\n(938804, 60)\n(390272, 60)\nTraining until validation scores don't improve for 100 rounds\n[500]\ttraining's rmse: 0.481739\tvalid_1's rmse: 0.498395\n[1000]\ttraining's rmse: 0.477831\tvalid_1's rmse: 0.498122\nEarly stopping, best iteration is:\n[1153]\ttraining's rmse: 0.476766\tvalid_1's rmse: 0.498057\nTRAIN    2.0793618724227554\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#(939092, 83)\n#(390272, 83)\n#train(data, df, dfT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data[data.d.isin(train_d_cols[-7:])].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#a = train(data[data.d.isin(train_d_cols[-7:])], df, dfT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data[data.d.isin(train_d_cols[-7:][:-3])]['TARGET']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#np.sqrt(mean_squared_error(a, data[data.d.isin(train_d_cols[-7:][:-3])]['TARGET']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#plt.hist(a)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#lag=28\n#TARGET_D_COLS = d_cols[-lag:]\n\n#train_d_cols = d_cols[-(lag+160+56):-lag]#+TARGET_D_COLS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def run_all(_state):\n    lag=28\n    train_d_cols = d_cols[-(lag+160+56):-lag]#+TARGET_D_COLS\n    data, df, dfT = make_input(train_d_cols, _state)\n    train(data, df, dfT)\n    \nfor _state in ['CA', 'WI', 'TX']:\n    run_all(_state)\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#for _store in ['TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3']:\n #   run_all(_store)\n  #  gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#'CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3'\n#_store='CA_1'\n#data, df, dfT = make_input(train_d_cols, _store)\n#train(data, df, dfT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"}},"nbformat":4,"nbformat_minor":4}