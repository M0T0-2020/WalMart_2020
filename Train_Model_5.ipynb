{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\n",
    "#path = '/Users/abcdm/Downloads/m5-forecasting-accuracy/'\n",
    "#path = '../input/m5-forecasting-accuracy/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os, gc\n",
    "import termcolor\n",
    "from typing import Union\n",
    "\n",
    "import math, random\n",
    "import pickle\n",
    "import datetime, time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(train_df, calendar_df, sell_prices_df):\n",
    "    sell_prices_df['id'] = sell_prices_df['item_id'].astype('str')+'_'+sell_prices_df['store_id']+'_validation'\n",
    "    d_cols = [f'd_{i}' for i in range(1,1914)]\n",
    "    \n",
    "    event_type_1 = pd.get_dummies(calendar_df.event_type_1)\n",
    "    event_type_1.columns = [f'{col}_event_type_1' for col in event_type_1.columns]\n",
    "    event_type_2 = pd.get_dummies(calendar_df.event_type_1)\n",
    "    event_type_2.columns = [f'{col}_event_type_2' for col in event_type_2.columns]\n",
    "    calendar_data = pd.concat([\n",
    "        calendar_df.drop(columns=['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2'])[['wday', 'd','month','snap_CA', 'snap_TX', 'snap_WI']],\n",
    "        event_type_1,\n",
    "        event_type_2\n",
    "    ], axis=1)\n",
    "    calendar_data = calendar_data.set_index('d').T\n",
    "    \n",
    "    \n",
    "    \n",
    "    sell_prices_data = sell_prices_df[sell_prices_df.wm_yr_wk.isin(calendar_df.wm_yr_wk.unique())]\n",
    "    sell_prices_data.reset_index(drop=True, inplace=True)\n",
    "    tmp = sell_prices_data.groupby(['id'])[['wm_yr_wk', 'sell_price']].apply(lambda x: x.set_index('wm_yr_wk')['sell_price'].to_dict()).to_dict()\n",
    "    d = calendar_df.d\n",
    "    wm_yr_wk = calendar_df.wm_yr_wk\n",
    "    price_data = {}\n",
    "    for col in tqdm(train_df.id.unique()):\n",
    "        price_data[col] = wm_yr_wk.map(tmp[col])\n",
    "    price_data = pd.DataFrame(price_data)\n",
    "    price_data.index = d\n",
    "    \n",
    "    \n",
    "    is_sell = price_data.notnull().astype(float).T\n",
    "    price_data = price_data.fillna(0)\n",
    "    \n",
    "    train_df = train_df.T\n",
    "    train_df.columns = train_df.loc['id', :].values\n",
    "    train_df = train_df.T\n",
    "    \n",
    "    return train_df, calendar_df, calendar_data, price_data, is_sell\n",
    "\n",
    "\n",
    "def make_calendar_data(calendar_data, train_cols):\n",
    "    calendar_index = [\n",
    "        'wday', 'month',\n",
    "        'Cultural_event_type_1', 'National_event_type_1', 'Religious_event_type_1', 'Sporting_event_type_1',\n",
    "        'Cultural_event_type_2', 'National_event_type_2', 'Religious_event_type_2', 'Sporting_event_type_2'\n",
    "    ]\n",
    "    calendar = calendar_data.loc[calendar_index,:]\n",
    "    event_index = [\n",
    "        'Cultural_event_type_1', 'National_event_type_1', 'Religious_event_type_1', 'Sporting_event_type_1',\n",
    "        'Cultural_event_type_2', 'National_event_type_2', 'Religious_event_type_2', 'Sporting_event_type_2'\n",
    "    ]\n",
    "    for shift in [3, 7, 14, 28]:\n",
    "        tmp_calendar = calendar.loc[event_index, :]\n",
    "        tmp_calendar = tmp_calendar.T.shift(-shift).T\n",
    "        tmp_calendar.index = [f'{col}_shift{shift}' for col in tmp_calendar.index]\n",
    "        calendar = pd.concat([\n",
    "            calendar,\n",
    "            tmp_calendar\n",
    "        ], axis=0)\n",
    "    calendar = calendar[train_cols]\n",
    "    calendar = torch.FloatTensor(calendar.values.astype(float))\n",
    "    return calendar\n",
    "\n",
    "def make_data(train_cols, state, train_df, calendar_data, price_data, is_sell_data, sample_submission_df):\n",
    "    data_train = train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']+train_cols]\n",
    "    train_product = sample_submission_df[(sample_submission_df.id.str.contains(state))&(sample_submission_df.id.str.contains('_validation'))].id.values\n",
    "    #train_product = data_train[data_train.state_id==state]['id'].unique()\n",
    "    \n",
    "    data = data_train.loc[train_product,train_cols]\n",
    "    \n",
    "    calendar_index = [ f'snap_{state}']\n",
    "    event_index = [ f'snap_{state}']\n",
    "    calendar = calendar_data.loc[calendar_index,:]\n",
    "    for shift in [3, 7, 14, 28]:\n",
    "        tmp_calendar = calendar.loc[event_index, :]\n",
    "        tmp_calendar = tmp_calendar.T.shift(shift).T\n",
    "        tmp_calendar.index = [f'{col}_shift{shift}' for col in tmp_calendar.index]\n",
    "        calendar = pd.concat([\n",
    "            calendar,\n",
    "            tmp_calendar\n",
    "        ], axis=0)\n",
    "    calendar = calendar[train_cols]\n",
    "    \n",
    "    price = price_data.T[train_cols].loc[train_product,:]\n",
    "    past_price_1 = price_data.loc[:,train_product].shift(3).T[train_cols]\n",
    "    past_price_2 = price_data.loc[:,train_product].shift(7).T[train_cols]\n",
    "    past_price_3 = price_data.loc[:,train_product].shift(14).T[train_cols]\n",
    "    \n",
    "    \n",
    "    is_sell = is_sell_data[train_cols].loc[train_product,:]\n",
    "    past_is_sell_1 = is_sell_data.T.shift(3).T.loc[train_product, train_cols]\n",
    "    past_is_sell_2 = is_sell_data.T.shift(7).T.loc[train_product, train_cols]\n",
    "    past_is_sell_3 = is_sell_data.T.shift(14).T.loc[train_product, train_cols]\n",
    "\n",
    "    data = torch.FloatTensor(data.values.astype(float))\n",
    "    \n",
    "    calendar = torch.FloatTensor(calendar.values.astype(float))\n",
    "    \n",
    "    price = torch.FloatTensor(price.values.astype(float))\n",
    "    \n",
    "    past_price_1 = torch.FloatTensor(past_price_1.values.astype(float))\n",
    "    past_price_2 = torch.FloatTensor(past_price_2.values.astype(float))\n",
    "    past_price_3 = torch.FloatTensor(past_price_3.values.astype(float))\n",
    "    \n",
    "    is_sell = torch.FloatTensor(is_sell.values.astype(float))\n",
    "    past_is_sell_1 = torch.FloatTensor(past_is_sell_1.values.astype(float))\n",
    "    past_is_sell_2 = torch.FloatTensor(past_is_sell_2.values.astype(float))\n",
    "    past_is_sell_3 = torch.FloatTensor(past_is_sell_3.values.astype(float))\n",
    "    \n",
    "    data_list = []\n",
    "    for idx in range(len(data)):\n",
    "        _data = data[[idx],:]\n",
    "        _price = price[[idx],:]\n",
    "        \n",
    "        _past_price_1 = past_price_1[[idx],:]\n",
    "        _past_price_2 = past_price_2[[idx],:]\n",
    "        _past_price_3 = past_price_3[[idx],:]\n",
    "        \n",
    "        _is_sell = is_sell[[idx],:]\n",
    "        \n",
    "        _past_is_sell_1 = past_is_sell_1[[idx],:]\n",
    "        _past_is_sell_2 = past_is_sell_2[[idx],:]\n",
    "        _past_is_sell_3 = past_is_sell_3[[idx],:]\n",
    "        \n",
    "        x = torch.cat((\n",
    "            _data, calendar,\n",
    "            _price,\n",
    "            _past_price_1, _past_price_2, _past_price_3,\n",
    "            _is_sell,\n",
    "            _past_is_sell_1, _past_is_sell_2, _past_is_sell_3\n",
    "        ), dim=0)\n",
    "        data_list.append(x.tolist())\n",
    "    data_list = torch.FloatTensor(data_list)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish(input):\n",
    "    return input * torch.tanh(nn.functional.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return mish(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class residual_conv1d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel):\n",
    "        super(residual_conv1d, self).__init__()\n",
    "        \n",
    "        self.mish = Mish()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv1d(in_channel, in_channel, 1),\n",
    "            Mish(),\n",
    "            nn.Conv1d(in_channel, in_channel, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x+self.layer(x)\n",
    "        x = self.mish(x)\n",
    "        return x\n",
    "\n",
    "class Conv_1d_Net(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel):\n",
    "        super(Conv_1d_Net, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channel, 2*in_channel, 1),\n",
    "            nn.Dropout(0.2),\n",
    "            Mish(),\n",
    "            residual_conv1d(2*in_channel)\n",
    "        )\n",
    "        \n",
    "        self.layer_2 = nn.Sequential(\n",
    "            nn.Conv1d(2*in_channel, 4*in_channel, 1),\n",
    "            nn.Dropout(0.2),\n",
    "            Mish(),\n",
    "            residual_conv1d(4*in_channel)\n",
    "        )\n",
    "        \n",
    "        self.layer_3 = nn.Sequential(\n",
    "            nn.Conv1d(4*in_channel, 8*in_channel, 1),\n",
    "            nn.Dropout(0.2),\n",
    "            Mish(),\n",
    "            residual_conv1d(8*in_channel)\n",
    "        )\n",
    "       \n",
    "         \n",
    "        self.avgpool1d = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(8*in_channel, 8*in_channel),\n",
    "            nn.Dropout(0.1),\n",
    "            Mish(),\n",
    "            nn.Linear(8*in_channel, 16*in_channel),\n",
    "            nn.Dropout(0.1),\n",
    "            Mish(),\n",
    "            nn.Linear(16*in_channel, 28)\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        #_in = x.size()[1]\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        #x = self.layer_4(x)\n",
    "        x = self.avgpool1d(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:            \n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydatasets(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, calendar):\n",
    "        self.data = data\n",
    "        self.calendar = calendar\n",
    "        self.datanum = len(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _data = self.data[idx, :, :]\n",
    "        x = torch.cat((_data, self.calendar), dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_df = pd.read_csv(path+'sales_train_validation.csv')\n",
    "calendar_df = pd.read_csv(path+'calendar.csv')\n",
    "sell_prices_df = pd.read_csv(path+'sell_prices.csv')\n",
    "sample_submission_df = pd.read_csv(path+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_ids = (\n",
    "    'all_id',\n",
    "    'state_id',\n",
    "    'store_id',\n",
    "    'cat_id',\n",
    "    'dept_id',\n",
    "    ['state_id', 'cat_id'],\n",
    "    ['state_id', 'dept_id'],\n",
    "    ['store_id', 'cat_id'],\n",
    "    ['store_id', 'dept_id'],\n",
    "    'item_id',\n",
    "    ['item_id', 'state_id'],\n",
    "    ['item_id', 'store_id']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd_1',\n",
       "       'd_2', 'd_3', 'd_4',\n",
       "       ...\n",
       "       'd_1904', 'd_1905', 'd_1906', 'd_1907', 'd_1908', 'd_1909', 'd_1910',\n",
       "       'd_1911', 'd_1912', 'd_1913'],\n",
       "      dtype='object', length=1919)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806ecd4f09e5494880fc97c4cd1b6a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=12), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "d_cols = [f'd_{i}' for i in range(1,1914)]\n",
    "train_target_columns = d_cols[:1500]\n",
    "valid_target_columns = d_cols[1500:]\n",
    "\n",
    "original_train_df['all_id'] = 0\n",
    "\n",
    "train_df = original_train_df[['all_id', 'id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']+train_target_columns]\n",
    "valid_df = original_train_df[['all_id', 'id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']+valid_target_columns]\n",
    "train_target_lv = {}\n",
    "valid_target_lv = {}\n",
    "for i, group_id in enumerate(tqdm(group_ids)):\n",
    "            train_target_lv[f'lv{i + 1}_train_df'] = train_df.groupby(group_id)[train_target_columns].sum()\n",
    "            valid_target_lv[f'lv{i + 1}_valid_df'] = valid_df.groupby(group_id)[valid_target_columns].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lv1_train_df', 'lv2_train_df', 'lv3_train_df', 'lv4_train_df', 'lv5_train_df', 'lv6_train_df', 'lv7_train_df', 'lv8_train_df', 'lv9_train_df', 'lv10_train_df', 'lv11_train_df', 'lv12_train_df'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_lv.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>d_1</th>\n",
       "      <th>d_2</th>\n",
       "      <th>d_3</th>\n",
       "      <th>d_4</th>\n",
       "      <th>d_5</th>\n",
       "      <th>d_6</th>\n",
       "      <th>d_7</th>\n",
       "      <th>d_8</th>\n",
       "      <th>d_9</th>\n",
       "      <th>d_10</th>\n",
       "      <th>...</th>\n",
       "      <th>d_1491</th>\n",
       "      <th>d_1492</th>\n",
       "      <th>d_1493</th>\n",
       "      <th>d_1494</th>\n",
       "      <th>d_1495</th>\n",
       "      <th>d_1496</th>\n",
       "      <th>d_1497</th>\n",
       "      <th>d_1498</th>\n",
       "      <th>d_1499</th>\n",
       "      <th>d_1500</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">CA</th>\n",
       "      <th>FOODS</th>\n",
       "      <td>10101</td>\n",
       "      <td>9862</td>\n",
       "      <td>6944</td>\n",
       "      <td>7864</td>\n",
       "      <td>7178</td>\n",
       "      <td>8256</td>\n",
       "      <td>9005</td>\n",
       "      <td>11870</td>\n",
       "      <td>10977</td>\n",
       "      <td>8637</td>\n",
       "      <td>...</td>\n",
       "      <td>8240</td>\n",
       "      <td>10825</td>\n",
       "      <td>12258</td>\n",
       "      <td>9229</td>\n",
       "      <td>8943</td>\n",
       "      <td>8380</td>\n",
       "      <td>8997</td>\n",
       "      <td>9570</td>\n",
       "      <td>11735</td>\n",
       "      <td>13136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOBBIES</th>\n",
       "      <td>1802</td>\n",
       "      <td>1561</td>\n",
       "      <td>1472</td>\n",
       "      <td>1405</td>\n",
       "      <td>1181</td>\n",
       "      <td>1459</td>\n",
       "      <td>1314</td>\n",
       "      <td>1986</td>\n",
       "      <td>1482</td>\n",
       "      <td>1508</td>\n",
       "      <td>...</td>\n",
       "      <td>1890</td>\n",
       "      <td>2256</td>\n",
       "      <td>2198</td>\n",
       "      <td>1643</td>\n",
       "      <td>1708</td>\n",
       "      <td>1576</td>\n",
       "      <td>1769</td>\n",
       "      <td>1721</td>\n",
       "      <td>2184</td>\n",
       "      <td>2241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUSEHOLD</th>\n",
       "      <td>2292</td>\n",
       "      <td>2382</td>\n",
       "      <td>1692</td>\n",
       "      <td>1778</td>\n",
       "      <td>1566</td>\n",
       "      <td>1607</td>\n",
       "      <td>1932</td>\n",
       "      <td>2754</td>\n",
       "      <td>2237</td>\n",
       "      <td>1677</td>\n",
       "      <td>...</td>\n",
       "      <td>3664</td>\n",
       "      <td>5127</td>\n",
       "      <td>5040</td>\n",
       "      <td>3498</td>\n",
       "      <td>3529</td>\n",
       "      <td>3046</td>\n",
       "      <td>3040</td>\n",
       "      <td>3328</td>\n",
       "      <td>4521</td>\n",
       "      <td>5763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">TX</th>\n",
       "      <th>FOODS</th>\n",
       "      <td>6853</td>\n",
       "      <td>7030</td>\n",
       "      <td>5124</td>\n",
       "      <td>5470</td>\n",
       "      <td>4602</td>\n",
       "      <td>7067</td>\n",
       "      <td>4671</td>\n",
       "      <td>7055</td>\n",
       "      <td>6920</td>\n",
       "      <td>5505</td>\n",
       "      <td>...</td>\n",
       "      <td>5153</td>\n",
       "      <td>6790</td>\n",
       "      <td>8304</td>\n",
       "      <td>6446</td>\n",
       "      <td>6542</td>\n",
       "      <td>5577</td>\n",
       "      <td>5706</td>\n",
       "      <td>6374</td>\n",
       "      <td>7816</td>\n",
       "      <td>7664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOBBIES</th>\n",
       "      <td>879</td>\n",
       "      <td>870</td>\n",
       "      <td>526</td>\n",
       "      <td>809</td>\n",
       "      <td>501</td>\n",
       "      <td>831</td>\n",
       "      <td>390</td>\n",
       "      <td>785</td>\n",
       "      <td>794</td>\n",
       "      <td>524</td>\n",
       "      <td>...</td>\n",
       "      <td>1063</td>\n",
       "      <td>1348</td>\n",
       "      <td>1359</td>\n",
       "      <td>947</td>\n",
       "      <td>983</td>\n",
       "      <td>945</td>\n",
       "      <td>912</td>\n",
       "      <td>815</td>\n",
       "      <td>1135</td>\n",
       "      <td>1158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUSEHOLD</th>\n",
       "      <td>1706</td>\n",
       "      <td>1730</td>\n",
       "      <td>1128</td>\n",
       "      <td>1102</td>\n",
       "      <td>809</td>\n",
       "      <td>1108</td>\n",
       "      <td>1165</td>\n",
       "      <td>1600</td>\n",
       "      <td>1662</td>\n",
       "      <td>1290</td>\n",
       "      <td>...</td>\n",
       "      <td>2746</td>\n",
       "      <td>3657</td>\n",
       "      <td>3753</td>\n",
       "      <td>2768</td>\n",
       "      <td>2880</td>\n",
       "      <td>2711</td>\n",
       "      <td>2334</td>\n",
       "      <td>2553</td>\n",
       "      <td>3579</td>\n",
       "      <td>3258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">WI</th>\n",
       "      <th>FOODS</th>\n",
       "      <td>6224</td>\n",
       "      <td>5866</td>\n",
       "      <td>5106</td>\n",
       "      <td>5544</td>\n",
       "      <td>2823</td>\n",
       "      <td>6770</td>\n",
       "      <td>6814</td>\n",
       "      <td>8826</td>\n",
       "      <td>6965</td>\n",
       "      <td>4759</td>\n",
       "      <td>...</td>\n",
       "      <td>6456</td>\n",
       "      <td>8759</td>\n",
       "      <td>8418</td>\n",
       "      <td>7856</td>\n",
       "      <td>7576</td>\n",
       "      <td>6886</td>\n",
       "      <td>7460</td>\n",
       "      <td>8388</td>\n",
       "      <td>9139</td>\n",
       "      <td>10139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOBBIES</th>\n",
       "      <td>1083</td>\n",
       "      <td>926</td>\n",
       "      <td>684</td>\n",
       "      <td>455</td>\n",
       "      <td>132</td>\n",
       "      <td>930</td>\n",
       "      <td>1240</td>\n",
       "      <td>1215</td>\n",
       "      <td>623</td>\n",
       "      <td>583</td>\n",
       "      <td>...</td>\n",
       "      <td>1089</td>\n",
       "      <td>1310</td>\n",
       "      <td>1049</td>\n",
       "      <td>900</td>\n",
       "      <td>576</td>\n",
       "      <td>780</td>\n",
       "      <td>790</td>\n",
       "      <td>903</td>\n",
       "      <td>1280</td>\n",
       "      <td>1053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOUSEHOLD</th>\n",
       "      <td>1691</td>\n",
       "      <td>1522</td>\n",
       "      <td>1107</td>\n",
       "      <td>985</td>\n",
       "      <td>354</td>\n",
       "      <td>1183</td>\n",
       "      <td>1479</td>\n",
       "      <td>1841</td>\n",
       "      <td>1076</td>\n",
       "      <td>1089</td>\n",
       "      <td>...</td>\n",
       "      <td>3265</td>\n",
       "      <td>4082</td>\n",
       "      <td>2807</td>\n",
       "      <td>2111</td>\n",
       "      <td>1650</td>\n",
       "      <td>1873</td>\n",
       "      <td>1811</td>\n",
       "      <td>2285</td>\n",
       "      <td>2840</td>\n",
       "      <td>2967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9 rows × 1500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      d_1   d_2   d_3   d_4   d_5   d_6   d_7    d_8    d_9  \\\n",
       "state_id cat_id                                                               \n",
       "CA       FOODS      10101  9862  6944  7864  7178  8256  9005  11870  10977   \n",
       "         HOBBIES     1802  1561  1472  1405  1181  1459  1314   1986   1482   \n",
       "         HOUSEHOLD   2292  2382  1692  1778  1566  1607  1932   2754   2237   \n",
       "TX       FOODS       6853  7030  5124  5470  4602  7067  4671   7055   6920   \n",
       "         HOBBIES      879   870   526   809   501   831   390    785    794   \n",
       "         HOUSEHOLD   1706  1730  1128  1102   809  1108  1165   1600   1662   \n",
       "WI       FOODS       6224  5866  5106  5544  2823  6770  6814   8826   6965   \n",
       "         HOBBIES     1083   926   684   455   132   930  1240   1215    623   \n",
       "         HOUSEHOLD   1691  1522  1107   985   354  1183  1479   1841   1076   \n",
       "\n",
       "                    d_10   ...    d_1491  d_1492  d_1493  d_1494  d_1495  \\\n",
       "state_id cat_id            ...                                             \n",
       "CA       FOODS      8637   ...      8240   10825   12258    9229    8943   \n",
       "         HOBBIES    1508   ...      1890    2256    2198    1643    1708   \n",
       "         HOUSEHOLD  1677   ...      3664    5127    5040    3498    3529   \n",
       "TX       FOODS      5505   ...      5153    6790    8304    6446    6542   \n",
       "         HOBBIES     524   ...      1063    1348    1359     947     983   \n",
       "         HOUSEHOLD  1290   ...      2746    3657    3753    2768    2880   \n",
       "WI       FOODS      4759   ...      6456    8759    8418    7856    7576   \n",
       "         HOBBIES     583   ...      1089    1310    1049     900     576   \n",
       "         HOUSEHOLD  1089   ...      3265    4082    2807    2111    1650   \n",
       "\n",
       "                    d_1496  d_1497  d_1498  d_1499  d_1500  \n",
       "state_id cat_id                                             \n",
       "CA       FOODS        8380    8997    9570   11735   13136  \n",
       "         HOBBIES      1576    1769    1721    2184    2241  \n",
       "         HOUSEHOLD    3046    3040    3328    4521    5763  \n",
       "TX       FOODS        5577    5706    6374    7816    7664  \n",
       "         HOBBIES       945     912     815    1135    1158  \n",
       "         HOUSEHOLD    2711    2334    2553    3579    3258  \n",
       "WI       FOODS        6886    7460    8388    9139   10139  \n",
       "         HOBBIES       780     790     903    1280    1053  \n",
       "         HOUSEHOLD    1873    1811    2285    2840    2967  \n",
       "\n",
       "[9 rows x 1500 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target_lv['lv6_train_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WRMSSEEvaluator(object):\n",
    "\n",
    "    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n",
    "        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n",
    "        train_target_columns = train_y.columns.tolist()\n",
    "        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n",
    "\n",
    "        train_df['all_id'] = 0  # for lv1 aggregation\n",
    "\n",
    "        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n",
    "        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n",
    "\n",
    "        if not all([c in valid_df.columns for c in id_columns]):\n",
    "            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n",
    "\n",
    "        self.train_df = train_df\n",
    "        self.valid_df = valid_df\n",
    "        self.calendar = calendar\n",
    "        self.prices = prices\n",
    "\n",
    "        self.weight_columns = weight_columns\n",
    "        self.id_columns = id_columns\n",
    "        self.valid_target_columns = valid_target_columns\n",
    "\n",
    "        weight_df = self.get_weight_df()\n",
    "\n",
    "        self.group_ids = (\n",
    "            'all_id',\n",
    "            'state_id',\n",
    "            'store_id',\n",
    "            'cat_id',\n",
    "            'dept_id',\n",
    "            ['state_id', 'cat_id'],\n",
    "            ['state_id', 'dept_id'],\n",
    "            ['store_id', 'cat_id'],\n",
    "            ['store_id', 'dept_id'],\n",
    "            'item_id',\n",
    "            ['item_id', 'state_id'],\n",
    "            ['item_id', 'store_id']\n",
    "        )\n",
    "\n",
    "        for i, group_id in enumerate(tqdm(self.group_ids)):\n",
    "            setattr(self, f'lv{i + 1}_train_df', train_df.groupby(group_id)[train_target_columns].sum())\n",
    "            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n",
    "\n",
    "            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n",
    "            setattr(self, f'lv{i + 1}_weight', lv_weight / lv_weight.sum())\n",
    "\n",
    "    def get_weight_df(self) -> pd.DataFrame:\n",
    "        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n",
    "        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n",
    "        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n",
    "        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n",
    "\n",
    "        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n",
    "        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n",
    "        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n",
    "        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n",
    "        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n",
    "        return weight_df\n",
    "\n",
    "    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n",
    "        train_y = getattr(self, f'lv{lv}_train_df')\n",
    "        valid_y = getattr(self, f'lv{lv}_valid_df')\n",
    "        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n",
    "        scale = ((train_y.iloc[:, 1:].values - train_y.iloc[:, :-1].values) ** 2).mean(axis=1)\n",
    "        return (score / scale).map(np.sqrt)\n",
    "\n",
    "    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n",
    "        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n",
    "\n",
    "        if isinstance(valid_preds, np.ndarray):\n",
    "            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n",
    "\n",
    "        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n",
    "\n",
    "        all_scores = []\n",
    "        for i, group_id in enumerate(self.group_ids):\n",
    "            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n",
    "            weight = getattr(self, f'lv{i + 1}_weight')\n",
    "            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n",
    "            all_scores.append(lv_scores.sum())\n",
    "\n",
    "        return np.mean(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rmsse(self, valid_preds) -> pd.Series:\n",
    "        '''\n",
    "        returns rmsse scores for all 42840 series\n",
    "        '''\n",
    "        score = ((self.valid_series - valid_preds) ** 2).mean(axis=1)\n",
    "        self.scale = np.where(self.scale != 0 , self.scale, 1)\n",
    "        rmsse = (score / self.scale).map(np.sqrt)\n",
    "        return rmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0c2de54fd24a01b5c6e64a0fc5c218",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30490), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 51.4 s, sys: 3.89 s, total: 55.2 s\n",
      "Wall time: 55.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_df, calendar_df, calendar_data, price_data, is_sell = Preprocessing(original_train_df, calendar_df, sell_prices_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'snap_CA', 'snap_TX', 'snap_WI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 691 µs, sys: 47 µs, total: 738 µs\n",
      "Wall time: 784 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d_cols = [f'd_{i}' for i in range(1,1914)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_CvData(cols, train_df, calendar_data, price_data, is_sell, sample_submission_df):\n",
    "    trn_cols = cols[:-28]\n",
    "    val_cols = cols\n",
    "    \n",
    "    state='CA'\n",
    "    data_ca = make_data(trn_cols, state, train_df, calendar_data, price_data, is_sell, sample_submission_df)\n",
    "    state='TX'\n",
    "    data_tx = make_data(trn_cols, state, train_df, calendar_data, price_data, is_sell, sample_submission_df)\n",
    "    state='WI'\n",
    "    data_wi = make_data(trn_cols, state, train_df, calendar_data, price_data, is_sell, sample_submission_df)\n",
    "    trn_data = torch.cat(\n",
    "        (data_ca, data_tx, data_wi),\n",
    "        dim=0\n",
    "    )\n",
    "    trn_calendar = make_calendar_data(calendar_data, train_cols)\n",
    "    del data_ca, data_tx, data_wi\n",
    "    gc.collect()\n",
    "\n",
    "    state='CA'\n",
    "    data_ca = make_data(val_cols, state, train_df, calendar_data, price_data, is_sell, sample_submission_df)\n",
    "    state='TX'\n",
    "    data_tx = make_data(val_cols, state, train_df, calendar_data, price_data, is_sell, sample_submission_df)\n",
    "    state='WI'\n",
    "    data_wi = make_data(val_cols, state, train_df, calendar_data, price_data, is_sell, sample_submission_df)\n",
    "    val_data = torch.cat(\n",
    "        (data_ca, data_tx, data_wi),\n",
    "        dim=0\n",
    "    )\n",
    "    val_calendar = make_calendar_data(calendar_data, val_cols)\n",
    "    del data_ca, data_tx, data_wi\n",
    "    gc.collect()\n",
    "    \n",
    "    print(val_data.size()[1]+ val_calendar.size()[0])\n",
    "    trn_data_set=Mydatasets(trn_data, trn_calendar, train = True)\n",
    "    trn_loader = torch.utils.data.DataLoader(trn_data_set, batch_size = 200, shuffle = True)\n",
    "\n",
    "    val_data_set=Mydatasets(val_data, val_calendar, train = True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data_set, batch_size = 50, shuffle = False)\n",
    "    \n",
    "    return trn_loader,  val_loader\n",
    "\n",
    "\n",
    "\n",
    "def Nest_cv(d_cols=[f'd_{i}' for i in range(900,1914)], cv=5):\n",
    "    cv_lndex=[]\n",
    "    N = int((len(d_cols)/cv)+1)\n",
    "    for i in range(cv):\n",
    "        cols = d_cols[N*i:]\n",
    "        n=int(len(cols)*0.75)\n",
    "        holdout = {}\n",
    "        holdout['trn'] = cols[:n]\n",
    "        holdout['val'] = cols[n:]\n",
    "        \n",
    "        inner_cols = cols[:n]\n",
    "        n=int(len(inner_cols)*0.75)\n",
    "        inner={}\n",
    "        inner['trn'] = inner_cols[:n]\n",
    "        inner['val'] = inner_cols[n:]\n",
    "        \n",
    "        Index ={}\n",
    "        Index['holdout'] = holdout\n",
    "        Index['inner'] = inner\n",
    "        cv_lndex.append(Index)\n",
    "    return cv_lndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=200\n",
    "i=0\n",
    "if i==0:\n",
    "    cols = d_cols[-n*(i+1):]\n",
    "else:\n",
    "    cols = d_cols[-n*(i+1):-n*i]\n",
    "\n",
    "state='CA'\n",
    "data_ca = make_data(cols, state, train_df, calendar_data, price_data, is_sell, sample_submission_df)\n",
    "state='TX'\n",
    "data_tx = make_data(cols, state, train_df, calendar_data, price_data, is_sell, sample_submission_df)\n",
    "state='WI'\n",
    "data_wi = make_data(cols, state, train_df, calendar_data, price_data, is_sell, sample_submission_df)\n",
    "\n",
    "\n",
    "data = torch.cat(\n",
    "    (data_ca, data_tx, data_wi),\n",
    "    dim=0\n",
    ")\n",
    "calendar = make_calendar_data(calendar_data, cols)\n",
    "del data_ca, data_tx, data_wi\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set=Mydatasets(data, calendar)\n",
    "#data_loader = torch.utils.data.DataLoader(data_set, batch_size = 200, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([200, 56, 200])\n",
      "torch.Size([200, 56, 200])\n",
      "torch.Size([200, 56, 200])\n",
      "torch.Size([200, 56, 200])\n",
      "torch.Size([200, 56, 200])\n"
     ]
    }
   ],
   "source": [
    "c=0\n",
    "for x in data_loader:\n",
    "    if c<5:\n",
    "        print(x.size())\n",
    "    c+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_set):\n",
    "    num_epochs = 40\n",
    "    lr = 1e-4\n",
    "    eta_min = 1e-3\n",
    "    t_max = 10\n",
    "    numclass = 5\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = RAdam(params=model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max, eta_min=eta_min)\n",
    "\n",
    "\n",
    "    best_epoch = -1\n",
    "    best_score = 10000\n",
    "    early_stoppping_cnt = 0\n",
    "    best_model = model\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        data_loader = torch.utils.data.DataLoader(data_set, batch_size = 200, shuffle = True)\n",
    "        for x_batch in tqdm(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            \n",
    "            y_batch = x_batch[:,0,-28*2:-28]\n",
    "            x_batch = x_batch[:,:,:-28*2]\n",
    "            \n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            preds = model(x_batch)\n",
    "            loss = torch.sqrt(criterion(preds.squeeze(1), y_batch))\n",
    "            \n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            avg_loss += loss.item() / len(data_loader)\n",
    "        \n",
    "        model.eval()\n",
    "        avg_val_loss = 0.\n",
    "        \n",
    "        data_loader = torch.utils.data.DataLoader(data_set, batch_size = 100, shuffle = False)\n",
    "        for x_batch in data_loader:\n",
    "            \n",
    "            y_batch = x_batch[:,0,-28:]\n",
    "            x_batch = x_batch[:,:,:-28]\n",
    "            \n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            preds = model(x_batch)\n",
    "            loss = torch.sqrt(criterion(preds.squeeze(1), y_batch))\n",
    "\n",
    "            avg_val_loss += loss.item() / len(data_loader)\n",
    "            \n",
    "            \n",
    "        if best_score>avg_val_loss:\n",
    "            best_score = avg_val_loss\n",
    "            early_stoppping_cnt=0\n",
    "            best_epoch=epoch\n",
    "            best_model = model\n",
    "            elapsed = time.time() - start_time\n",
    "            p_avg_val_loss = termcolor.colored(np.round(avg_val_loss, 4),\"red\")\n",
    "            \n",
    "            print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {p_avg_val_loss} time: {elapsed:.0f}s')\n",
    "        else:\n",
    "            early_stoppping_cnt+=1\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f} time: {elapsed:.0f}s')\n",
    "        \n",
    "        if (epoch>10) and (early_stoppping_cnt>7):\n",
    "                break\n",
    "    \n",
    "    print(f'best_score : {best_score}    best_epoch : {best_epoch}')\n",
    "    torch.save(model.state_dict(), 'net.pt')\n",
    "    \n",
    "    return best_model, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size=56\n",
    "model = Conv_1d_Net(in_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "558"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#best_model, best_score = train_model(model, data_set)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
