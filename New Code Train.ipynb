{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os, gc\n",
    "import termcolor\n",
    "\n",
    "import math, random\n",
    "import pickle\n",
    "import datetime, time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "RANDOM_SEED = 2020\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish(input):\n",
    "    return input * torch.tanh(nn.functional.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return mish(input)\n",
    "    \n",
    "class residual_conv1d(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel):\n",
    "        super(residual_conv1d, self).__init__()\n",
    "        \n",
    "        self.mish = Mish()\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Conv1d(in_channel, in_channel, 1),\n",
    "            Mish(),\n",
    "            nn.Conv1d(in_channel, in_channel, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x+self.layer(x)\n",
    "        x = self.mish(x)\n",
    "        return x\n",
    "\n",
    "class Conv_1d_Net(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channel):\n",
    "        super(Conv_1d_Net, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channel, 2*in_channel, 1),\n",
    "            nn.Dropout(0.2),\n",
    "            Mish(),\n",
    "            residual_conv1d(2*in_channel)\n",
    "        )\n",
    "        \n",
    "        self.layer_2 = nn.Sequential(\n",
    "            nn.Conv1d(2*in_channel, 4*in_channel, 1),\n",
    "            nn.Dropout(0.2),\n",
    "            Mish(),\n",
    "            residual_conv1d(4*in_channel)\n",
    "        )\n",
    "        \n",
    "        self.layer_3 = nn.Sequential(\n",
    "            nn.Conv1d(4*in_channel, 8*in_channel, 1),\n",
    "            nn.Dropout(0.2),\n",
    "            Mish(),\n",
    "            residual_conv1d(8*in_channel)\n",
    "        )\n",
    "       \n",
    "         \n",
    "        self.avgpool1d = nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(8*in_channel, 8*in_channel),\n",
    "            nn.Dropout(0.1),\n",
    "            Mish(),\n",
    "            nn.Linear(8*in_channel, 16*in_channel),\n",
    "            nn.Dropout(0.1),\n",
    "            Mish(),\n",
    "            nn.Linear(16*in_channel, 28)\n",
    "        ) \n",
    "\n",
    "    def forward(self, x):\n",
    "        #_in = x.size()[1]\n",
    "        x = self.layer_1(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = self.layer_3(x)\n",
    "        #x = self.layer_4(x)\n",
    "        x = self.avgpool1d(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "from torch.optim.optimizer import Optimizer\n",
    "import math\n",
    "\n",
    "class RAdam(Optimizer):\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
    "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        super(RAdam, self).__init__(params, defaults)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(RAdam, self).__setstate__(state)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('RAdam does not support sparse gradients')\n",
    "\n",
    "                p_data_fp32 = p.data.float()\n",
    "\n",
    "                state = self.state[p]\n",
    "\n",
    "                if len(state) == 0:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "\n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n",
    "                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n",
    "\n",
    "                state['step'] += 1\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "\n",
    "                    # more conservative since it's an approximated value\n",
    "                    if N_sma >= 5:\n",
    "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "\n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n",
    "\n",
    "                # more conservative since it's an approximated value\n",
    "                if N_sma >= 5:            \n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(-step_size, exp_avg, denom)\n",
    "                else:\n",
    "                    p_data_fp32.add_(-step_size, exp_avg)\n",
    "\n",
    "                p.data.copy_(p_data_fp32)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_X_y(x_batch):\n",
    "    y_batch = x_batch[:,0,-28:]\n",
    "    x_batch = x_batch[:,:,:-28]\n",
    "    return x_batch, y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(train_df, calendar_df, sell_prices_df):\n",
    "    sell_prices_df['id'] = sell_prices_df['item_id'].astype('str')+'_'+sell_prices_df['store_id']+'_validation'\n",
    "    d_cols = [f'd_{i}' for i in range(1,1914)]\n",
    "    \n",
    "    event_type_1 = pd.get_dummies(calendar_df.event_type_1)\n",
    "    event_type_1.columns = [f'{col}_event_type_1' for col in event_type_1.columns]\n",
    "    event_type_2 = pd.get_dummies(calendar_df.event_type_1)\n",
    "    event_type_2.columns = [f'{col}_event_type_2' for col in event_type_2.columns]\n",
    "    calendar_data = pd.concat([\n",
    "        calendar_df.drop(columns=['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2'])[['wday', 'd','month','snap_CA', 'snap_TX', 'snap_WI']],\n",
    "        event_type_1,\n",
    "        event_type_2\n",
    "    ], axis=1)\n",
    "    calendar_data = calendar_data.set_index('d').T\n",
    "    \n",
    "    \n",
    "    \n",
    "    sell_prices_data = sell_prices_df[sell_prices_df.wm_yr_wk.isin(calendar_df.wm_yr_wk.unique())]\n",
    "    sell_prices_data.reset_index(drop=True, inplace=True)\n",
    "    tmp = sell_prices_data.groupby(['id'])[['wm_yr_wk', 'sell_price']].apply(lambda x: x.set_index('wm_yr_wk')['sell_price'].to_dict()).to_dict()\n",
    "    d = calendar_df.d\n",
    "    wm_yr_wk = calendar_df.wm_yr_wk\n",
    "    price_data = {}\n",
    "    for col in tqdm(train_df.id.unique()):\n",
    "        price_data[col] = wm_yr_wk.map(tmp[col])\n",
    "    price_data = pd.DataFrame(price_data)\n",
    "    price_data.index = d\n",
    "    \n",
    "    \n",
    "    is_sell = price_data.notnull().astype(float).T\n",
    "    price_data = price_data.fillna(0)\n",
    "    \n",
    "    train_df = train_df.T\n",
    "    train_df.columns = train_df.loc['id', :].values\n",
    "    train_df = train_df.T\n",
    "    \n",
    "    return train_df, calendar_df, calendar_data, price_data, is_sell\n",
    "\n",
    "\n",
    "def make_calendar_data(calendar_data, train_cols):\n",
    "    calendar_index = [\n",
    "        'wday', 'month',\n",
    "        'Cultural_event_type_1', 'National_event_type_1', 'Religious_event_type_1', 'Sporting_event_type_1',\n",
    "        'Cultural_event_type_2', 'National_event_type_2', 'Religious_event_type_2', 'Sporting_event_type_2'\n",
    "    ]\n",
    "    calendar = calendar_data.loc[calendar_index,:]\n",
    "    event_index = [\n",
    "        'Cultural_event_type_1', 'National_event_type_1', 'Religious_event_type_1', 'Sporting_event_type_1',\n",
    "        'Cultural_event_type_2', 'National_event_type_2', 'Religious_event_type_2', 'Sporting_event_type_2'\n",
    "    ]\n",
    "    for shift in [28, -28]:\n",
    "        tmp_calendar = calendar.loc[event_index, :]\n",
    "        tmp_calendar = tmp_calendar.T.shift(-shift).T\n",
    "        tmp_calendar.index = [f'{col}_shift{shift}' for col in tmp_calendar.index]\n",
    "        calendar = pd.concat([\n",
    "            calendar,\n",
    "            tmp_calendar\n",
    "        ], axis=0)\n",
    "    calendar = calendar[train_cols]\n",
    "    calendar = torch.FloatTensor(calendar.values.astype(float))\n",
    "    return calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class item_id_store_id_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, calendar):\n",
    "        self.data = data\n",
    "        self.calendar = calendar\n",
    "        self.datanum = len(data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.datanum\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _data = self.data[idx, :, :]\n",
    "        x = torch.cat((_data, self.calendar), dim=0)\n",
    "        return x, idx\n",
    "\n",
    "class indicate_index(torch.utils.data.Dataset):\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return idx\n",
    "    \n",
    "def _create_batch_data(index, data, calendar):\n",
    "    _data = data[index, :, :]\n",
    "    x = torch.tensor([])\n",
    "    for tmp_x in _data:\n",
    "        tmp_x = torch.cat((tmp_x, calendar),dim=0)\n",
    "        x = torch.cat((x,tmp_x.unsqueeze(0)), dim=0)\n",
    "    return x\n",
    "\n",
    "class Loss_func_item_id_store_id_(nn.Module):\n",
    "    def __init__(self, df, cols):\n",
    "        super(Loss_func_item_id_store_id_, self).__init__()\n",
    "        last_d = int(cols[-1].replace('d_', ''))\n",
    "        d_cols = df.columns[df.columns.str.startswith('d_')]\n",
    "        train_d_cols = last_d-28*2\n",
    "        self.train_d_cols = d_cols[:train_d_cols]\n",
    "        test_d_cols = last_d-28\n",
    "        self.test_d_cols = d_cols[:test_d_cols]\n",
    "        self._create_denominator(df)\n",
    "        \n",
    "    def _create_denominator(self, df):\n",
    "        \n",
    "        train_value = df[self.train_d_cols]\n",
    "        train_value = train_value.values\n",
    "        train_value = train_value[:,1:]-train_value[:,:-1]\n",
    "        train_value = train_value**2\n",
    "        train_value = train_value.mean(1)\n",
    "        train_value[train_value==0]=1\n",
    "        self.train_value = torch.FloatTensor(train_value)\n",
    "        \n",
    "        test_value = df[self.test_d_cols]\n",
    "        test_value = test_value.values\n",
    "        test_value = test_value[:,1:]-test_value[:,:-1]\n",
    "        test_value = test_value**2\n",
    "        test_value = test_value.mean(1)\n",
    "        test_value[test_value==0]=1\n",
    "        self.test_value = torch.FloatTensor(test_value)\n",
    "        \n",
    "    def forward(self, preds, true, idx, train):\n",
    "        loss = (preds-true)**2\n",
    "        loss = loss.mean(1)\n",
    "        loss = loss.squeeze()\n",
    "        if train:\n",
    "            loss = loss/self.train_value[idx]\n",
    "        else:\n",
    "            loss = loss/self.test_value[idx]\n",
    "        loss = torch.sqrt(loss)\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "class Loss_func_groupid(nn.Module):\n",
    "    def __init__(self, df, cols, index_index, group_id):\n",
    "        super(Loss_func_groupid, self).__init__()\n",
    "        \n",
    "        self.index_index = index_index\n",
    "        last_d = int(cols[-1].replace('d_', ''))\n",
    "        d_cols = df.columns[df.columns.str.startswith('d_')]\n",
    "        train_d_cols = last_d-28*2\n",
    "        self.train_d_cols = d_cols[:train_d_cols]\n",
    "        test_d_cols = last_d-28\n",
    "        self.test_d_cols = d_cols[:test_d_cols]\n",
    "        self._create_denominator(df, group_id)\n",
    "        \n",
    "    def _create_denominator(self, df, group_id):\n",
    "        g_df = df.groupby(group_id)#[d_cols].sum()\n",
    "        \n",
    "        train_value = g_df[self.train_d_cols].sum()\n",
    "        train_value = train_value.loc[self.index_index,:]\n",
    "        train_value = train_value.values\n",
    "        train_value = train_value[:,1:]-train_value[:,:-1]\n",
    "        train_value = train_value**2\n",
    "        train_value = train_value.mean(1)\n",
    "        train_value[train_value==0]=1\n",
    "        self.train_value = torch.FloatTensor(train_value)\n",
    "        \n",
    "        test_value = g_df[self.test_d_cols].sum()\n",
    "        test_value = test_value.loc[self.index_index,:]\n",
    "        test_value = test_value.values\n",
    "        test_value = test_value[:,1:]-test_value[:,:-1]\n",
    "        test_value = test_value**2\n",
    "        test_value = test_value.mean(1)\n",
    "        test_value[test_value==0]=1\n",
    "        self.test_value = torch.FloatTensor(test_value)\n",
    "        \n",
    "    def forward(self, preds, true, idx, length, train):\n",
    "        a1=0\n",
    "        a2=0\n",
    "        Loss=0\n",
    "        for i, _len in enumerate(length):\n",
    "            _idx = idx[i]\n",
    "            a2=a1+_len\n",
    "            _preds = preds[a1:a2].sum(0)\n",
    "            _true = true[a1:a2].sum(0)\n",
    "            loss = (_preds -_true)**2\n",
    "            loss = loss.mean()\n",
    "            loss = loss.squeeze()\n",
    "            if train:\n",
    "                loss = loss/self.train_value[_idx]\n",
    "            else:\n",
    "                loss = loss/self.test_value[_idx]\n",
    "            loss = torch.sqrt(loss)\n",
    "            Loss+=loss/len(length)\n",
    "            a1=a2\n",
    "        return Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessing():\n",
    "    def __init__(self, path, group_id=None):\n",
    "        if group_id is None:\n",
    "            self.is_Group=False\n",
    "        else:\n",
    "            self.is_Group=True\n",
    "        self.group_id = group_id\n",
    "        self.path=path\n",
    "        self.df = pd.read_csv(self.path+'sales_train_validation.csv')\n",
    "        self.calendar_df = pd.read_csv(self.path+'calendar.csv')\n",
    "        self.sell_prices_df = pd.read_csv(self.path+'sell_prices.csv')\n",
    "        self.sample_submission_df = pd.read_csv(self.path+'sample_submission.csv')\n",
    "        \n",
    "        self.d_cols = self.df.columns[self.df.columns.str.startswith('d_')].values.tolist()\n",
    "        \n",
    "        self.train_df, self.calendar_df, self.calendar_data, self.price_data, self.is_sell = preprocessing(\n",
    "            self.df, self.calendar_df, self.sell_prices_df\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def make_data_loader(self, cols):\n",
    "        self.cols = cols\n",
    "        if self.is_Group:\n",
    "            self.group_sum_df = self.train_df.groupby(self.group_id)[cols].transform('sum')\n",
    "        \n",
    "        state='CA'\n",
    "        data_ca = self.make_data_g(cols, state)\n",
    "        state='TX'\n",
    "        data_tx = self.make_data_g(cols, state)\n",
    "        state='WI'\n",
    "        data_wi = self.make_data_g(cols, state)\n",
    "\n",
    "        data = torch.cat(\n",
    "            (data_ca, data_tx, data_wi),\n",
    "            dim=0\n",
    "        )\n",
    "        calendar = make_calendar_data(self.calendar_data, cols)\n",
    "        del data_ca, data_tx, data_wi; gc.collect()\n",
    "        \n",
    "        self.in_size=data.size()[1]+calendar.size()[0]\n",
    "        self.data = data\n",
    "        self.calendar = calendar\n",
    "\n",
    "    def make_data_g(self, train_cols, state):\n",
    "        \n",
    "        data_train = self.train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']+train_cols]\n",
    "        train_product = self.sample_submission_df[\n",
    "            (self.sample_submission_df.id.str.contains(state))&(self.sample_submission_df.id.str.contains('_validation'))\n",
    "            ].id.values\n",
    "        \n",
    "        data = data_train.loc[train_product,train_cols]\n",
    "        \n",
    "        \n",
    "        calendar_index = [ f'snap_{state}']\n",
    "        event_index = [ f'snap_{state}']\n",
    "        calendar = self.calendar_data.loc[calendar_index,:]\n",
    "        for shift in [28, -28]:\n",
    "            tmp_calendar = calendar.loc[event_index, :]\n",
    "            tmp_calendar = tmp_calendar.T.shift(shift).T\n",
    "            tmp_calendar.index = [f'{col}_shift{shift}' for col in tmp_calendar.index]\n",
    "            calendar = pd.concat([\n",
    "                calendar,\n",
    "                tmp_calendar\n",
    "            ], axis=0)\n",
    "        calendar = calendar[train_cols]\n",
    "        \n",
    "        price = self.price_data.T[train_cols].loc[train_product,:]\n",
    "        past_price_1 = self.price_data.loc[:,train_product].shift(28).T[train_cols]\n",
    "        past_price_2 = self.price_data.loc[:,train_product].shift(-28).T[train_cols]\n",
    "        \n",
    "        \n",
    "        is_sell = self.is_sell[train_cols].loc[train_product,:]\n",
    "        past_is_sell_1 = self.is_sell.T.shift(28).T.loc[train_product, train_cols]\n",
    "        past_is_sell_2 = self.is_sell.T.shift(-28).T.loc[train_product, train_cols]\n",
    "\n",
    "        data = torch.FloatTensor(data.values.astype(float))\n",
    "        \n",
    "        if self.is_Group:\n",
    "            group_sum_df = self.group_sum_df.loc[train_product, :]\n",
    "            group_sum_df = torch.FloatTensor(group_sum_df.values.astype(float))\n",
    "            \n",
    "        calendar = torch.FloatTensor(calendar.values.astype(float))\n",
    "        \n",
    "        price = torch.FloatTensor(price.values.astype(float))\n",
    "        \n",
    "        past_price_1 = torch.FloatTensor(past_price_1.values.astype(float))\n",
    "        past_price_2 = torch.FloatTensor(past_price_2.values.astype(float))\n",
    "        \n",
    "        is_sell = torch.FloatTensor(is_sell.values.astype(float))\n",
    "        past_is_sell_1 = torch.FloatTensor(past_is_sell_1.values.astype(float))\n",
    "        past_is_sell_2 = torch.FloatTensor(past_is_sell_2.values.astype(float))\n",
    "        \n",
    "        data_list = []\n",
    "        for idx in range(len(data)):\n",
    "            if self.is_Group:\n",
    "                _data = data[[idx],:]\n",
    "                _group_sum_data = group_sum_df[[idx],:]\n",
    "                _price = price[[idx],:]\n",
    "\n",
    "                _past_price_1 = past_price_1[[idx],:]\n",
    "                _past_price_2 = past_price_2[[idx],:]\n",
    "\n",
    "                _is_sell = is_sell[[idx],:]\n",
    "\n",
    "                _past_is_sell_1 = past_is_sell_1[[idx],:]\n",
    "                _past_is_sell_2 = past_is_sell_2[[idx],:]\n",
    "\n",
    "                x = torch.cat((\n",
    "                    _data, _group_sum_data,\n",
    "                    calendar,\n",
    "                    _price,\n",
    "                    _past_price_1, _past_price_2,\n",
    "                    _is_sell,\n",
    "                    _past_is_sell_1, _past_is_sell_2,\n",
    "                ), dim=0)\n",
    "            else:\n",
    "                _data = data[[idx],:]\n",
    "                #_group_sum_data = group_sum_df[[idx],:]\n",
    "                _price = price[[idx],:]\n",
    "\n",
    "                _past_price_1 = past_price_1[[idx],:]\n",
    "                _past_price_2 = past_price_2[[idx],:]\n",
    "\n",
    "                _is_sell = is_sell[[idx],:]\n",
    "\n",
    "                _past_is_sell_1 = past_is_sell_1[[idx],:]\n",
    "                _past_is_sell_2 = past_is_sell_2[[idx],:]\n",
    "\n",
    "                x = torch.cat((\n",
    "                    _data,\n",
    "                    calendar,\n",
    "                    _price,\n",
    "                    _past_price_1, _past_price_2,\n",
    "                    _is_sell,\n",
    "                    _past_is_sell_1, _past_is_sell_2,\n",
    "                ), dim=0)\n",
    "            data_list.append(x.tolist())\n",
    "        data_list = torch.FloatTensor(data_list)\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train_Model():        \n",
    "    def prepare_training_MIX(self, model, Preprocesing_agent, index_index):\n",
    "        lr = 4e-4\n",
    "        eta_min = 1e-3\n",
    "        t_max = 10\n",
    "        model = model.to(DEVICE)\n",
    "        criterion_2 = Loss_func_groupid(\n",
    "            cols=Preprocesing_agent.cols, df=Preprocesing_agent.df, index_index=index_index, group_id=Preprocesing_agent.group_id\n",
    "        )\n",
    "        criterion_1 = Loss_func_item_id_store_id_(df=Preprocesing_agent.df, cols=Preprocesing_agent.cols)\n",
    "        optimizer = RAdam(params=model.parameters(), lr=lr)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max, eta_min=eta_min)\n",
    "        return model, criterion_1, criterion_2, optimizer, scheduler\n",
    "\n",
    "    def train_model_MIX(self, model, Preprocesing_agent):\n",
    "        Preprocesing_agent.df['index'] = Preprocesing_agent.df.index\n",
    "        index_df = pd.concat([\n",
    "            Preprocesing_agent.df.groupby(Preprocesing_agent.group_id)['index'].unique(),\n",
    "            Preprocesing_agent.df.groupby(Preprocesing_agent.group_id)['index'].nunique()\n",
    "        ], axis=1)\n",
    "        index_df.columns=['index', 'length']\n",
    "        index_df['index'] = index_df['index'].apply(lambda x: x.tolist())\n",
    "\n",
    "        index_index = index_df.index\n",
    "        index_df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        data_set=indicate_index(index_df)\n",
    "        \n",
    "        batch_size = int(150/index_df['length'].mean())\n",
    "        \n",
    "        data_loader = torch.utils.data.DataLoader(data_set, batch_size = batch_size, shuffle = True)\n",
    "        \n",
    "        model, criterion_, criterion_g, optimizer, scheduler = self.prepare_training_MIX(\n",
    "            model, Preprocesing_agent, index_index\n",
    "        )\n",
    "        \n",
    "        num_epochs = 40\n",
    "        best_epoch = -1\n",
    "        best_score = 10000\n",
    "        early_stoppping_cnt = 0\n",
    "        best_model = model\n",
    "        \n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "            \n",
    "            model.train()\n",
    "            avg_loss = 0.\n",
    "            \n",
    "            for idx in tqdm(data_loader):\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                index = sum(index_df.iloc[idx]['index'].values.tolist(),[])\n",
    "                length = index_df.iloc[idx]['length'].values.tolist()\n",
    "                x_batch = _create_batch_data(index, Preprocesing_agent.data, Preprocesing_agent.calendar)\n",
    "                x_batch = x_batch[:,:,:-28]\n",
    "                gc.collect()\n",
    "                \n",
    "                x_batch, y_batch = split_X_y(x_batch)\n",
    "                x_batch = x_batch.to(DEVICE)\n",
    "                y_batch = y_batch.to(DEVICE)\n",
    "                \n",
    "                preds = model(x_batch)\n",
    "                \n",
    "                loss_ = criterion_(preds.cpu(), y_batch.cpu(), index, train=True)\n",
    "                loss_g = criterion_g(preds.cpu(), y_batch.cpu(), idx, length, train=True)\n",
    "                loss = 0.45*loss_+0.55*loss_g\n",
    "                del loss_; gc.collect()\n",
    "                loss = loss.to(DEVICE)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                avg_loss += loss_g.item() / len(data_loader)\n",
    "                del loss_g,loss; gc.collect()\n",
    "            \n",
    "            model.eval()\n",
    "            avg_val_loss = 0.\n",
    "            \n",
    "            for idx in data_loader:\n",
    "                index = sum(index_df.iloc[idx]['index'].values.tolist(),[])\n",
    "                length = index_df.iloc[idx]['length'].values.tolist()\n",
    "\n",
    "                x_batch = _create_batch_data(index, Preprocesing_agent.data, Preprocesing_agent.calendar)\n",
    "                \n",
    "                x_batch, y_batch = split_X_y(x_batch)\n",
    "                x_batch = x_batch.to(DEVICE)\n",
    "                y_batch = y_batch.to(DEVICE)\n",
    "                \n",
    "                preds = model(x_batch)\n",
    "                #loss_ = criterion_(preds.cpu(), y_batch.cpu(), index, train=False)\n",
    "                loss_g = criterion_g(preds.cpu(), y_batch.cpu(), idx, length, train=False)\n",
    "                \n",
    "                avg_val_loss += loss_g.item() / len(data_loader)\n",
    "                del loss_g; gc.collect()\n",
    "                \n",
    "                \n",
    "            if best_score>avg_val_loss:\n",
    "                best_score = avg_val_loss\n",
    "                early_stoppping_cnt=0\n",
    "                best_epoch=epoch\n",
    "                best_model = model\n",
    "                elapsed = time.time() - start_time\n",
    "                p_avg_val_loss = termcolor.colored(np.round(avg_val_loss, 4),\"red\")\n",
    "                \n",
    "                print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {p_avg_val_loss} time: {elapsed:.0f}s')\n",
    "            else:\n",
    "                early_stoppping_cnt+=1\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f} time: {elapsed:.0f}s')\n",
    "            \n",
    "            if (epoch>10) and (early_stoppping_cnt>5):\n",
    "                    break\n",
    "        \n",
    "        print(f'best_score : {best_score}    best_epoch : {best_epoch}')\n",
    "        #torch.save(best_score.state_dict(), 'net.pt')\n",
    "        \n",
    "        return best_model, best_score\n",
    "    \n",
    "    \n",
    "    def prepare_training_item_id_store_id_(self, model, Preprocesing_agent):\n",
    "        lr = 7e-4\n",
    "        eta_min = 1e-3\n",
    "        t_max = 10\n",
    "        model = model.to(DEVICE)\n",
    "        criterion = Loss_func_item_id_store_id_(cols=Preprocesing_agent.cols, df=Preprocesing_agent.df)\n",
    "        optimizer = RAdam(params=model.parameters(), lr=lr)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=t_max, eta_min=eta_min)\n",
    "        return model, criterion, optimizer, scheduler\n",
    "    \n",
    "    def train_model_item_id_store_id_(self, model, Preprocesing_agent):\n",
    "        data_set = item_id_store_id_Dataset(Preprocesing_agent.data, Preprocesing_agent.calendar)\n",
    "        data_loader = torch.utils.data.DataLoader(data_set, batch_size = 150, shuffle = True)\n",
    "        \n",
    "        model, criterion, optimizer, scheduler = self.prepare_training_item_id_store_id_(model, Preprocesing_agent)\n",
    "\n",
    "        num_epochs = 40\n",
    "        best_epoch = -1\n",
    "        best_score = 10000\n",
    "        early_stoppping_cnt = 0\n",
    "        best_model = model\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            start_time = time.time()\n",
    "\n",
    "            model.train()\n",
    "            avg_loss = 0.\n",
    "            #\n",
    "            for x_batch, idx in tqdm(data_loader):\n",
    "                optimizer.zero_grad()\n",
    "                x_batch = x_batch[:,:,:-28]; gc.collect()\n",
    "\n",
    "                x_batch, y_batch = split_X_y(x_batch)\n",
    "                x_batch = x_batch.to(DEVICE)\n",
    "                y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "                preds = model(x_batch)\n",
    "\n",
    "                loss = criterion(preds.cpu(), y_batch.cpu(), idx, train=True)\n",
    "                loss = loss.to(DEVICE)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                #scheduler.step()\n",
    "\n",
    "                avg_loss += loss.item() / len(data_loader)\n",
    "                del loss; gc.collect()\n",
    "\n",
    "            model.eval()\n",
    "            avg_val_loss = 0.\n",
    "\n",
    "            for x_batch, idx in data_loader:\n",
    "                x_batch, y_batch = split_X_y(x_batch)\n",
    "                x_batch = x_batch.to(DEVICE)\n",
    "                y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "                preds = model(x_batch)\n",
    "                loss = criterion(preds.cpu(), y_batch.cpu(), idx, train=False)\n",
    "\n",
    "                avg_val_loss += loss.item() / len(data_loader)\n",
    "                del loss; gc.collect()\n",
    "\n",
    "\n",
    "            if best_score>avg_val_loss:\n",
    "                best_score = avg_val_loss\n",
    "                early_stoppping_cnt=0\n",
    "                best_epoch=epoch+1\n",
    "                best_model = model\n",
    "                elapsed = time.time() - start_time\n",
    "                p_avg_val_loss = termcolor.colored(np.round(avg_val_loss, 4),\"red\")\n",
    "\n",
    "                print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {p_avg_val_loss} time: {elapsed:.0f}s')\n",
    "            else:\n",
    "                early_stoppping_cnt+=1\n",
    "                elapsed = time.time() - start_time\n",
    "                print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f} time: {elapsed:.0f}s')\n",
    "\n",
    "            if (epoch>10) and (early_stoppping_cnt>7):\n",
    "                    break\n",
    "\n",
    "        print(f'best_score : {best_score}    best_epoch : {best_epoch}')\n",
    "        #torch.save(best_score.state_dict(), 'net.pt')\n",
    "        return best_model, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89104ad546e14d5cacc8fa4622f4d3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30490.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([30490, 11, 150]) torch.Size([26, 150])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Train_Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-703401fd89eb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv_1d_Net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPreprocessing_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mTrain_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrain_Model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrain_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model_MIX\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPreprocessing_agent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Train_Model' is not defined"
     ]
    }
   ],
   "source": [
    "#path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\n",
    "path = '/Users/abcdm/Downloads/m5-forecasting-accuracy/'\n",
    "#path = '../input/m5-forecasting-accuracy/'\n",
    "\n",
    "groupId = ['item_id', 'state_id']\n",
    "#groupId = ['item_id', 'dept_id']\n",
    "\n",
    "Preprocessing_agent = PreProcessing(path=path, group_id=groupId)\n",
    "d_cols = Preprocessing_agent.d_cols\n",
    "cols = d_cols[-150:]\n",
    "Preprocessing_agent.make_data_loader(cols)\n",
    "print(Preprocessing_agent.data.size(), Preprocessing_agent.calendar.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90723b4c03d9440984429f27f97454d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 - avg_train_loss: 1.1549  avg_val_loss: \u001b[31m1.0257\u001b[0m time: 67s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eaba49f1d28414396475c6732db030e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 - avg_train_loss: 0.9702  avg_val_loss: \u001b[31m0.9502\u001b[0m time: 65s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a107b00eb4d4540b8db316f0aa88f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-bd5a1a5e9b8a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv_1d_Net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPreprocessing_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mTrain_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrain_Model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbest_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrain_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model_MIX\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPreprocessing_agent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-bd500c1587ab>\u001b[0m in \u001b[0;36mtrain_model_MIX\u001b[1;34m(self, model, Preprocesing_agent)\u001b[0m\n\u001b[0;32m     67\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.45\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloss_\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m0.55\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloss_g\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m                 \u001b[1;32mdel\u001b[0m \u001b[0mloss_\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Conv_1d_Net(Preprocessing_agent.in_size)\n",
    "Train_model = Train_Model()\n",
    "best_model, best_score = Train_model.train_model_MIX(model, Preprocessing_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d4d0ecfd2649b5a10a00e671f0c2a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30490.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "torch.Size([30490, 10, 150]) torch.Size([26, 150])\n"
     ]
    }
   ],
   "source": [
    "#path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\n",
    "path = '/Users/abcdm/Downloads/m5-forecasting-accuracy/'\n",
    "#path = '../input/m5-forecasting-accuracy/'\n",
    "\n",
    "#groupId = ['item_id', 'state_id']\n",
    "#groupId = ['item_id', 'dept_id']\n",
    "\n",
    "Preprocessing_agent = PreProcessing(path=path)\n",
    "d_cols = Preprocessing_agent.d_cols\n",
    "cols = d_cols[-150:]\n",
    "Preprocessing_agent.make_data_loader(cols)\n",
    "print(Preprocessing_agent.data.size(), Preprocessing_agent.calendar.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f09a97551dc446dbba88e30ef9898456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 - avg_train_loss: 1.0147  avg_val_loss: \u001b[31m0.9224\u001b[0m time: 31s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0a612e3c84c428a898204dd307b26fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 - avg_train_loss: 0.9134  avg_val_loss: \u001b[31m0.9145\u001b[0m time: 32s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f573c6451041ea9bfb392b3ca80872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 - avg_train_loss: 0.9102  avg_val_loss: 0.9155 time: 32s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f49f2643484a699bd58fc8f67a939d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 - avg_train_loss: 0.9068  avg_val_loss: 0.9148 time: 32s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d23383a2d798407da9ec2b02577c161d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 - avg_train_loss: 0.9056  avg_val_loss: \u001b[31m0.9065\u001b[0m time: 32s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa729f5e15014a9a825a7e15cedfd16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 - avg_train_loss: 0.9037  avg_val_loss: \u001b[31m0.9057\u001b[0m time: 32s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81015f4e3fb4644bfcd8f19dbd6d0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 - avg_train_loss: 0.9033  avg_val_loss: 0.9105 time: 32s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb1b7bf20b642279537a762de043aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 - avg_train_loss: 0.9030  avg_val_loss: 0.9099 time: 32s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c359b63c2fe8492e92188e3a8e1a4f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 - avg_train_loss: 0.9016  avg_val_loss: 0.9057 time: 32s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed474e93742e4b559c0c263b3a7e8159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 - avg_train_loss: 0.9003  avg_val_loss: \u001b[31m0.9014\u001b[0m time: 32s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19da85f4a0ff40eb8ccd23d06c2eb881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 - avg_train_loss: 0.8984  avg_val_loss: 0.9177 time: 32s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a774c53e938748118544ea6675d3275f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 - avg_train_loss: 0.8961  avg_val_loss: 0.9424 time: 32s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e8a92922fa41f3b662e818d314f18a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 - avg_train_loss: 0.8961  avg_val_loss: 0.9557 time: 32s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2ce386e74b4d52a008d7edfd6db314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 - avg_train_loss: 0.8953  avg_val_loss: 0.9561 time: 32s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96598d71dd264422a6991b03f313421e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 - avg_train_loss: 0.8943  avg_val_loss: 0.9340 time: 32s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73efdff5b944eb09de1dd24099bb5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 - avg_train_loss: 0.8946  avg_val_loss: 0.9463 time: 32s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a61fedd298544ad2993a7a13c1fb4030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=204.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-77136351b6d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv_1d_Net\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPreprocessing_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0min_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mTrain_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrain_Model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbest_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrain_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_model_item_id_store_id_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPreprocessing_agent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-bd500c1587ab>\u001b[0m in \u001b[0;36mtrain_model_item_id_store_id_\u001b[1;34m(self, model, Preprocesing_agent)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                 \u001b[0mavg_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m                 \u001b[1;32mdel\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m;\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Conv_1d_Net(Preprocessing_agent.in_size)\n",
    "Train_model = Train_Model()\n",
    "best_model, best_score = Train_model.train_model_item_id_store_id_(model, Preprocessing_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
