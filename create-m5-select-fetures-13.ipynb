{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "import gc, pickle\n",
    "import ast\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold,TimeSeriesSplit, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_squared_error, log_loss\n",
    "from sklearn.linear_model import Ridge,Lasso, BayesianRidge\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.preprocessing import minmax_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMS = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'tweedie',\n",
    "    'tweedie_variance_power': 1.1,\n",
    "    'metric': 'rmse',\n",
    "    'subsample': 0.5,\n",
    "    'subsample_freq': 1,\n",
    "    'learning_rate': 0.03,\n",
    "    'num_leaves': 2**11-1,\n",
    "    'min_data_in_leaf': 2**12-1,\n",
    "    'feature_fraction': 0.5,\n",
    "    'max_bin': 100,\n",
    "    'n_estimators': 1400,\n",
    "    'boost_from_average': False,\n",
    "    'verbose': 1,\n",
    "    \n",
    "    'random_state':2020\n",
    "    } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                       df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def create_is_sell_data(sell_prices_df, calendar_df, train_df):\n",
    "    sell_prices_df['id'] = sell_prices_df['item_id'].astype('str')+'_'+sell_prices_df['store_id']+'_evaluation'\n",
    "    sell_prices_data = sell_prices_df[sell_prices_df.wm_yr_wk.isin(calendar_df.wm_yr_wk.unique())]\n",
    "    sell_prices_data.reset_index(drop=True, inplace=True)\n",
    "    tmp = sell_prices_data.groupby(['id'])[['wm_yr_wk', 'sell_price']].apply(\n",
    "        lambda x: x.set_index('wm_yr_wk')['sell_price'].to_dict()\n",
    "    ).to_dict()\n",
    "    d = calendar_df.d\n",
    "    wm_yr_wk = calendar_df.wm_yr_wk\n",
    "    price_data = {}\n",
    "    for col in tqdm(train_df.id.unique()):\n",
    "        price_data[col] = wm_yr_wk.map(tmp[col])\n",
    "    price_data = pd.DataFrame(price_data)\n",
    "    price_data.index = d\n",
    "    is_sell = price_data.notnull().astype(float).T\n",
    "    price_data = price_data.fillna(0)\n",
    "    \n",
    "    is_sell.index=train_df.id\n",
    "    train_df.index=train_df.id\n",
    "    is_sell = pd.concat([\n",
    "        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']], is_sell\n",
    "    ], axis=1)\n",
    "    price_data = pd.concat([\n",
    "        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']], price_data.T  \n",
    "    ], axis=1)\n",
    "    \n",
    "    return price_data, is_sell\n",
    "\n",
    "def set_index(df, name):\n",
    "    d = {}\n",
    "    for col, value in df.iloc[0,:].items():\n",
    "        if type(col)==str:\n",
    "            if type(df[col].values[0])!=str:\n",
    "                v = 'd'\n",
    "            else:\n",
    "                v='id'\n",
    "        else:\n",
    "            v=name\n",
    "        d[col]=v\n",
    "    return d\n",
    "\n",
    "def dcol2int(col):\n",
    "    if col[:2]=='d_':\n",
    "        return int(col.replace('d_', ''))\n",
    "    else:\n",
    "        return col\n",
    "    \n",
    "def create_event_data(train_df, calendar_df):\n",
    "    new_df = pd.DataFrame()\n",
    "    D_COLS = [d for d in train_df.columns if type(d)!=str]\n",
    "    for event_name in ['event_name_1', 'event_name_2']:\n",
    "        tmp_df = pd.concat([\n",
    "            train_df.groupby(['dept_id'])[D_COLS].mean().T.astype(float),\n",
    "            train_df.groupby(['cat_id'])[D_COLS].mean().T.astype(float),\n",
    "            calendar_df.loc[D_COLS,event_name].replace(np.nan, 'NAN')\n",
    "        ],axis=1)\n",
    "\n",
    "        dept_id_cols = train_df.dept_id.unique().tolist()\n",
    "        cat_id_cols = train_df.cat_id.unique().tolist()\n",
    "\n",
    "        tmp_df = pd.concat([\n",
    "            tmp_df[[event_name]],\n",
    "            tmp_df.groupby([event_name])[dept_id_cols].transform(\n",
    "            lambda x: x.shift(1).rolling(len(x), min_periods=1).mean()\n",
    "            ),\n",
    "            tmp_df.groupby([event_name])[cat_id_cols].transform(\n",
    "            lambda x: x.shift(1).rolling(len(x), min_periods=1).mean()\n",
    "            )\n",
    "        ], axis=1)\n",
    "\n",
    "        tmp_df[dept_id_cols] = tmp_df[dept_id_cols]/tmp_df[dept_id_cols].rolling(56, min_periods=1).mean().shift(1)\n",
    "        tmp_df[cat_id_cols] = tmp_df[cat_id_cols]/tmp_df[cat_id_cols].rolling(56, min_periods=1).mean().shift(1)\n",
    "        tmp_df.loc[tmp_df[event_name]=='NAN', dept_id_cols+cat_id_cols]=1\n",
    "        \n",
    "        tmp_df.columns=[f'{event_name}_{col}' for col in tmp_df.columns]\n",
    "        \n",
    "        new_df = pd.concat([\n",
    "            new_df, tmp_df\n",
    "        ] ,axis=1)\n",
    "    new_df.index=D_COLS\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def create_metadata(path, d_cols, submmit=True):\n",
    "    train_df = pd.read_csv(path+'sales_train_evaluation.csv')\n",
    "    calendar_df = pd.read_csv(path+'calendar.csv')\n",
    "    sell_prices_df = pd.read_csv(path+'sell_prices.csv')\n",
    "    #sample_submission_df = pd.read_csv(path+'sample_submission.csv')\n",
    "\n",
    "    calendar_df['d'] = calendar_df.d.str.replace('d_', '').astype(int)\n",
    "    cols = train_df.columns\n",
    "    cols = [dcol2int(col) for col in cols]\n",
    "    train_df.columns=cols\n",
    "    calendar_df['date']=pd.to_datetime(calendar_df.date)\n",
    "    calendar_df.index = calendar_df.d\n",
    "    price_data, is_sell = create_is_sell_data(sell_prices_df, calendar_df, train_df)\n",
    "    \n",
    "    str_cols = [ col for col in train_df.columns if 'id' in str(col)]\n",
    "    new_columns = str_cols+d_cols\n",
    "    train_df = train_df.reindex(columns=new_columns)\n",
    "    \n",
    "    \n",
    "    train_df = pd.concat([\n",
    "        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']],\n",
    "        train_df.loc[:,d_cols]+is_sell[d_cols].replace(0, np.nan).replace(1, 0)\n",
    "    ], axis=1)\n",
    "    train_df.index = train_df.id\n",
    "    del is_sell;gc.collect()\n",
    "    \n",
    "    df = train_df.loc[:,d_cols].T.astype(float)\n",
    "    a = df.loc[d_cols[28:-56]].rolling(28, min_periods=1).sum().replace(0,np.nan)+df.loc[d_cols[28:-56]][::-1].rolling(28, min_periods=1).sum()[::-1].replace(0,np.nan)\n",
    "    a[a.notnull()]=0\n",
    "    df.loc[d_cols[28:-56]] += a\n",
    "    df = df.loc[d_cols,:].T.astype(float)\n",
    "    del a;gc.collect()\n",
    "    \n",
    "    #snap_data\n",
    "    snap_data = calendar_df[['snap_CA', 'snap_WI', 'snap_TX', 'd']]\n",
    "    snap_data.set_index('d', inplace=True)\n",
    "    \n",
    "    #dept_id_price\n",
    "    dept_id_price = price_data[d_cols]/price_data.groupby(['dept_id', 'store_id'])[d_cols].transform('mean')\n",
    "    dept_id_price = dept_id_price.T.astype(float)\n",
    "    #dept_id_price['d'] = dept_id_price.index\n",
    "    dept_id_price = dept_id_price.replace(0,np.nan)\n",
    "    \n",
    "    #cat_id_price\n",
    "    cat_id_price = price_data[d_cols]/price_data.groupby(['cat_id', 'store_id'])[d_cols].transform('mean')\n",
    "    cat_id_price = cat_id_price.T.astype(float)\n",
    "    #cat_id_price['d'] = cat_id_price.index\n",
    "    cat_id_price = cat_id_price.replace(0,np.nan)\n",
    "    \n",
    "    #price_data\n",
    "    price_data = price_data[d_cols].T\n",
    "    price_data.replace(0,np.nan, inplace=True)\n",
    "    #price_data['d']=price_data.index\n",
    "    \n",
    "    #event_df\n",
    "    event_df = create_event_data(train_df, calendar_df)\n",
    "    #event_df.reset_index(inplace=True)\n",
    "    \n",
    "    #calendar_dict\n",
    "    calendar_dict = calendar_df[['wday', 'month']].to_dict()\n",
    "    \n",
    "    return train_df, snap_data, dept_id_price, cat_id_price, price_data, event_df, calendar_dict, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_shift_roll_data(data, shift, agg={'mean', 'std'}):\n",
    "    data_2 = data.groupby(['id'])['TARGET'].apply(\n",
    "        lambda x:\n",
    "        x.shift(shift).rolling(14, min_periods=1).agg(agg)\n",
    "    )\n",
    "    for col in data_2.columns:\n",
    "        data[f'shift_roll_14_{col}'] = data_2[col]\n",
    "        \n",
    "    data_2 = data.groupby(['id'])['TARGET'].apply(\n",
    "        lambda x:\n",
    "        x.shift(shift).rolling(28, min_periods=1).agg(agg)\n",
    "    )\n",
    "    for col in data_2.columns:\n",
    "        data[f'shift_roll_28_{col}'] = data_2[col]\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def make_roll_data(data, win, agg={'mean', 'std'}):\n",
    "    data_2 = data.groupby(['id'])['TARGET'].apply(\n",
    "        lambda x:\n",
    "        x.shift(1).rolling(win, min_periods=1).agg(agg)\n",
    "    )\n",
    "    for col in data_2.columns:\n",
    "        data[f'roll_{win}_{col}'] = data_2[col]\n",
    "        \n",
    "    return data\n",
    "\n",
    "def make_diff_data(data, win):\n",
    "    diff_data = data.groupby(['id'])['TARGET'].apply(\n",
    "        lambda x:\n",
    "        abs(x.shift(1).diff(1)).rolling(win, min_periods=1).agg({'mean', 'std'})\n",
    "    ) \n",
    "    for col in diff_data.columns:\n",
    "        data[f'diff_{col}_{win}_1'] = diff_data[col]\n",
    "    \n",
    "    diff_data = data.groupby(['id'])['TARGET'].apply(\n",
    "        lambda x:\n",
    "        abs(x.shift(1).diff(7)).rolling(win, min_periods=1).agg({'mean', 'std'})\n",
    "    ) \n",
    "    for col in diff_data.columns:\n",
    "        data[f'diff_{col}_{win}_7'] = diff_data[col]\n",
    "    \n",
    "    diff_data = data.groupby(['id'])['TARGET'].apply(\n",
    "        lambda x:\n",
    "        abs(x.shift(1).diff(28)).rolling(win, min_periods=1).agg({'mean', 'std'})\n",
    "    ) \n",
    "    for col in diff_data.columns:\n",
    "        data[f'diff_{col}_{win}_28'] = diff_data[col]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def make_shift_data(data):\n",
    "    shift=7\n",
    "    for i, p in  enumerate(range(4)):\n",
    "        data[f'shift_{i+1}'] = data.groupby(['id'])['TARGET'].shift(shift+7*p)\n",
    "        data[f'shift_{i+1}_by_dept'] = data.groupby(['dept_id'])['TARGET'].shift(shift+7*p)\n",
    "        data[f'shift_{i+1}_by_cat'] = data.groupby(['cat_id'])['TARGET'].shift(shift+7*p)\n",
    "        data[f'shift_{i+1}_by_store'] = data.groupby(['store_id'])['TARGET'].shift(shift+7*p)\n",
    "        data[f'shift_{i+1}_by_state'] = data.groupby(['state_id'])['TARGET'].shift(shift+7*p)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def preprocessing(path,d_cols,test):\n",
    "    train_df, snap_data, dept_id_price, cat_id_price, price_data, event_df, calendar_dict, df = create_metadata(path, d_cols)\n",
    "    if test:\n",
    "        train_df = train_df[train_df.id.isin(train_df.id.unique()[:2000])]\n",
    "    \n",
    "    data = train_df[d_cols[-128:]].stack(dropna=False).reset_index()\n",
    "    data = data.rename(columns=set_index(data, 'TARGET'))\n",
    "    data.sort_values('d', inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data = reduce_mem_usage(data)\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    for key, value in train_df[['dept_id', 'cat_id', 'state_id', 'store_id']].to_dict().items():\n",
    "        data[key] = data.id.map(value)\n",
    "    \n",
    "    data[f'snap']=0\n",
    "    for key, value in snap_data.to_dict().items():\n",
    "        k = key.replace('snap_', '')\n",
    "        data.loc[data.state_id==k,'snap'] = data.loc[data.state_id==k, 'd'].map(value).fillna(0)\n",
    "    for shift in [-3,-2,-1,1,2,3]:\n",
    "        data[f'snap_{shift}'] = data.groupby(['id'])['snap'].shift(shift).fillna(0)\n",
    "\n",
    "\n",
    "    dept_id_price = dept_id_price.stack(dropna=False).reset_index()\n",
    "    cat_id_price = cat_id_price.stack(dropna=False).reset_index()\n",
    "\n",
    "    dept_id_price.rename(columns=set_index(dept_id_price, 'dept_id_price'), inplace=True)\n",
    "    cat_id_price.rename(columns=set_index(cat_id_price, 'cat_id_price'), inplace=True)\n",
    "\n",
    "    data = pd.merge(\n",
    "        data, dept_id_price, on=['d', 'id'], how='left'\n",
    "    )\n",
    "    data = pd.merge(\n",
    "        data, cat_id_price, on=['d', 'id'], how='left'\n",
    "    )\n",
    "\n",
    "\n",
    "    del dept_id_price,cat_id_price;gc.collect()\n",
    "\n",
    "    price_data = price_data.stack(dropna=False).reset_index()\n",
    "    price_data.rename(columns=set_index(price_data, 'price'), inplace=True)\n",
    "    data = pd.merge(\n",
    "        data, price_data, on=['d', 'id'], how='left'\n",
    "    )\n",
    "    del price_data;gc.collect()\n",
    "\n",
    "    data['wday'] = data.d.map(calendar_dict['wday'])\n",
    "    data['month'] = data.d.map(calendar_dict['month'])\n",
    "    del calendar_dict;gc.collect()\n",
    "\n",
    "\n",
    "    tmp_dic = event_df.to_dict()\n",
    "    data[f'dept_id_event_name_1']=1\n",
    "    data[f'cat_id_event_name_1']=1\n",
    "    for key, value in tmp_dic.items():\n",
    "        if 'event_name_1' in key:\n",
    "            if key[13:] in train_df.dept_id.unique().tolist():\n",
    "                data.loc[data.dept_id==key[13:], f'dept_id_{key[:12]}']=data.loc[data.dept_id==key[13:], 'd'].map(value).fillna(1)\n",
    "            if key[13:] in train_df.cat_id.unique().tolist():\n",
    "                data.loc[data.cat_id==key[13:], f'cat_id_{key[:12]}']=data.loc[data.cat_id==key[13:], 'd'].map(value).fillna(1)\n",
    "    for shift in [-7,-1,1]:\n",
    "        for event_name in ['dept_id_event_name_1', 'cat_id_event_name_1']:\n",
    "            data[f'{event_name}_shift{shift}'] = data.groupby(['id'])[event_name].shift(shift).fillna(1)\n",
    "\n",
    "    categories = [c for c in data.columns if data[c].dtype==object]\n",
    "    print(categories)\n",
    "    for c in categories:\n",
    "        if c=='id':\n",
    "            pass\n",
    "        else:\n",
    "            data[c] = pd.factorize(data[c])[0]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def create_sale_feature(data):\n",
    "    cols = data.columns.tolist()\n",
    "    \n",
    "    data = make_shift_roll_data(data=data, shift=14, agg={'mean', 'std'})\n",
    "    data = make_shift_roll_data(data=data, shift=28, agg={'max'})\n",
    "    data = make_roll_data(data=data,win=28,agg={'mean', 'std', 'max', 'min'})\n",
    "    data = make_roll_data(data=data,win=7,agg={'mean', 'min', 'max'})\n",
    "    data = make_diff_data(data=data, win=28)\n",
    "    data = make_diff_data(data=data, win=7)\n",
    "    one_cols = [col for col in data.columns if not col in cols]\n",
    "    cols = data.columns.tolist()\n",
    "    \n",
    "    data = make_shift_data(data=data)\n",
    "    seven_cols = [col for col in data.columns if not col in cols]\n",
    "    \n",
    "    return data, one_cols, seven_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_2(train_df, snap_data, dept_id_price, cat_id_price, price_data, event_df, calendar_dict, df, d_cols,test):\n",
    "    \n",
    "    if test:\n",
    "        train_df = train_df[train_df.id.isin(train_df.id.unique()[:2000])]\n",
    "    \n",
    "    data = train_df[d_cols[-370:]].stack(dropna=False).reset_index()\n",
    "    data = data.rename(columns=set_index(data, 'TARGET'))\n",
    "    data.sort_values('d', inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data = reduce_mem_usage(data)\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    for key, value in train_df[['dept_id', 'cat_id', 'state_id', 'store_id']].to_dict().items():\n",
    "        data[key] = data.id.map(value)\n",
    "    \n",
    "    data[f'snap']=0\n",
    "    for key, value in snap_data.to_dict().items():\n",
    "        k = key.replace('snap_', '')\n",
    "        data.loc[data.state_id==k,'snap'] = data.loc[data.state_id==k, 'd'].map(value).fillna(0)\n",
    "    for shift in [-3,-2,-1,1,2,3]:\n",
    "        data[f'snap_{shift}'] = data.groupby(['id'])['snap'].shift(shift).fillna(0)\n",
    "\n",
    "\n",
    "    dept_id_price = dept_id_price.stack(dropna=False).reset_index()\n",
    "    cat_id_price = cat_id_price.stack(dropna=False).reset_index()\n",
    "\n",
    "    dept_id_price.rename(columns=set_index(dept_id_price, 'dept_id_price'), inplace=True)\n",
    "    cat_id_price.rename(columns=set_index(cat_id_price, 'cat_id_price'), inplace=True)\n",
    "\n",
    "    data = pd.merge(\n",
    "        data, dept_id_price, on=['d', 'id'], how='left'\n",
    "    )\n",
    "    data = pd.merge(\n",
    "        data, cat_id_price, on=['d', 'id'], how='left'\n",
    "    )\n",
    "\n",
    "\n",
    "    del dept_id_price,cat_id_price;gc.collect()\n",
    "\n",
    "    price_data = price_data.stack(dropna=False).reset_index()\n",
    "    price_data.rename(columns=set_index(price_data, 'price'), inplace=True)\n",
    "    data = pd.merge(\n",
    "        data, price_data, on=['d', 'id'], how='left'\n",
    "    )\n",
    "    del price_data;gc.collect()\n",
    "\n",
    "    data['wday'] = data.d.map(calendar_dict['wday'])\n",
    "    data['month'] = data.d.map(calendar_dict['month'])\n",
    "    del calendar_dict;gc.collect()\n",
    "\n",
    "\n",
    "    tmp_dic = event_df.to_dict()\n",
    "    data[f'dept_id_event_name_1']=1\n",
    "    data[f'cat_id_event_name_1']=1\n",
    "    for key, value in tmp_dic.items():\n",
    "        if 'event_name_1' in key:\n",
    "            if key[13:] in train_df.dept_id.unique().tolist():\n",
    "                data.loc[data.dept_id==key[13:], f'dept_id_{key[:12]}']=data.loc[data.dept_id==key[13:], 'd'].map(value).fillna(1)\n",
    "            if key[13:] in train_df.cat_id.unique().tolist():\n",
    "                data.loc[data.cat_id==key[13:], f'cat_id_{key[:12]}']=data.loc[data.cat_id==key[13:], 'd'].map(value).fillna(1)\n",
    "    for shift in [-7,-1,1]:\n",
    "        for event_name in ['dept_id_event_name_1', 'cat_id_event_name_1']:\n",
    "            data[f'{event_name}_shift{shift}'] = data.groupby(['id'])[event_name].shift(shift).fillna(1)\n",
    "\n",
    "    categories = [c for c in data.columns if data[c].dtype==object]\n",
    "    print(categories)\n",
    "    for c in categories:\n",
    "        if c=='id':\n",
    "            pass\n",
    "        else:\n",
    "            data[c] = pd.factorize(data[c])[0]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_3(path,d_cols,test):\n",
    "    train_df, snap_data, dept_id_price, cat_id_price, price_data, event_df, calendar_dict, df = create_metadata(path, d_cols)\n",
    "    if test:\n",
    "        train_df = train_df[train_df.id.isin(train_df.id.unique()[:2000])]\n",
    "    \n",
    "    data = train_df[d_cols].stack(dropna=False).reset_index()\n",
    "    data = data.rename(columns=set_index(data, 'TARGET'))\n",
    "    data.sort_values('d', inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data = reduce_mem_usage(data)\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    for key, value in train_df[['dept_id', 'cat_id', 'state_id', 'store_id']].to_dict().items():\n",
    "        data[key] = data.id.map(value)\n",
    "    \n",
    "    data[f'snap']=0\n",
    "    for key, value in snap_data.to_dict().items():\n",
    "        k = key.replace('snap_', '')\n",
    "        data.loc[data.state_id==k,'snap'] = data.loc[data.state_id==k, 'd'].map(value).fillna(0)\n",
    "    for shift in [-3,-2,-1,1,2,3]:\n",
    "        data[f'snap_{shift}'] = data.groupby(['id'])['snap'].shift(shift).fillna(0)\n",
    "\n",
    "\n",
    "    dept_id_price = dept_id_price.stack(dropna=False).reset_index()\n",
    "    cat_id_price = cat_id_price.stack(dropna=False).reset_index()\n",
    "\n",
    "    dept_id_price.rename(columns=set_index(dept_id_price, 'dept_id_price'), inplace=True)\n",
    "    cat_id_price.rename(columns=set_index(cat_id_price, 'cat_id_price'), inplace=True)\n",
    "\n",
    "    data = pd.merge(\n",
    "        data, dept_id_price, on=['d', 'id'], how='left'\n",
    "    )\n",
    "    data = pd.merge(\n",
    "        data, cat_id_price, on=['d', 'id'], how='left'\n",
    "    )\n",
    "\n",
    "\n",
    "    del dept_id_price,cat_id_price;gc.collect()\n",
    "\n",
    "    price_data = price_data.stack(dropna=False).reset_index()\n",
    "    price_data.rename(columns=set_index(price_data, 'price'), inplace=True)\n",
    "    data = pd.merge(\n",
    "        data, price_data, on=['d', 'id'], how='left'\n",
    "    )\n",
    "    del price_data;gc.collect()\n",
    "\n",
    "    data['wday'] = data.d.map(calendar_dict['wday'])\n",
    "    data['month'] = data.d.map(calendar_dict['month'])\n",
    "    del calendar_dict;gc.collect()\n",
    "\n",
    "\n",
    "    tmp_dic = event_df.to_dict()\n",
    "    data[f'dept_id_event_name_1']=1\n",
    "    data[f'cat_id_event_name_1']=1\n",
    "    for key, value in tmp_dic.items():\n",
    "        if 'event_name_1' in key:\n",
    "            if key[13:] in train_df.dept_id.unique().tolist():\n",
    "                data.loc[data.dept_id==key[13:], f'dept_id_{key[:12]}']=data.loc[data.dept_id==key[13:], 'd'].map(value).fillna(1)\n",
    "            if key[13:] in train_df.cat_id.unique().tolist():\n",
    "                data.loc[data.cat_id==key[13:], f'cat_id_{key[:12]}']=data.loc[data.cat_id==key[13:], 'd'].map(value).fillna(1)\n",
    "    for shift in [-7,-1,1]:\n",
    "        for event_name in ['dept_id_event_name_1', 'cat_id_event_name_1']:\n",
    "            data[f'{event_name}_shift{shift}'] = data.groupby(['id'])[event_name].shift(shift).fillna(1)\n",
    "\n",
    "    categories = [c for c in data.columns if data[c].dtype==object]\n",
    "    print(categories)\n",
    "    for c in categories:\n",
    "        if c=='id':\n",
    "            pass\n",
    "        else:\n",
    "            data[c] = pd.factorize(data[c])[0]\n",
    "    \n",
    "    data['TARGET'] = data['TARGET']*data['price']\n",
    "    data, one_cols, seven_cols = create_sale_feature(data)\n",
    "    \n",
    "    data = data[data.d.isin(d_cols[-188:])]\n",
    "    \n",
    "    return data, one_cols, seven_cols\n",
    "\n",
    "\n",
    "def predict_cv(x_val, models):\n",
    "    preds = np.zeros(len(x_val))\n",
    "    for model in models:\n",
    "        pred = model.predict(x_val)\n",
    "        pred = np.e**(pred)-1\n",
    "        preds+=pred/len(models)\n",
    "    return preds\n",
    "\n",
    "def train(data, params=PARAMS):\n",
    "    #data.reset_index(drop=True, inplace=True)\n",
    "    data = data[data.TARGET.notnull()]\n",
    "    data = data.reset_index(drop=True)\n",
    "    models = []\n",
    "    k = GroupKFold(n_splits=6)\n",
    "    categories = ['cat_id', 'dept_id', 'store_id']\n",
    "\n",
    "    y = np.log1p(data['TARGET'])\n",
    "    group = data['group']\n",
    "    data = data.drop(columns=['id', 'd', 'TARGET', 'month', 'group'], axis=1)\n",
    "    cols = data.columns.tolist()\n",
    "\n",
    "    stop_cnt = 0\n",
    "    for trn_indx, val_indx in k.split(data, groups=group):\n",
    "        train_set = lgb.Dataset(data.loc[trn_indx,:], y.loc[trn_indx])\n",
    "        val_set = lgb.Dataset(data.loc[val_indx,:], y.loc[val_indx])\n",
    "\n",
    "        \n",
    "        model = lgb.train(\n",
    "            train_set=train_set, \n",
    "            valid_sets=[train_set, val_set],\n",
    "            params=params, num_boost_round=3000, early_stopping_rounds=100, verbose_eval=500,\n",
    "            categorical_feature=categories+['wday']\n",
    "        )\n",
    "        \n",
    "        models.append(model)\n",
    "        gc.collect()\n",
    "        stop_cnt+=1\n",
    "        if stop_cnt==3:\n",
    "            break\n",
    "    return models, cols\n",
    "\n",
    "\n",
    "def main():\n",
    "    #path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\n",
    "    #path = '/Users/abcdm/Downloads/m5-forecasting-accuracy/'\n",
    "    path = '../input/m5-forecasting-accuracy/'\n",
    "    d_cols=[i+1 for i in range(1969)]\n",
    "    test = False\n",
    "    data, one_cols, seven_cols = preprocessing_3(path,d_cols[200:],test=test)\n",
    "    \n",
    "    \n",
    "    data['group'] = data['wday'].astype(str)+'_'+data['dept_id'].astype(str)\n",
    "    print(data.shape)\n",
    "    day_one = [-28]\n",
    "    sub_df = pd.DataFrame()\n",
    "    for i in range(28):\n",
    "        models, cols = train(data, params=PARAMS)\n",
    "        pred_X=data[data.d==day_one+i]\n",
    "        pred = predict_cv(pred_X[cols], models)\n",
    "        pred_X['pred'] = pred/pred_X['price']\n",
    "        sub_df = pd.concat([\n",
    "            sub_df, pred_X[['id', 'd', 'pred']]\n",
    "        ], axis=0)\n",
    "        if i%7==0 and i>0:\n",
    "            data[seven_cols] = data.groupby(['id'])[seven_cols].shift(7)\n",
    "        data[one_cols] = data.groupby(['id'])[one_cols].shift(1)\n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_importance(models, col, name):\n",
    "    importances = np.zeros(len(col))\n",
    "    for model in models:\n",
    "        importances+=model.feature_importance(importance_type='gain')\n",
    "    importance = pd.DataFrame()\n",
    "    importance['col'] = col\n",
    "    importance[f'importance_{name}'] = minmax_scale(importances)\n",
    "    #importance[f'importance_{name}'] = (importance)\n",
    "    #importance.to_csv(f'importance_{name}.csv',index=False)\n",
    "    return importance\n",
    "\n",
    "def predict_cv(x_val, models):\n",
    "    preds = np.zeros(len(x_val))\n",
    "    for model in models:\n",
    "        preds+=model.predict(x_val)/len(models)\n",
    "    return preds\n",
    "\n",
    "def split_data_for_sub(data):\n",
    "    data = data[data.TARGET.notnull()]\n",
    "    data = data[data.shift_4.notnull()]\n",
    "    data = data[data.diff_std_7.notnull()]\n",
    "    trn_df = data[['id', 'd', 'TARGET']]\n",
    "    y = np.log1p(data['TARGET']).astype(float)\n",
    "    X = data.drop(columns=['id','d', 'TARGET','state_id']).astype(float)\n",
    "    X.reset_index(drop=True, inplace=True)\n",
    "    y.reset_index(drop=True, inplace=True)\n",
    "    trn_df.reset_index(drop=True, inplace=True)\n",
    "    return X, y, trn_df\n",
    "\n",
    "def run_cv_for_sub(X, y, trn_df, params=PARAMS):\n",
    "    models = []\n",
    "    k = GroupKFold(n_splits=5)\n",
    "    trn_df['y_pred'] = 0\n",
    "    \n",
    "    for trn_indx, val_indx in k.split(X[['dept_id']],groups=X['dept_id']):\n",
    "        train_set = lgb.Dataset(X.loc[trn_indx,:], y.loc[trn_indx])\n",
    "        val_set = lgb.Dataset(X.loc[val_indx,:], y.loc[val_indx])\n",
    "        \n",
    "        categories = ['cat_id', 'dept_id', 'store_id']\n",
    "        \n",
    "        model = lgb.train(\n",
    "            train_set=train_set, \n",
    "            valid_sets=[train_set, val_set],\n",
    "            params=params, num_boost_round=3000, early_stopping_rounds=100, verbose_eval=500,\n",
    "            categorical_feature=categories+['wday']\n",
    "        )\n",
    "        \n",
    "        models.append(model)\n",
    "        trn_df.loc[val_indx, 'y_pred']=np.e**(model.predict(X.loc[val_indx,:]))-1\n",
    "        gc.collect()\n",
    "        \n",
    "    return models, trn_df\n",
    "\n",
    "def train_sub_predict_direct(data, for_predict):\n",
    "    train_d_cols = data.d.unique().tolist()\n",
    "    predict_day=train_d_cols[-28:][for_predict-1]\n",
    "    sub_predict_data = data[data.d==predict_day]\n",
    "    X, y, trn_df = split_data_for_sub(data)\n",
    "    print(X.shape)\n",
    "    models, trn_df = run_cv_for_sub(X, y, trn_df)\n",
    "    preds = predict_cv(sub_predict_data[X.columns], models)\n",
    "    \n",
    "    sub_df = sub_predict_data[['id', 'd', 'TARGET']]\n",
    "    sub_df[f'y_pred'] = preds\n",
    "    return trn_df, sub_df\n",
    "\n",
    "def train_sub_predict_recursive(data, params=PARAMS):\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    trn_df = data[['id', 'd', 'TARGET']]\n",
    "    trn_df['y_pred'] = np.nan\n",
    "    models = []\n",
    "    k = GroupKFold(n_splits=5)\n",
    "    categories = ['cat_id', 'dept_id', 'store_id']\n",
    "\n",
    "    y = data['TARGET']\n",
    "    group = data['group']\n",
    "    data = data.drop(columns=['id', 'd', 'TARGET', 'month', 'group'], axis=1)\n",
    "    cols = data.columns.tolist()\n",
    "\n",
    "    for trn_indx, val_indx in k.split(data, groups=group):\n",
    "        train_set = lgb.Dataset(data.loc[trn_indx,:], y.loc[trn_indx])\n",
    "        val_set = lgb.Dataset(data.loc[val_indx,:], y.loc[val_indx])\n",
    "\n",
    "        \n",
    "        model = lgb.train(\n",
    "            train_set=train_set, \n",
    "            valid_sets=[train_set, val_set],\n",
    "            params=params, num_boost_round=3000, early_stopping_rounds=100, verbose_eval=500,\n",
    "            categorical_feature=categories+['wday']\n",
    "        )\n",
    "        \n",
    "        models.append(model)\n",
    "        trn_df.loc[val_indx, 'y_pred']=model.predict(data.loc[val_indx,:])\n",
    "        gc.collect()\n",
    "    return models, trn_df, cols\n",
    "\n",
    "def predict_recursive(data, predict_d, cols, models):\n",
    "    \n",
    "    input_X = data[data.d==predict_d]\n",
    "    input_X = input_X[cols]\n",
    "    preds = predict_cv(input_X, models)\n",
    "    data.loc[data.d==predict_d,'TARGET'] = preds\n",
    "    #data = data.drop(columns=del_cols)\n",
    "    for _ in range(27):\n",
    "        data = create_sale_feature(data)\n",
    "        predict_d += 1\n",
    "        input_X = data[data.d==predict_d]\n",
    "        input_X = input_X[cols]\n",
    "        preds = predict_cv(input_X, models)\n",
    "        data.loc[data.d==predict_d,'TARGET'] = preds\n",
    "        #data = data.drop(columns=del_cols)\n",
    "        #del_index = data[data.d==data.d.min()].index\n",
    "        #data = data.drop(index=del_index)\n",
    "        gc.collect()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\\n#path = '/Users/abcdm/Downloads/m5-forecasting-accuracy/'\\npath = '../input/m5-forecasting-accuracy/'\\nd_cols=[i+1 for i in range(1941)]\\n%%time\\ntest = False\\ndata = preprocessing(path,d_cols[400:],test=test)\\ndata = create_sale_feature(data)\\ndata.to_csv('data.csv', index=False)\\nmodels, trn_df, cols = train_sub_predict_recursive(data[data.TARGET.notnull()], params=PARAMS)\\ndata = predict_recursive(data[data.d.isin(d_cols[100:])], d_cols[400:][-28], cols, models)\\ndata[data.d.isin(d_cols[400:][-28:])].to_csv('data_2.csv', index=False)\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\n",
    "#path = '/Users/abcdm/Downloads/m5-forecasting-accuracy/'\n",
    "path = '../input/m5-forecasting-accuracy/'\n",
    "d_cols=[i+1 for i in range(1941)]\n",
    "%%time\n",
    "test = False\n",
    "data = preprocessing(path,d_cols[400:],test=test)\n",
    "data = create_sale_feature(data)\n",
    "data.to_csv('data.csv', index=False)\n",
    "models, trn_df, cols = train_sub_predict_recursive(data[data.TARGET.notnull()], params=PARAMS)\n",
    "data = predict_recursive(data[data.d.isin(d_cols[100:])], d_cols[400:][-28], cols, models)\n",
    "data[data.d.isin(d_cols[400:][-28:])].to_csv('data_2.csv', index=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\n",
    "    #path = '/Users/abcdm/Downloads/m5-forecasting-accuracy/'\n",
    "    path = '../input/m5-forecasting-accuracy/'\n",
    "    d_cols=[i+1 for i in range(1969)]\n",
    "    test = False\n",
    "    data = preprocessing_3(path,d_cols[200:],test=test)\n",
    "    \n",
    "    \n",
    "    data['group'] = data['wday'].astype(str)+'_'+data['dept_id'].astype(str)\n",
    "    print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "41+28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\n",
    "    #path = '/Users/abcdm/Downloads/m5-forecasting-accuracy/'\n",
    "    path = '../input/m5-forecasting-accuracy/'\n",
    "    #path = '../input/null-of-create-m5-select-fetures/'\n",
    "    \n",
    "    path = '../input/m5-forecasting-accuracy/'\n",
    "    d_cols=[i+1 for i in range(1969)]\n",
    "    test = False\n",
    "    \n",
    "    d_cols = d_cols[-200:]\n",
    "    \n",
    "    data = preprocessing(path,d_cols,test)\n",
    "    data['TARGET'] = data['TARGET']*data['price']\n",
    "    data['group'] = data['wday'].astype(str)+'_'+data['dept_id'].astype(str)\n",
    "    data = create_sale_feature(data)\n",
    "    \n",
    "    print(data.shape)\n",
    "\n",
    "    models, trn_df, cols = train_sub_predict_recursive(data, params=PARAMS)\n",
    "    importance = plot_importance(models, cols, 'true')\n",
    "    importance.to_csv('importance_true.csv', index=False)\n",
    "    \n",
    "    predict_d = 1914\n",
    "    data = data[data.d.isin(d_cols[-70:])]\n",
    "    data = predict_recursive(data, predict_d, cols, models)\n",
    "    \n",
    "    data[data.d.isin(d_cols[-28:])][['id', 'd', 'TARGET', 'price']]#.to_csv(f'submisson_{store_id}.csv', index=False)\n",
    "    \n",
    "    \n",
    "    submissoin['TARGET'] = submissoin['TARGET']/submissoin['price']\n",
    "    \n",
    "    sample = pd.read_csv('../input/tx-of-create-m5-select-fetures-11/submisson.csv')\n",
    "    sub = pd.DataFrame()\n",
    "    for i in range(28):\n",
    "        d = 1942+i\n",
    "        tmp_df = submissoin[submissoin.d==d]\n",
    "        tmp_df = tmp_df[['id', 'TARGET']]\n",
    "        tmp_df.columns = ['id', f'F{i+1}']\n",
    "        tmp_df = tmp_df.set_index('id')\n",
    "        sub = pd.concat([\n",
    "            sub,\n",
    "            tmp_df\n",
    "        ], axis=1)\n",
    "\n",
    "    sub = sub.reindex(index=sample.id, fill_value=0)\n",
    "    sub.index.name = 'id'\n",
    "    sub = sub.reset_index()\n",
    "    sub.to_csv('submissoin.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\n",
    "    #path = '/Users/abcdm/Downloads/m5-forecasting-accuracy/'\n",
    "    path = '../input/m5-forecasting-accuracy/'\n",
    "    #path = '../input/null-of-create-m5-select-fetures/'\n",
    "    \n",
    "    path = '../input/m5-forecasting-accuracy/'\n",
    "    d_cols=[i+1 for i in range(1941)]\n",
    "    test = False\n",
    "    \n",
    "    d_cols = d_cols[-200:]\n",
    "    \n",
    "    train_df, snap_data, dept_id_price, cat_id_price, price_data, event_df, calendar_dict, df = create_metadata(path, d_cols)\n",
    "    \n",
    "    ca_store = [f'CA_{l}' for l in range(1,5)]\n",
    "    tx_store = [f'TX_{l}' for l in range(1,4)]\n",
    "    wi_store = [f'WI_{l}' for l in range(1,4)]\n",
    "    store_ids = ca_store+tx_store+wi_store\n",
    "    \n",
    "    sub_df = pd.DataFrame()\n",
    "    \n",
    "    for store_id in store_ids:\n",
    "        #store_id = f'TX_{l}'\n",
    "        data = preprocessing_2(train_df[train_df.store_id==store_id], snap_data, dept_id_price,\n",
    "                                    cat_id_price, price_data, event_df, calendar_dict, df, d_cols,test)\n",
    "\n",
    "\n",
    "        data['TARGET'] = data['TARGET']*data['price']\n",
    "        data['group'] = data['wday'].astype(str)+'_'+data['dept_id'].astype(str)\n",
    "\n",
    "\n",
    "        data = create_sale_feature(data)\n",
    "\n",
    "\n",
    "\n",
    "        #data.to_csv('data_ca_1.csv', index=False)\n",
    "        #data = pd.read_csv(path+'data.csv')\n",
    "        #data = data[data.store_id==0]\n",
    "\n",
    "        print(data.shape)\n",
    "\n",
    "        models, trn_df, cols = train_sub_predict_recursive(data, params=PARAMS)\n",
    "        importance = plot_importance(models, cols, 'true')\n",
    "        importance.to_csv(f'importance_true_{store_id}.csv', index=False)\n",
    "\n",
    "        predict_d = 1914\n",
    "        data = data[data.d.isin(d_cols[-56:])]\n",
    "        data = predict_recursive(data, predict_d, cols, models)\n",
    "    \n",
    "        sub_df = pd.concat([\n",
    "            sub_df,\n",
    "            data[data.d.isin(d_cols[-28:])][['id', 'd', 'TARGET', 'price']]\n",
    "        ], axis=0)\n",
    "    sub_df.to_csv(f'submisson.csv', index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\n",
    "    #path = '/Users/abcdm/Downloads/m5-forecasting-accuracy/'\n",
    "    #path = '../input/m5-forecasting-accuracy/'\n",
    "    path = '../input/null-of-create-m5-select-fetures/'\n",
    "    \n",
    "    data = pd.read_csv(path+'data.csv')\n",
    "    data = data[data.store_id==0]\n",
    "    \n",
    "    print(data.shape)\n",
    "    for p in [1.1, 1.4, 1.7, 1.9][::-1]:\n",
    "        print(p)\n",
    "        PARAMS = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'tweedie',\n",
    "            'tweedie_variance_power': p,\n",
    "            'metric': 'rmse',\n",
    "            'subsample': 0.5,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.03,\n",
    "            'num_leaves': 2**11-1,\n",
    "            'min_data_in_leaf': 2**12-1,\n",
    "            'feature_fraction': 0.5,\n",
    "            'max_bin': 100,\n",
    "            'n_estimators': 1400,\n",
    "            'boost_from_average': False,\n",
    "            'verbose': 1,\n",
    "\n",
    "            'random_state':2020\n",
    "            } \n",
    "        models, trn_df, cols = train_sub_predict_recursive(data, params=PARAMS)\n",
    "        importance = plot_importance(models, cols, 'true')\n",
    "        importance.to_csv('importance_true.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
