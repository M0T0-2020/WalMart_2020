{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport numpy as np\nimport pandas as pd\nimport datetime\nfrom catboost import CatBoostClassifier\nimport lightgbm as lgb\nfrom time import time\nfrom tqdm import tqdm_notebook as tqdm\nfrom collections import Counter\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport ast\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, RepeatedKFold, GroupKFold, GridSearchCV, train_test_split, TimeSeriesSplit\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn import linear_model\n\nfrom tqdm import tqdm_notebook as tqdm\nimport gc, pickle\n\nimport datetime\nfrom catboost import CatBoostClassifier\nfrom time import time\nfrom collections import Counter\nfrom scipy import stats\n\nfrom sklearn import preprocessing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_is_sell_data(sell_prices_df, calendar_df, train_df):\n    sell_prices_df['id'] = sell_prices_df['item_id'].astype('str')+'_'+sell_prices_df['store_id']+'_validation'\n    sell_prices_data = sell_prices_df[sell_prices_df.wm_yr_wk.isin(calendar_df.wm_yr_wk.unique())]\n    sell_prices_data.reset_index(drop=True, inplace=True)\n    tmp = sell_prices_data.groupby(['id'])[['wm_yr_wk', 'sell_price']].apply(lambda x: x.set_index('wm_yr_wk')['sell_price'].to_dict()).to_dict()\n    d = calendar_df.d\n    wm_yr_wk = calendar_df.wm_yr_wk\n    price_data = {}\n    for col in tqdm(train_df.id.unique()):\n        price_data[col] = wm_yr_wk.map(tmp[col])\n    price_data = pd.DataFrame(price_data)\n    price_data.index = d\n    is_sell = price_data.notnull().astype(float).T\n    price_data = price_data.fillna(0)\n    \n    is_sell.index=train_df.id\n    train_df.index=train_df.id\n    is_sell = pd.concat([\n        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']], is_sell\n    ], axis=1)\n    price_data = pd.concat([\n        train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']], price_data.T  \n    ], axis=1)\n    \n    return price_data, is_sell\n\ndef set_index(df, name):\n    d = {}\n    for col, value in df.iloc[0,:].items():\n        if type(col)==str:\n            if 'level' in col:\n                v = 'd'\n            else:\n                v='id'\n        else:\n            v=name\n        d[col]=v\n    return d\n\ndef dcol2int(col):\n    if col[:2]=='d_':\n        return int(col.replace('d_', ''))\n    else:\n        return col","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_category_data(data, dfT):\n    C = ['item_id', 'cat_id', 'dept_id']\n    tmp = dfT[C]\n    for c in C:\n        tmp[c] = tmp[c]\n    tmp = tmp.to_dict()\n    for key, value in tmp.items():\n        data[key] = data.id.map(value)\n    return data\ndef create_calendar_data_dict(calendar_df, use_d_cols, _store):\n    calendar_data_dict = {}\n    calendar_data_dict.update(\n        calendar_df[calendar_df.d.isin(use_d_cols)][['wday', 'month', f'snap_{_store[:2]}']].to_dict()\n    )\n    \n    for shift in [0,1]:\n        tmp = calendar_df[calendar_df.d.isin(use_d_cols)][[f'snap_{_store[:2]}']].shift(shift)\n        tmp=tmp.rename(columns={f'snap_{_store[:2]}':f'snap_{_store[:2]}_shift{shift}'})\n        calendar_data_dict.update(tmp.to_dict())\n        \n    return calendar_data_dict\n\ndef tmp_a_calendar_data(train_df):\n    a = train_df.groupby(['store_id', 'dept_id'])[d_cols].agg('mean').T.astype(float)\n    a.columns = [f'{c1}_and_{c2}' for c1, c2 in a.columns]\n    a = a/a.rolling(len(a), min_periods=1).mean()\n    a['d'] = a.index\n    a.index = a.d\n    return a\n\ndef create_tmp_data_dict(event_day_dict, a, shift):\n    tmp_data_dict={}\n    for tmp_days in event_day_dict.values():\n        t = a.shift(shift).loc[tmp_days]\n        t = t.shift(1).rolling(len(t), min_periods=1).mean()\n        t = t.loc[:,t.columns.str.startswith(_store)]\n        for col in t.columns:\n            for v in t[col].iteritems():\n                tmp_data_dict[f'{col}_and_{v[0]}'] = v[1]\n    return tmp_data_dict\n\ndef create_event_data(data, name, shifts):\n    event = calendar_df[name].dropna().unique().tolist()\n    event_day_dict={}\n    for event_name in event:\n        event_day_dict[event_name] = calendar_df[calendar_df[name]==event_name].index.tolist()\n    a = tmp_a_calendar_data(train_df)\n    \n    for shift in shifts:\n        tmp_data_dict = create_tmp_data_dict(event_day_dict, a, shift)\n        new_col_name = f'{name}_{shift}' if shift>=0 else f'{name}_mns_{-shift}'\n        data[new_col_name] = data.store_and_deptid_and_d.map(tmp_data_dict).shift(-shift)\n    return data\n\ndef create_groupby_price_data(data, store_price_data, use_d_cols, groups=['dept_id', 'cat_id']):\n    for group in groups:\n        tmp = store_price_data[use_d_cols]/store_price_data.groupby([group])[use_d_cols].transform('mean')\n        tmp = tmp.T.astype(float)\n        tmp = tmp.rolling(7, min_periods=1).mean().stack(dropna=False).reset_index()\n        tmp = tmp.rename(columns=set_index(tmp, f'price_mean_{group}'))\n        data = pd.merge(data, tmp, on=['d', 'id'], how='left')\n    \n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_diff_data(data, df, i, wins, use_d_cols, name=None):\n    a = df.diff(i)\n    for win in wins:\n        for agg in ['mean', 'std']:\n            tmp = a.rolling(window=win, min_periods=1).agg(agg).loc[use_d_cols].stack(dropna=False).reset_index()\n            if name is None:\n                tmp = tmp.rename(columns=set_index(tmp, f'diff_{i}_roll{win}_{agg}'))\n            else:\n                tmp = tmp.rename(columns=set_index(tmp, f'{name}_diff_{i}_roll{win}_{agg}'))\n            data = pd.merge(data, tmp, on=['d', 'id'], how='left')\n    a = abs(a)\n    for win in wins:\n        for agg in ['mean', 'std']:\n            tmp = a.rolling(window=win, min_periods=1).agg(agg).loc[use_d_cols].stack(dropna=False).reset_index()\n            if name is None:\n                tmp = tmp.rename(columns=set_index(tmp, f'abs_diff_{i}_roll{win}_{agg}'))\n            else:\n                tmp = tmp.rename(columns=set_index(tmp, f'{name}_abs_diff_{i}_roll{win}_{agg}'))\n            data = pd.merge(data, tmp, on=['d', 'id'], how='left')\n    return data\n\ndef create_group_sell_data(data, dfT, use_d_cols, groups=['dept_id', 'cat_id']):\n    for group in groups:\n        tmp = dfT[use_d_cols]/ dfT.groupby([group])[use_d_cols].transform('mean')\n        tmp = tmp.T.astype(float)\n        data = create_diff_data(data, tmp, 7, wins=[360, 30, 28, 7], use_d_cols=use_d_cols, name=f'By_{group}')\n        data = create_diff_data(data, tmp, 1, wins=[360, 7], use_d_cols=use_d_cols, name=f'By_{group}')\n        data = create_diff_data(data, tmp, 28, wins=[360, 7], use_d_cols=use_d_cols, name=f'By_{group}')\n    return data\n\n\ndef create_shift_data(data, df, shifts, use_d_cols, name=None):\n    for i, shift in enumerate(shifts):\n        tmp = df.shift(shift).loc[use_d_cols].stack(dropna=False).reset_index()\n        if name is None:\n            tmp = tmp.rename(columns=set_index(tmp, f'shift_no{i+1}'))\n        else:\n            tmp = tmp.rename(columns=set_index(tmp, f'{name}_shift_no{i+1}'))\n        data = pd.merge(data, tmp, on=['d', 'id'], how='left')\n    return data\n\ndef create_group_shift_data(data, dfT, shifts, use_d_cols):\n    for group in ['dept_id', 'cat_id']:\n        tmp = dfT.groupby([group])[use_d_cols].transform('mean').T.astype(float)\n        data = create_shift_data(data, tmp, shifts, use_d_cols, name=f'By_{group}')\n    return data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def make_input(train_d_cols, _store):\n    use_d_cols = [i for i in range(train_d_cols[0]-380,train_d_cols[0])]+train_d_cols\n    ids = train_df[train_df.store_id==_store].id.unique().tolist()\n    \n    df = train_df.loc[ids,:]\n    df = df.T.loc[d_cols]\n    df = pd.DataFrame(df.values.astype(float), index=d_cols, columns=df.columns)\n\n    df['d']=df.index\n    calendar_dict = calendar_df[[\n        'date', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'd', 'event_name_1',\n        'event_type_1', 'event_name_2', 'event_type_2',f'snap_{_store[:2]}'\n    ]].set_index('d').to_dict()\n\n    for key, value in calendar_dict.items():\n        df[key] = df['d'].map(value)\n    df.drop('d', axis=1, inplace=True)\n    \n    store_price_data = price_data.loc[ids,:]\n    store_is_sell = is_sell.loc[ids,:]\n    store_price_data[[col for col in store_price_data.columns if type(col)!=str]] =\\\n            store_price_data[[col for col in store_price_data.columns if type(col)!=str]].replace(0, np.nan)\n    \n    \n    df = df.loc[use_d_cols]\n    dfT = train_df.loc[ids,['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']+use_d_cols]\n    store_is_sell = store_is_sell[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']+use_d_cols]\n    store_price_data = store_price_data[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']+use_d_cols]\n    \n    data = pd.concat([\n        df.loc[train_d_cols, ids].stack(dropna=False).reset_index().rename(columns={'level_0':'d', 'id':'id', 0:'TARGET'})\n    ], axis=0)\n    \n    \n    data = fe(data, ids, df, dfT, store_price_data, calendar_df, use_d_cols, train_d_cols, _store)\n    \n    return data\n\ndef fe(data, ids, df, dfT, store_price_data, calendar_df, use_d_cols, train_d_cols, _store):\n    data = create_category_data(data, dfT)\n    \n    calendar_data_dict = create_calendar_data_dict(calendar_df, use_d_cols, _store)\n    for key, values in calendar_data_dict.items():\n        data[key] = data.d.map(values)\n\n    data['store_and_deptid_and_d'] = _store+'_and_'+data.dept_id+'_and_'+data.d.astype(str)\n\n    data = create_event_data(data, 'event_name_1', [-3,-2,-1,0,1,2,3])\n    data = create_event_data(data, 'event_name_2', [-3,-2,-1,0,1,2,3])\n\n    data['event_type_1_0'] = data.d.map(calendar_df['event_type_1'])\n    data['event_type_2_0'] = data.d.map(calendar_df['event_type_2'])\n    for i in [-3,-2,-1,1,2,3]:\n        new_col_name = f'event_type_1_{i}' if i>=0 else f'event_type_1_mns_{-i}'\n        data[new_col_name] = data.event_type_1_0.shift(i)\n        new_col_name = f'event_type_2_{i}' if i>=0 else f'event_type_2_mns_{-i}'\n        data[new_col_name] = data.event_type_2_0.shift(i)\n\n    data = create_groupby_price_data(data, store_price_data, use_d_cols)\n    \n    data = create_diff_data(data, df[ids], 7, wins=[360, 30, 28, 7], use_d_cols=train_d_cols)\n    data = create_diff_data(data, df[ids], 1, wins=[360, 7], use_d_cols=train_d_cols)\n    data = create_diff_data(data, df[ids], 28, wins=[360, 7], use_d_cols=train_d_cols)\n    data = create_group_sell_data(data, dfT, use_d_cols, groups=['dept_id', 'cat_id'])\n    \n    categories = [\n        'item_id', 'cat_id', 'dept_id',\n        'event_type_1_0', 'event_type_2_0', 'event_type_1_mns_3', 'event_type_2_mns_3', 'event_type_1_mns_2',\n        'event_type_2_mns_2', 'event_type_1_mns_1', 'event_type_2_mns_1', 'event_type_1_1',\n        'event_type_2_1', 'event_type_1_2', 'event_type_2_2', 'event_type_1_3', 'event_type_2_3'\n    ]\n    for c in categories:\n        data[c] = pd.factorize(data[c])[0]\n\n    del data['store_and_deptid_and_d']\n    \n    return data, df, dfT","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def train(data, df, dfT):\n    train_d_cols = data.d.unique().tolist()\n    use_d_cols = [i for i in range(train_d_cols[0]-380,train_d_cols[0])]+train_d_cols\n    \n    for for_predict in range(28):\n        print(\"\"\"\n        #########################################\n           ##############  {}  ##############\n        #########################################\"\"\".format(for_predict+1))\n        \n        g_for_predict = for_predict//7 + 1\n        shifts = [7*i for i in range(g_for_predict, g_for_predict+4)]\n        \n        if for_predict%7==0:\n            if for_predict>0:\n                data.drop(columns=shift_cols, inplace=True)\n            data = create_shift_data(data, df, shifts, use_d_cols)\n            data = create_group_shift_data(data, dfT, shifts, use_d_cols)\n                \n        \n        diff_cols = [col for col in data.columns if ('diff' in col)]\n        shift_cols = [col for col in data.columns if ('shift' in col) and (not 'diff' in col)and (not 'snap' in col)]\n        data[diff_cols] = data.groupby(['id'])[diff_cols].transform(\n            lambda x: x.sort_index().shift(1)\n        )\n        \n        data =pd.concat([\n            data['id'],\n            data[[c for c in data.columns if c!='id']].astype(float)\n        ], axis=1)\n        \n        \n        y = data[['TARGET']+['d']].astype(float)\n        X = data.drop(columns=['id', 'TARGET']).astype(float)\n        \n        split=10\n        x_train, x_val = X[X.d.isin(train_d_cols[shifts[0]:-split])], X[X.d.isin(train_d_cols[-split:])]\n        y_train, y_val = y[y.d.isin(train_d_cols[shifts[0]:-split])], y[y.d.isin(train_d_cols[-split:])]\n        x_train.drop('d', axis=1, inplace=True)\n        x_val.drop('d', axis=1, inplace=True)\n        y_train = y_train['TARGET']\n        y_val = y_val['TARGET']\n        \n        train_set = lgb.Dataset(x_train, y_train)\n        val_set = lgb.Dataset(x_val, y_val)\n\n        model = lgb.train(\n                    train_set=train_set, \n                    valid_sets=[train_set, val_set],\n                    params=params, num_boost_round=3000, early_stopping_rounds=100, verbose_eval=500,\n            categorical_feature=categories+['wday', 'month']\n        )\n        \n        pred = model.predict(x_val)\n        \n        plt.figure(figsize=(20,10))\n        plt.subplot(1,2,1)\n        sns.scatterplot(pred, y_val)\n        \n        loss = (pred-y_val)**2\n        plt.subplot(1,2,2)\n        sns.scatterplot(y_val, loss)\n        plt.show()\n        \n        importance = pd.DataFrame()\n        importance['importance'] = model.feature_importance(importance_type='gain')\n        importance['importance'] = preprocessing.minmax_scale(importance.importance)\n        importance.index = x_train.columns\n        importance = importance['importance']\n\n        plt.figure(figsize=(10,30))\n        importance.sort_values( ascending=True).plot('barh')\n        plt.title(f'for {for_predict+1}')\n        gc.collect()\n        del train_set, val_set, X, x_train, x_val, y_train, y_val;gc.collect()\n    gc.collect()\n\ndef plot_importance(model, col, num):\n    importance = pd.DataFrame()\n    importance['importance'] = model.feature_importance(importance_type='gain')\n    importance['importance'] = preprocessing.minmax_scale(importance.importance)\n    importance.index = col\n    importance = importance['importance']\n\n    plt.figure(figsize=(10,30))\n    importance.sort_values( ascending=True).plot('barh')\n    plt.title(f'for {num}')\n    \ndef train_predict(data, df, dfT, _store):\n    train_d_cols = data[data.TARGET.notnull()].d.unique().tolist()\n    use_d_cols = [i for i in range(train_d_cols[0]-380,train_d_cols[0])]+data.d.unique().tolist()\n    ids = data.id.unique().tolist()\n    \n    sub_df = pd.DataFrame()\n    #sub_df.index = ids\n    models = {}\n    for for_predict in range(28):\n        print(\"\"\"\n        #########################################\n           ##############  {}  ##############\n        #########################################\"\"\".format(for_predict+1))\n        \n        g_for_predict = for_predict//7 + 1\n        shifts = [7*i for i in range(g_for_predict, g_for_predict+4)]\n        \n        if for_predict%7==0:\n            if for_predict>0:\n                data.drop(columns=shift_cols, inplace=True)\n            data = create_shift_data(data, df, shifts, use_d_cols)\n            data = create_group_shift_data(data, dfT, shifts, use_d_cols)\n                \n        \n        diff_cols = [col for col in data.columns if ('diff' in col)]\n        shift_cols = [col for col in data.columns if ('shift' in col) and (not 'diff' in col)and (not 'snap' in col)]\n        data[diff_cols] = data.groupby(['id'])[diff_cols].transform(\n            lambda x: x.sort_index().shift(1)\n        )\n        \n        data =pd.concat([\n            data['id'],\n            data[[c for c in data.columns if c!='id']].astype(float)\n        ], axis=1)\n        \n        sub_df = pd.concat([\n            sub_df,\n            data[data.d==TARGET_D_COLS[for_predict]]\n        ],axis=0)\n        \n        y = data[data.TARGET.notnull()][['TARGET']+['d']].astype(float)\n        X = data[data.TARGET.notnull()].drop(columns=['id', 'TARGET']).astype(float)\n        \n        split=10\n        x_train, x_val = X[X.d.isin(train_d_cols[shifts[0]:-split])], X[X.d.isin(train_d_cols[-split:])]\n        y_train, y_val = y[y.d.isin(train_d_cols[shifts[0]:-split])], y[y.d.isin(train_d_cols[-split:])]\n        x_train.drop('d', axis=1, inplace=True)\n        x_val.drop('d', axis=1, inplace=True)\n        y_train = y_train['TARGET']\n        y_val = y_val['TARGET']\n\n        train_set = lgb.Dataset(x_train, y_train)\n        val_set = lgb.Dataset(x_val, y_val)\n\n        model = lgb.train(\n                    train_set=train_set, \n                    valid_sets=[train_set, val_set],\n                    params=params, num_boost_round=3000, early_stopping_rounds=100, verbose_eval=500,\n            categorical_feature=categories+['wday', 'month']\n        )\n        \n        models[f'{_store}_model_{for_predict+1}']=model\n        \n        plot_importance(model, x_train.columns, for_predict+1)\n        gc.collect()\n        \n        \n        del train_set, val_set, X, y, x_train, x_val, y_train, y_val;gc.collect()\n    \n    gc.collect()\n    \n    sub_df.reset_index(drop=True, inplace=True)\n    sub_df.to_pickle(f'{_store}_csv.pickle')\n    with open(f'models_{_store}.pickle', 'wb') as f:\n        pickle.dump(models, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n#path = '/Users/kanoumotoharu/Downloads/m5-forecasting-accuracy/'\n#path = '/Users/abcdm/Downloads/m5-forecasting-accuracy/'\npath = '../input/m5-forecasting-accuracy/'\n\ntrain_df = pd.read_csv(path+'sales_train_validation.csv')\ncalendar_df = pd.read_csv(path+'calendar.csv')\nsell_prices_df = pd.read_csv(path+'sell_prices.csv')\nsample_submission_df = pd.read_csv(path+'sample_submission.csv')\n\n\ncategories = [\n    'item_id', 'cat_id', 'dept_id',\n    'event_type_1_0', 'event_type_2_0', 'event_type_1_mns_3', 'event_type_2_mns_3', 'event_type_1_mns_2',\n    'event_type_2_mns_2', 'event_type_1_mns_1', 'event_type_2_mns_1', 'event_type_1_1',\n    'event_type_2_1', 'event_type_1_2', 'event_type_2_2', 'event_type_1_3', 'event_type_2_3'\n]\n\n\ncalendar_df['d'] = calendar_df.d.str.replace('d_', '').astype(int)\ncols = train_df.columns\ncols = [dcol2int(col) for col in cols]\ntrain_df.columns=cols\ncalendar_df['date']=pd.to_datetime(calendar_df.date)\ncalendar_df.index = calendar_df.d\nprice_data, is_sell = create_is_sell_data(sell_prices_df, calendar_df, train_df)\n\nd_cols = [ col for col in train_df.columns if type(col)!=str ]\nfor i in range(1,29):\n    train_df[d_cols[-1]+i]=np.nan\nd_cols = [ col for col in train_df.columns if type(col)!=str ]\n\ntrain_df = pd.concat([\n    train_df[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']],\n    train_df[d_cols]+is_sell[d_cols].replace(0, np.nan).replace(1, 0)\n], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params = {\n            'n_estimators':2000,\n            'boosting_type': 'gbdt',\n            'objective': 'poisson',\n            'metric': 'rmse',\n            'subsample': 0.75,\n            'subsample_freq': 1,\n            'learning_rate': 0.01,\n            'feature_fraction': 0.85,\n            'max_depth': 15,\n            'lambda_l1': 1,  \n            'lambda_l2': 1,\n            'verbose': 100,\n            'random_state':123,\n            }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lag=30\nTARGET_D_COLS = d_cols[-lag:]\n\ntrain_d_cols = d_cols[-(lag+160+56):-lag]#+TARGET_D_COLS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#'CA_1', 'CA_2', 'CA_3', 'CA_4', 'TX_1', 'TX_2', 'TX_3', 'WI_1', 'WI_2', 'WI_3'\n_store='CA_1'\n\ndata, df, dfT = make_input(train_d_cols, _store)\ntrain(data, df, dfT)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}